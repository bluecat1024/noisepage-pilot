{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/ad/ff3b21ebfe79a4d25b4a4f8e5cf9fd44a204adb6b33c09010f566f51027a/numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7MB 11.8MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.21.6\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RldqATgHNcQU"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-35964790e227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfigparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import configparser\n",
    "import psycopg2\n",
    "import pymysql\n",
    "import pymysql.cursors as pycursor\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F52IBvtcNcQX"
   },
   "source": [
    "# 1. Generate Workload Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "3elP0zchNcQX",
    "outputId": "03516379-cc75-426a-f5b3-adbb07dba02a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nargus = { \"mysql\": {\\n    \"host\": \"166.111.121.62\",\\n    \"password\": \"db10204\",\\n    \"port\": 3306,\\n    \"user\": \"feng\"},\\n    \"postgresql\": {\\n            \"host\": \"166.111.121.62\",\\n            \"password\": \"db10204\",\\n            \"port\": 5433,\\n            \"user\": \"postgres\"}}\\nargus[\"postgresql\"][\"host\"]\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_path = os.path.abspath('.')\n",
    "data_path = cur_path + '/tpcc_1_1/'\n",
    "\n",
    "edge_dim = 25000 # upper bound of edges\n",
    "node_dim = 300 # upper bound of nodes\n",
    "\n",
    "'''\n",
    "class DataType(IntEnum):\n",
    "    Aggregate = 0\n",
    "    NestedLoop = 1\n",
    "    IndexScan = 2\n",
    "'''\n",
    "mp_optype = {'Aggregate': 0, 'Nested Loop': 1, 'Index Scan': 2, 'Hash Join': 3, 'Seq Scan': 4, 'Hash': 5, 'Update': 6} # operator types in the queries\n",
    "\n",
    "oid = 0 # operator number\n",
    "\n",
    "'''\n",
    "argus = { \"mysql\": {\n",
    "    \"host\": \"166.111.121.62\",\n",
    "    \"password\": \"db10204\",\n",
    "    \"port\": 3306,\n",
    "    \"user\": \"feng\"},\n",
    "    \"postgresql\": {\n",
    "            \"host\": \"166.111.121.62\",\n",
    "            \"password\": \"db10204\",\n",
    "            \"port\": 5433,\n",
    "            \"user\": \"postgres\"}}\n",
    "argus[\"postgresql\"][\"host\"]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1XmizGpNcQY"
   },
   "outputs": [],
   "source": [
    "# obtain and normalize configuration knobs\n",
    "\n",
    "class DictParser(configparser.ConfigParser):\n",
    "    def read_dict(self):\n",
    "        d = dict(self._sections)\n",
    "        for k in d:\n",
    "            d[k] = dict(d[k])\n",
    "        return d\n",
    "\n",
    "cf = DictParser()\n",
    "cf.read(\"config.ini\", encoding=\"utf-8\")\n",
    "config_dict = cf.read_dict()\n",
    "\n",
    "\n",
    "def parse_knob_config():\n",
    "    _knob_config = config_dict[\"knob_config\"]\n",
    "    for key in _knob_config:\n",
    "        _knob_config[key] = json.loads(str(_knob_config[key]).replace(\"\\'\", \"\\\"\"))\n",
    "    return _knob_config\n",
    "\n",
    "\n",
    "class Database:\n",
    "    def __init__(self, server_name='postgresql'):\n",
    "        \n",
    "        knob_config = parse_knob_config()\n",
    "        self.knob_names = [knob for knob in knob_config]\n",
    "        self.knob_config = knob_config\n",
    "        self.server_name = server_name\n",
    "        \n",
    "        # print(\"knob_names:\", self.knob_names)\n",
    "        \n",
    "        try:\n",
    "            conn = self._get_conn()\n",
    "            cursor = conn.cursor()\n",
    "            sql = \"SELECT count FROM INFORMATION_SCHEMA.INNODB_METRICS where status='enabled'\"\n",
    "            cursor.execute(sql)\n",
    "            result = cursor.fetchall()\n",
    "\n",
    "            self.internal_metric_num = len(result)\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "        except Exception as err:\n",
    "            print(\"execute sql error:\", err)\n",
    "\n",
    "    def _get_conn(self):\n",
    "        if self.server_name == 'mysql':\n",
    "            sucess = 0\n",
    "            conn = -1\n",
    "            count = 0\n",
    "            while not sucess and count < 3:\n",
    "                try:\n",
    "                    conn = pymysql.connect(host=\"166.111.121.62\",\n",
    "                                           port=3306,\n",
    "                                           user=\"feng\",\n",
    "                                           password=\"db10204\",\n",
    "                                           db='INFORMATION_SCHEMA',\n",
    "                                           connect_timeout=36000,\n",
    "                                           cursorclass=pycursor.DictCursor)\n",
    "\n",
    "                    sucess = 1\n",
    "                except Exception as result:\n",
    "                    count += 1\n",
    "                    time.sleep(10)\n",
    "            if conn == -1:\n",
    "                raise Exception\n",
    "                \n",
    "            return conn\n",
    "            \n",
    "        elif self.server_name == 'postgresql':\n",
    "            sucess = 0\n",
    "            conn = -1\n",
    "            count = 0\n",
    "            while not sucess and count < 3:\n",
    "                try:\n",
    "                    conn = psycopg2.connect(database=dbname, user=\"lixizhang\", password=\"xi10261026zhang\", host=\"166.111.5.177\", port=\"5433\")\n",
    "                    sucess = 1\n",
    "                except Exception as result:\n",
    "                    count += 1\n",
    "                    time.sleep(10)\n",
    "            if conn == -1:\n",
    "                raise Exception\n",
    "            return conn\n",
    "\n",
    "        else:\n",
    "            print('数据库连接不上...')\n",
    "            return\n",
    "\n",
    "    def fetch_knob(self):\n",
    "        state_list = np.append([], [])\n",
    "        try:\n",
    "            conn = self._get_conn()\n",
    "            cursor = conn.cursor()\n",
    "            sql = \"select\"\n",
    "            for i, knob in enumerate(self.knob_names):\n",
    "                sql = sql + ' @@' + knob\n",
    "\n",
    "                if i < len(self.knob_names) - 1:\n",
    "                    sql = sql + ', '\n",
    "\n",
    "            # state metrics\n",
    "            cursor.execute(sql)\n",
    "            result = cursor.fetchall()\n",
    "            \n",
    "            for i in range(len(self.knob_names)):\n",
    "                value = result[0][\"@@%s\" % self.knob_names[i]] if result[0][\"@@%s\" % self.knob_names[i]]!=0 else self.knob_config[self.knob_names[i]][\"max_value\"] # not limit if value equals 0\n",
    "                \n",
    "                # print(value, self.knob_config[self.knob_names[i]][\"max_value\"], self.knob_config[self.knob_names[i]][\"min_value\"])\n",
    "                state_list = np.append(state_list, value / (self.knob_config[self.knob_names[i]][\"max_value\"] - self.knob_config[self.knob_names[i]][\"min_value\"]))\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "        except Exception as error:\n",
    "            print(\"fetch_knob Error:\", error)\n",
    "        \n",
    "        return state_list\n",
    "\n",
    "# db = Database(\"mysql\")\n",
    "# print(db.fetch_knob())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUukvAPgNcQZ"
   },
   "outputs": [],
   "source": [
    "# actual runtime:  actuall executed (training data) / estimated by our model\n",
    "# operators in the same plan can have data conflicts (parallel)\n",
    "\n",
    "def compute_cost(node):\n",
    "    return float(node[\"Total Cost\"]) - float(node[\"Startup Cost\"]) \n",
    "\n",
    "def compute_time(node):\n",
    "    # return float(node[\"Actual Total Time\"]) - float(node[\"Actual Startup Time\"]) \n",
    "    return float(node[\"Actual Total Time\"]) # mechanism within pg\n",
    "    \n",
    "def get_used_tables(node):\n",
    "\n",
    "    tables = []\n",
    "\n",
    "    stack = [node]\n",
    "    while stack != []:\n",
    "        parent = stack.pop(0)\n",
    "        \n",
    "        if \"Relation Name\" in parent:\n",
    "            tables.append(parent[\"Relation Name\"])\n",
    "        \n",
    "        if \"Plans\" in parent:\n",
    "            for n in parent[\"Plans\"]:\n",
    "                stack.append(n)\n",
    "\n",
    "    return  tables\n",
    "\n",
    "\n",
    "def extract_plan(sample, conflict_operators):\n",
    "    global mp_optype, oid\n",
    "    # function: extract SQL feature\n",
    "    # return: start_time, node feature, edge feature\n",
    "    \n",
    "    plan = sample[\"plan\"]\n",
    "    while isinstance(plan, list):\n",
    "        plan = plan[0]\n",
    "    # Features: print(plan.keys()) \n",
    "        # start time = plan[\"start_time\"]\n",
    "        # node feature = [Node Type, Total Cost:: Actual Total Time]\n",
    "        # node label = [Actual Startup Time, Actual Total Time]\n",
    "\n",
    "    plan = plan[\"Plan\"] # root node\n",
    "    node_matrix = []\n",
    "    edge_matrix = []\n",
    "    \n",
    "    # add oid for each operator\n",
    "    stack = [plan]\n",
    "    while stack != []:\n",
    "        parent = stack.pop(0)\n",
    "        parent[\"oid\"] = oid\n",
    "        oid = oid + 1\n",
    "        \n",
    "        if \"Plans\" in parent:\n",
    "            for node in parent[\"Plans\"]:\n",
    "                stack.append(node)    \n",
    "    \n",
    "    stack = [plan]\n",
    "    while stack != []:\n",
    "        parent = stack.pop(0)\n",
    "        run_cost = compute_cost(parent)\n",
    "        run_time = compute_time(parent)\n",
    "        # print(parent[\"Actual Total Time\"], parent[\"Actual Startup Time\"], run_time)\n",
    "        \n",
    "        if parent[\"Node Type\"] not in mp_optype:\n",
    "            mp_optype[parent[\"Node Type\"]] = len(mp_optype)\n",
    "        \n",
    "        tables = get_used_tables(parent)\n",
    "        # print(\"[tables]\", tables)\n",
    "        \n",
    "        operator_info = [parent[\"oid\"], parent[\"Startup Cost\"], parent[\"Total Cost\"]]\n",
    "        \n",
    "        for table in tables:\n",
    "            if table not in conflict_operators:\n",
    "                conflict_operators[table] = [operator_info]\n",
    "            else:\n",
    "                conflict_operators[table].append(operator_info)\n",
    "        \n",
    "                \n",
    "        node_feature = [parent[\"oid\"], mp_optype[parent[\"Node Type\"]], run_cost, float(parent[\"Actual Startup Time\"]), run_time]\n",
    "        \n",
    "        node_matrix = [node_feature] + node_matrix\n",
    "        \n",
    "        if \"Plans\" in parent:\n",
    "            for node in parent[\"Plans\"]:\n",
    "                stack.append(node)\n",
    "                edge_matrix = [[node[\"oid\"], parent[\"oid\"], 1]] + edge_matrix\n",
    "\n",
    "    # node: 18 * featuers\n",
    "    # edge: 18 * 18\n",
    "\n",
    "    return float(sample[\"start_time\"]), node_matrix, edge_matrix, conflict_operators\n",
    "\n",
    "\n",
    "def overlap(node_i, node_j):\n",
    "    \n",
    "    if (node_j[1] < node_i[2] and node_i[2] < node_j[2]):\n",
    "        \n",
    "        return (node_i[2] - node_j[1]) / (node_j[2] - min(node_i[1], node_j[1]))\n",
    "    \n",
    "    elif (node_i[1] < node_j[2] and node_j[2] < node_i[2]):\n",
    "        \n",
    "        return (node_j[2] - node_i[1]) / (node_i[2] - min(node_i[1], node_j[1]))\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def add_across_plan_relations(conflict_operators, knobs, ematrix):\n",
    "    \n",
    "    data_weight = 0.1\n",
    "    for knob in knobs:\n",
    "        data_weight *= knob\n",
    "    # print(conflict_operators)\n",
    "    \n",
    "    # add relations [rw/ww, rr, config]\n",
    "    for table in conflict_operators:\n",
    "        for i in range(len(conflict_operators[table])):\n",
    "            for j in range(i+1, len(conflict_operators[table])):\n",
    "\n",
    "                node_i = conflict_operators[table][i]\n",
    "                node_j = conflict_operators[table][j]\n",
    "                \n",
    "                time_overlap = overlap(node_i, node_j)\n",
    "                if time_overlap:\n",
    "                    ematrix = ematrix + [[node_i[0], node_j[0], -data_weight * time_overlap]]\n",
    "                    ematrix = ematrix + [[node_j[0], node_i[0], -data_weight * time_overlap]]\n",
    "\n",
    "                '''\n",
    "                if overlap(i, j) and (\"rw\" or \"ww\"):\n",
    "                    ematrix = ematrix + [[conflict_operators[table][i], conflict_operators[table][j], data_weight * time_overlap]]\n",
    "                    ematrix = ematrix + [[conflict_operators[table][j], conflict_operators[table][i], data_weight * time_overlap]]\n",
    "                '''\n",
    "                    \n",
    "    return ematrix\n",
    "\n",
    "\n",
    "def generate_graph(wid, path = data_path):\n",
    "    global oid\n",
    "    # fuction\n",
    "    # return\n",
    "    # todo: timestamp\n",
    "    \n",
    "    vmatrix = []\n",
    "    ematrix = [] \n",
    "    conflict_operators = {}\n",
    "\n",
    "    oid = 0\n",
    "    with open(path + \"sample-plan-\" + str(wid), \"r\") as f:        \n",
    "        \n",
    "        # vertex: operators\n",
    "        # edge: child-parent relations\n",
    "        for sample in f.readlines():\n",
    "            \n",
    "            sample = json.loads(sample)\n",
    "            \n",
    "            # Step 1: read (operators, parent-child edges) in separate plans\n",
    "            start_time, node_matrix, edge_matrix, conflict_operators = extract_plan(sample, conflict_operators)\n",
    "            \n",
    "            vmatrix = vmatrix + node_matrix\n",
    "            ematrix = ematrix + edge_matrix\n",
    "\n",
    "        # Step 2: read related knobs\n",
    "        # db = Database(\"mysql\")\n",
    "        # knobs = db.fetch_knob()\n",
    "        knobs = []\n",
    "            \n",
    "        # Step 3: add relations across queries\n",
    "        ematrix = add_across_plan_relations(conflict_operators, knobs, ematrix)\n",
    "        \n",
    "        # edge: data relations based on (access tables, related knob values)\n",
    "            \n",
    "\n",
    "    return vmatrix, ematrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "SnErfbZ-NcQa",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Step-0: split the worklaods into multiple concurrent queries at differnet time (\"sample-plan-x\")\n",
    "def process_graph_data(data_path : str, num_graphs = 1000):\n",
    "  workloads = glob.glob(f\"{data_path}/sample-plan-*\")\n",
    "\n",
    "  start_time = time.time()\n",
    "  shutil.rmtree(f\"{data_path}/graph/\")\n",
    "  os.mkdir(f\"{data_path}/graph/\")\n",
    "  for wid in range(num_graphs):\n",
    "      st = time.time()\n",
    "\n",
    "      vmatrix, ematrix = generate_graph(wid)\n",
    "      print(\"[graph {}]\".format(wid), \"time:{}; #-vertex:{}, #-edge:{}\".format(time.time() - st, len(vmatrix), len(ematrix)))\n",
    "      \n",
    "      with open(f\"{data_path}/graph/\" + \"sample-plan-\" + str(wid) + \".content\", \"w\") as wf:\n",
    "          for v in vmatrix:\n",
    "              wf.write(str(v[0]) + \"\\t\" + str(v[1]) + \"\\t\" + str(v[2]) + \"\\t\" + str(v[3]) + \"\\t\" + str(v[4]) + \"\\n\")\n",
    "      with open(f\"{data_path}/graph/\" + \"sample-plan-\" + str(wid) + \".cites\", \"w\") as wf:\n",
    "          for e in ematrix:\n",
    "              wf.write(str(e[0]) + \"\\t\" + str(e[1]) + \"\\t\" + str(e[2]) + \"\\n\")\n",
    "\n",
    "  end_time = time.time()\n",
    "\n",
    "  print(\"Total Time:{}\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Z8bbE1F1hGIV",
    "outputId": "ddfdf310-6853-4094-dd2c-be47fb07c9c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[graph 0] time:0.022179126739501953; #-vertex:170, #-edge:2850\n",
      "[graph 1] time:0.009698152542114258; #-vertex:180, #-edge:1596\n",
      "[graph 2] time:0.008410453796386719; #-vertex:174, #-edge:1542\n",
      "[graph 3] time:0.026463031768798828; #-vertex:176, #-edge:3372\n",
      "[graph 4] time:0.019646406173706055; #-vertex:170, #-edge:2874\n",
      "[graph 5] time:0.01567816734313965; #-vertex:174, #-edge:2336\n",
      "[graph 6] time:0.008656024932861328; #-vertex:174, #-edge:1450\n",
      "[graph 7] time:0.10458183288574219; #-vertex:188, #-edge:1348\n",
      "[graph 8] time:0.03148007392883301; #-vertex:173, #-edge:3147\n",
      "[graph 9] time:0.029314041137695312; #-vertex:175, #-edge:3581\n",
      "[graph 10] time:0.020031452178955078; #-vertex:173, #-edge:2737\n",
      "[graph 11] time:0.01772284507751465; #-vertex:186, #-edge:2438\n",
      "[graph 12] time:0.01149296760559082; #-vertex:172, #-edge:1976\n",
      "[graph 13] time:0.03536868095397949; #-vertex:174, #-edge:3978\n",
      "[graph 14] time:0.024755239486694336; #-vertex:178, #-edge:3022\n",
      "[graph 15] time:0.04615449905395508; #-vertex:170, #-edge:4652\n",
      "[graph 16] time:0.008603572845458984; #-vertex:174, #-edge:1388\n",
      "[graph 17] time:0.033223628997802734; #-vertex:175, #-edge:3967\n",
      "[graph 18] time:0.01166081428527832; #-vertex:180, #-edge:1488\n",
      "[graph 19] time:0.018384456634521484; #-vertex:174, #-edge:2778\n",
      "[graph 20] time:0.039157867431640625; #-vertex:167, #-edge:4391\n",
      "[graph 21] time:0.018642187118530273; #-vertex:177, #-edge:2393\n",
      "[graph 22] time:0.01805281639099121; #-vertex:172, #-edge:2622\n",
      "[graph 23] time:0.015368461608886719; #-vertex:175, #-edge:2427\n",
      "[graph 24] time:0.03749585151672363; #-vertex:171, #-edge:4203\n",
      "[graph 25] time:0.027617931365966797; #-vertex:178, #-edge:3488\n",
      "[graph 26] time:0.03006291389465332; #-vertex:175, #-edge:3737\n",
      "[graph 27] time:0.03778791427612305; #-vertex:170, #-edge:4006\n",
      "[graph 28] time:0.010540008544921875; #-vertex:178, #-edge:1706\n",
      "[graph 29] time:0.029465436935424805; #-vertex:168, #-edge:3636\n",
      "[graph 30] time:0.012395381927490234; #-vertex:183, #-edge:1941\n",
      "[graph 31] time:0.008504390716552734; #-vertex:176, #-edge:1528\n",
      "[graph 32] time:0.03928875923156738; #-vertex:175, #-edge:4293\n",
      "[graph 33] time:0.0394136905670166; #-vertex:175, #-edge:4277\n",
      "[graph 34] time:0.007697582244873047; #-vertex:173, #-edge:1311\n",
      "[graph 35] time:0.0220186710357666; #-vertex:186, #-edge:2438\n",
      "[graph 36] time:0.011286258697509766; #-vertex:176, #-edge:1868\n",
      "[graph 37] time:0.02164149284362793; #-vertex:173, #-edge:2347\n",
      "[graph 38] time:0.020748376846313477; #-vertex:171, #-edge:2995\n",
      "[graph 39] time:0.02422642707824707; #-vertex:174, #-edge:3218\n",
      "[graph 40] time:0.12829208374023438; #-vertex:170, #-edge:4352\n",
      "[graph 41] time:0.010759115219116211; #-vertex:188, #-edge:1558\n",
      "[graph 42] time:0.021372556686401367; #-vertex:168, #-edge:3070\n",
      "[graph 43] time:0.013672113418579102; #-vertex:176, #-edge:2072\n",
      "[graph 44] time:0.011098623275756836; #-vertex:182, #-edge:1818\n",
      "[graph 45] time:0.01737046241760254; #-vertex:180, #-edge:2390\n",
      "[graph 46] time:0.02035832405090332; #-vertex:175, #-edge:2857\n",
      "[graph 47] time:0.011218786239624023; #-vertex:179, #-edge:1831\n",
      "[graph 48] time:0.018108844757080078; #-vertex:174, #-edge:2726\n",
      "[graph 49] time:0.02283000946044922; #-vertex:174, #-edge:2956\n",
      "[graph 50] time:0.009560346603393555; #-vertex:172, #-edge:1616\n",
      "[graph 51] time:0.024258136749267578; #-vertex:171, #-edge:3303\n",
      "[graph 52] time:0.029837369918823242; #-vertex:175, #-edge:3677\n",
      "[graph 53] time:0.025096654891967773; #-vertex:174, #-edge:3256\n",
      "[graph 54] time:0.017731666564941406; #-vertex:173, #-edge:2545\n",
      "[graph 55] time:0.027237892150878906; #-vertex:171, #-edge:3525\n",
      "[graph 56] time:0.02119612693786621; #-vertex:173, #-edge:2905\n",
      "[graph 57] time:0.022061586380004883; #-vertex:174, #-edge:2804\n",
      "[graph 58] time:0.017525434494018555; #-vertex:172, #-edge:2554\n",
      "[graph 59] time:0.0355987548828125; #-vertex:174, #-edge:4110\n",
      "[graph 60] time:0.02631211280822754; #-vertex:167, #-edge:3333\n",
      "[graph 61] time:0.02175116539001465; #-vertex:179, #-edge:2987\n",
      "[graph 62] time:0.012827396392822266; #-vertex:179, #-edge:2117\n",
      "[graph 63] time:0.010039806365966797; #-vertex:173, #-edge:1791\n",
      "[graph 64] time:0.009853363037109375; #-vertex:183, #-edge:1639\n",
      "[graph 65] time:0.01319265365600586; #-vertex:174, #-edge:1990\n",
      "[graph 66] time:0.027936935424804688; #-vertex:174, #-edge:3542\n",
      "[graph 67] time:0.03481626510620117; #-vertex:175, #-edge:4009\n",
      "[graph 68] time:0.011731147766113281; #-vertex:170, #-edge:1878\n",
      "[graph 69] time:0.01716136932373047; #-vertex:178, #-edge:2196\n",
      "[graph 70] time:0.01654219627380371; #-vertex:181, #-edge:1717\n",
      "[graph 71] time:0.033103227615356445; #-vertex:169, #-edge:3785\n",
      "[graph 72] time:0.013979673385620117; #-vertex:178, #-edge:2130\n",
      "[graph 73] time:0.01676177978515625; #-vertex:172, #-edge:2594\n",
      "[graph 74] time:0.12511467933654785; #-vertex:176, #-edge:4268\n",
      "[graph 75] time:0.015565633773803711; #-vertex:170, #-edge:2450\n",
      "[graph 76] time:0.02956104278564453; #-vertex:172, #-edge:3614\n",
      "[graph 77] time:0.016121625900268555; #-vertex:180, #-edge:2292\n",
      "[graph 78] time:0.008390665054321289; #-vertex:176, #-edge:1520\n",
      "[graph 79] time:0.02126479148864746; #-vertex:173, #-edge:3037\n",
      "[graph 80] time:0.025814056396484375; #-vertex:173, #-edge:3289\n",
      "[graph 81] time:0.009248495101928711; #-vertex:169, #-edge:1607\n",
      "[graph 82] time:0.029547691345214844; #-vertex:174, #-edge:3754\n",
      "[graph 83] time:0.028199434280395508; #-vertex:173, #-edge:3509\n",
      "[graph 84] time:0.011910438537597656; #-vertex:172, #-edge:1834\n",
      "[graph 85] time:0.025471925735473633; #-vertex:174, #-edge:3346\n",
      "[graph 86] time:0.025046586990356445; #-vertex:168, #-edge:3308\n",
      "[graph 87] time:0.01669454574584961; #-vertex:176, #-edge:2512\n",
      "[graph 88] time:0.026552677154541016; #-vertex:172, #-edge:3414\n",
      "[graph 89] time:0.034950971603393555; #-vertex:173, #-edge:4061\n",
      "[graph 90] time:0.040636301040649414; #-vertex:177, #-edge:4243\n",
      "[graph 91] time:0.026154041290283203; #-vertex:172, #-edge:3454\n",
      "[graph 92] time:0.026651382446289062; #-vertex:173, #-edge:3317\n",
      "[graph 93] time:0.022591590881347656; #-vertex:172, #-edge:3144\n",
      "[graph 94] time:0.011855363845825195; #-vertex:173, #-edge:1985\n",
      "[graph 95] time:0.017778396606445312; #-vertex:173, #-edge:2725\n",
      "[graph 96] time:0.015765666961669922; #-vertex:178, #-edge:2446\n",
      "[graph 97] time:0.008244991302490234; #-vertex:174, #-edge:1550\n",
      "[graph 98] time:0.016093969345092773; #-vertex:173, #-edge:2517\n",
      "[graph 99] time:0.01284027099609375; #-vertex:176, #-edge:2010\n",
      "[graph 100] time:0.017075300216674805; #-vertex:186, #-edge:2438\n",
      "[graph 101] time:0.019210100173950195; #-vertex:176, #-edge:2796\n",
      "[graph 102] time:0.013792753219604492; #-vertex:177, #-edge:1909\n",
      "[graph 103] time:0.015349864959716797; #-vertex:176, #-edge:2106\n",
      "[graph 104] time:0.016857624053955078; #-vertex:185, #-edge:2537\n",
      "[graph 105] time:0.015972614288330078; #-vertex:181, #-edge:2409\n",
      "[graph 106] time:0.011945724487304688; #-vertex:172, #-edge:1818\n",
      "[graph 107] time:0.013021230697631836; #-vertex:176, #-edge:2118\n",
      "[graph 108] time:0.01700901985168457; #-vertex:172, #-edge:2660\n",
      "[graph 109] time:0.08351993560791016; #-vertex:172, #-edge:2768\n",
      "[graph 110] time:0.03764629364013672; #-vertex:173, #-edge:4265\n",
      "[graph 111] time:0.028896570205688477; #-vertex:176, #-edge:3548\n",
      "[graph 112] time:0.012066841125488281; #-vertex:171, #-edge:2097\n",
      "[graph 113] time:0.02594161033630371; #-vertex:174, #-edge:3460\n",
      "[graph 114] time:0.02120828628540039; #-vertex:172, #-edge:2992\n",
      "[graph 115] time:0.030797243118286133; #-vertex:170, #-edge:3822\n",
      "[graph 116] time:0.028402090072631836; #-vertex:171, #-edge:3645\n",
      "[graph 117] time:0.014911890029907227; #-vertex:175, #-edge:2331\n",
      "[graph 118] time:0.026462078094482422; #-vertex:175, #-edge:3485\n",
      "[graph 119] time:0.01752758026123047; #-vertex:172, #-edge:2674\n",
      "[graph 120] time:0.031491756439208984; #-vertex:176, #-edge:3758\n",
      "[graph 121] time:0.023974895477294922; #-vertex:171, #-edge:3303\n",
      "[graph 122] time:0.016658782958984375; #-vertex:173, #-edge:2493\n",
      "[graph 123] time:0.05048036575317383; #-vertex:169, #-edge:4961\n",
      "[graph 124] time:0.011210203170776367; #-vertex:179, #-edge:1829\n",
      "[graph 125] time:0.013777971267700195; #-vertex:174, #-edge:2266\n",
      "[graph 126] time:0.01082611083984375; #-vertex:180, #-edge:1882\n",
      "[graph 127] time:0.010544300079345703; #-vertex:170, #-edge:1800\n",
      "[graph 128] time:0.027571916580200195; #-vertex:173, #-edge:3615\n",
      "[graph 129] time:0.022781848907470703; #-vertex:172, #-edge:3146\n",
      "[graph 130] time:0.032074689865112305; #-vertex:173, #-edge:3615\n",
      "[graph 131] time:0.009662389755249023; #-vertex:185, #-edge:1563\n",
      "[graph 132] time:0.01645493507385254; #-vertex:175, #-edge:2553\n",
      "[graph 133] time:0.007420778274536133; #-vertex:178, #-edge:1246\n",
      "[graph 134] time:0.01921248435974121; #-vertex:171, #-edge:2875\n",
      "[graph 135] time:0.013127326965332031; #-vertex:174, #-edge:2268\n",
      "[graph 136] time:0.011096000671386719; #-vertex:169, #-edge:1707\n",
      "[graph 137] time:0.011351585388183594; #-vertex:177, #-edge:1819\n",
      "[graph 138] time:0.022196054458618164; #-vertex:185, #-edge:2437\n",
      "[graph 139] time:0.016989707946777344; #-vertex:177, #-edge:2651\n",
      "[graph 140] time:0.014729976654052734; #-vertex:168, #-edge:2204\n",
      "[graph 141] time:0.030952930450439453; #-vertex:174, #-edge:3826\n",
      "[graph 142] time:0.014973640441894531; #-vertex:175, #-edge:2389\n",
      "[graph 143] time:0.08563494682312012; #-vertex:175, #-edge:3637\n",
      "[graph 144] time:0.009784936904907227; #-vertex:161, #-edge:1825\n",
      "[graph 145] time:0.009849071502685547; #-vertex:181, #-edge:1657\n",
      "[graph 146] time:0.02189183235168457; #-vertex:178, #-edge:2672\n",
      "[graph 147] time:0.009361505508422852; #-vertex:179, #-edge:1443\n",
      "[graph 148] time:0.02016472816467285; #-vertex:172, #-edge:2910\n",
      "[graph 149] time:0.03395867347717285; #-vertex:174, #-edge:3940\n",
      "[graph 150] time:0.018637418746948242; #-vertex:170, #-edge:2720\n",
      "[graph 151] time:0.023741483688354492; #-vertex:176, #-edge:2958\n",
      "[graph 152] time:0.031528472900390625; #-vertex:175, #-edge:3847\n",
      "[graph 153] time:0.017143964767456055; #-vertex:170, #-edge:2490\n",
      "[graph 154] time:0.027395248413085938; #-vertex:173, #-edge:3459\n",
      "[graph 155] time:0.02599930763244629; #-vertex:169, #-edge:3447\n",
      "[graph 156] time:0.029595613479614258; #-vertex:172, #-edge:3614\n",
      "[graph 157] time:0.015849828720092773; #-vertex:178, #-edge:2198\n",
      "[graph 158] time:0.0276186466217041; #-vertex:173, #-edge:3509\n",
      "[graph 159] time:0.010953187942504883; #-vertex:181, #-edge:1699\n",
      "[graph 160] time:0.010262250900268555; #-vertex:182, #-edge:1740\n",
      "[graph 161] time:0.020182132720947266; #-vertex:174, #-edge:2580\n",
      "[graph 162] time:0.010849952697753906; #-vertex:182, #-edge:1720\n",
      "[graph 163] time:0.013333797454833984; #-vertex:174, #-edge:2130\n",
      "[graph 164] time:0.015778541564941406; #-vertex:168, #-edge:1706\n",
      "[graph 165] time:0.03162193298339844; #-vertex:174, #-edge:3706\n",
      "[graph 166] time:0.026752233505249023; #-vertex:169, #-edge:3441\n",
      "[graph 167] time:0.021076440811157227; #-vertex:173, #-edge:3037\n",
      "[graph 168] time:0.028537273406982422; #-vertex:177, #-edge:3429\n",
      "[graph 169] time:0.007191896438598633; #-vertex:164, #-edge:1272\n",
      "[graph 170] time:0.020946741104125977; #-vertex:174, #-edge:3028\n",
      "[graph 171] time:0.026207447052001953; #-vertex:175, #-edge:3381\n",
      "[graph 172] time:0.03247833251953125; #-vertex:168, #-edge:3262\n",
      "[graph 173] time:0.01061248779296875; #-vertex:188, #-edge:1590\n",
      "[graph 174] time:0.012945413589477539; #-vertex:172, #-edge:2194\n",
      "[graph 175] time:0.014605283737182617; #-vertex:168, #-edge:2134\n",
      "[graph 176] time:0.009280920028686523; #-vertex:175, #-edge:1633\n",
      "[graph 177] time:0.009618282318115234; #-vertex:182, #-edge:1618\n",
      "[graph 178] time:0.009502410888671875; #-vertex:175, #-edge:1253\n",
      "[graph 179] time:0.029484272003173828; #-vertex:170, #-edge:3622\n",
      "[graph 180] time:0.0211489200592041; #-vertex:177, #-edge:2857\n",
      "[graph 181] time:0.10537075996398926; #-vertex:171, #-edge:2359\n",
      "[graph 182] time:0.022877931594848633; #-vertex:170, #-edge:3162\n",
      "[graph 183] time:0.018177270889282227; #-vertex:177, #-edge:2717\n",
      "[graph 184] time:0.014968633651733398; #-vertex:169, #-edge:2347\n",
      "[graph 185] time:0.037801265716552734; #-vertex:175, #-edge:4259\n",
      "[graph 186] time:0.026002883911132812; #-vertex:171, #-edge:3391\n",
      "[graph 187] time:0.0165250301361084; #-vertex:174, #-edge:2482\n",
      "[graph 188] time:0.008622884750366211; #-vertex:170, #-edge:1592\n",
      "[graph 189] time:0.022740602493286133; #-vertex:171, #-edge:3227\n",
      "[graph 190] time:0.03833484649658203; #-vertex:173, #-edge:3939\n",
      "[graph 191] time:0.026292085647583008; #-vertex:172, #-edge:3412\n",
      "[graph 192] time:0.01243281364440918; #-vertex:178, #-edge:2118\n",
      "[graph 193] time:0.024174213409423828; #-vertex:171, #-edge:2543\n",
      "[graph 194] time:0.0334630012512207; #-vertex:176, #-edge:3158\n",
      "[graph 195] time:0.03277993202209473; #-vertex:173, #-edge:3785\n",
      "[graph 196] time:0.024532794952392578; #-vertex:174, #-edge:3326\n",
      "[graph 197] time:0.023290157318115234; #-vertex:173, #-edge:3219\n",
      "[graph 198] time:0.026226043701171875; #-vertex:170, #-edge:3486\n",
      "[graph 199] time:0.009413957595825195; #-vertex:186, #-edge:1550\n",
      "[graph 200] time:0.018385887145996094; #-vertex:169, #-edge:2853\n",
      "[graph 201] time:0.014437198638916016; #-vertex:185, #-edge:1849\n",
      "[graph 202] time:0.030412673950195312; #-vertex:175, #-edge:3757\n",
      "[graph 203] time:0.03398871421813965; #-vertex:173, #-edge:4031\n",
      "[graph 204] time:0.03384256362915039; #-vertex:174, #-edge:3460\n",
      "[graph 205] time:0.013152360916137695; #-vertex:181, #-edge:1979\n",
      "[graph 206] time:0.04063820838928223; #-vertex:176, #-edge:4454\n",
      "[graph 207] time:0.011621713638305664; #-vertex:177, #-edge:1647\n",
      "[graph 208] time:0.02185988426208496; #-vertex:173, #-edge:3163\n",
      "[graph 209] time:0.0194089412689209; #-vertex:179, #-edge:2841\n",
      "[graph 210] time:0.012117624282836914; #-vertex:179, #-edge:1899\n",
      "[graph 211] time:0.01825857162475586; #-vertex:166, #-edge:2724\n",
      "[graph 212] time:0.031237125396728516; #-vertex:177, #-edge:3825\n",
      "[graph 213] time:0.1159982681274414; #-vertex:173, #-edge:4955\n",
      "[graph 214] time:0.0200958251953125; #-vertex:178, #-edge:2710\n",
      "[graph 215] time:0.026813507080078125; #-vertex:170, #-edge:3402\n",
      "[graph 216] time:0.016027450561523438; #-vertex:175, #-edge:2045\n",
      "[graph 217] time:0.02539515495300293; #-vertex:170, #-edge:3442\n",
      "[graph 218] time:0.009605884552001953; #-vertex:188, #-edge:1456\n",
      "[graph 219] time:0.03347516059875488; #-vertex:171, #-edge:3109\n",
      "[graph 220] time:0.01118016242980957; #-vertex:171, #-edge:1173\n",
      "[graph 221] time:0.03377699851989746; #-vertex:169, #-edge:3129\n",
      "[graph 222] time:0.049082279205322266; #-vertex:179, #-edge:3905\n",
      "[graph 223] time:0.03534579277038574; #-vertex:174, #-edge:3358\n",
      "[graph 224] time:0.021720170974731445; #-vertex:165, #-edge:2423\n",
      "[graph 225] time:0.03546452522277832; #-vertex:173, #-edge:3339\n",
      "[graph 226] time:0.02657032012939453; #-vertex:178, #-edge:2672\n",
      "[graph 227] time:0.02114558219909668; #-vertex:166, #-edge:2382\n",
      "[graph 228] time:0.021262645721435547; #-vertex:178, #-edge:2180\n",
      "[graph 229] time:0.026618242263793945; #-vertex:169, #-edge:2785\n",
      "[graph 230] time:0.03375506401062012; #-vertex:168, #-edge:3170\n",
      "[graph 231] time:0.03906369209289551; #-vertex:181, #-edge:2883\n",
      "[graph 232] time:0.030608654022216797; #-vertex:169, #-edge:3087\n",
      "[graph 233] time:0.024115324020385742; #-vertex:170, #-edge:2614\n",
      "[graph 234] time:0.047705888748168945; #-vertex:172, #-edge:4010\n",
      "[graph 235] time:0.017520666122436523; #-vertex:175, #-edge:1775\n",
      "[graph 236] time:0.01904773712158203; #-vertex:170, #-edge:2152\n",
      "[graph 237] time:0.016635417938232422; #-vertex:170, #-edge:1922\n",
      "[graph 238] time:0.04955029487609863; #-vertex:172, #-edge:4136\n",
      "[graph 239] time:0.028455018997192383; #-vertex:173, #-edge:2845\n",
      "[graph 240] time:0.018259525299072266; #-vertex:174, #-edge:1898\n",
      "[graph 241] time:0.048149824142456055; #-vertex:174, #-edge:3976\n",
      "[graph 242] time:0.03546023368835449; #-vertex:173, #-edge:3289\n",
      "[graph 243] time:0.021881818771362305; #-vertex:184, #-edge:2160\n",
      "[graph 244] time:0.02814626693725586; #-vertex:174, #-edge:2844\n",
      "[graph 245] time:0.014300823211669922; #-vertex:173, #-edge:1481\n",
      "[graph 246] time:0.15119528770446777; #-vertex:172, #-edge:3514\n",
      "[graph 247] time:0.029868364334106445; #-vertex:174, #-edge:2854\n",
      "[graph 248] time:0.01547551155090332; #-vertex:180, #-edge:1588\n",
      "[graph 249] time:0.017576932907104492; #-vertex:173, #-edge:1885\n",
      "[graph 250] time:0.02883768081665039; #-vertex:172, #-edge:2852\n",
      "[graph 251] time:0.03399658203125; #-vertex:177, #-edge:3177\n",
      "[graph 252] time:0.03079056739807129; #-vertex:169, #-edge:3467\n",
      "[graph 253] time:0.032277584075927734; #-vertex:177, #-edge:3253\n",
      "[graph 254] time:0.025129079818725586; #-vertex:174, #-edge:3284\n",
      "[graph 255] time:0.010445117950439453; #-vertex:181, #-edge:1709\n",
      "[graph 256] time:0.017716169357299805; #-vertex:171, #-edge:2493\n",
      "[graph 257] time:0.022280454635620117; #-vertex:177, #-edge:3059\n",
      "[graph 258] time:0.03253817558288574; #-vertex:171, #-edge:3859\n",
      "[graph 259] time:0.03246712684631348; #-vertex:174, #-edge:3542\n",
      "[graph 260] time:0.025830984115600586; #-vertex:175, #-edge:3377\n",
      "[graph 261] time:0.03651285171508789; #-vertex:172, #-edge:4136\n",
      "[graph 262] time:0.025529861450195312; #-vertex:172, #-edge:3414\n",
      "[graph 263] time:0.03460836410522461; #-vertex:179, #-edge:3771\n",
      "[graph 264] time:0.022737741470336914; #-vertex:173, #-edge:3235\n",
      "[graph 265] time:0.024789810180664062; #-vertex:172, #-edge:3000\n",
      "[graph 266] time:0.015571117401123047; #-vertex:172, #-edge:2510\n",
      "[graph 267] time:0.012497663497924805; #-vertex:181, #-edge:2037\n",
      "[graph 268] time:0.01505136489868164; #-vertex:176, #-edge:2482\n",
      "[graph 269] time:0.008002758026123047; #-vertex:181, #-edge:1419\n",
      "[graph 270] time:0.010249137878417969; #-vertex:177, #-edge:1867\n",
      "[graph 271] time:0.01875472068786621; #-vertex:172, #-edge:2792\n",
      "[graph 272] time:0.00935673713684082; #-vertex:169, #-edge:1717\n",
      "[graph 273] time:0.030676841735839844; #-vertex:169, #-edge:3741\n",
      "[graph 274] time:0.010518312454223633; #-vertex:182, #-edge:1750\n",
      "[graph 275] time:0.012396812438964844; #-vertex:173, #-edge:2099\n",
      "[graph 276] time:0.01851344108581543; #-vertex:172, #-edge:2564\n",
      "[graph 277] time:0.009660482406616211; #-vertex:178, #-edge:1760\n",
      "[graph 278] time:0.008980035781860352; #-vertex:173, #-edge:1695\n",
      "[graph 279] time:0.02405571937561035; #-vertex:173, #-edge:3333\n",
      "[graph 280] time:0.012040376663208008; #-vertex:172, #-edge:2098\n",
      "[graph 281] time:0.014440536499023438; #-vertex:174, #-edge:2350\n",
      "[graph 282] time:0.009544849395751953; #-vertex:173, #-edge:1599\n",
      "[graph 283] time:0.09548830986022949; #-vertex:179, #-edge:3671\n",
      "[graph 284] time:0.021230220794677734; #-vertex:168, #-edge:3014\n",
      "[graph 285] time:0.024425983428955078; #-vertex:170, #-edge:3286\n",
      "[graph 286] time:0.01716303825378418; #-vertex:181, #-edge:2601\n",
      "[graph 287] time:0.03144383430480957; #-vertex:168, #-edge:3808\n",
      "[graph 288] time:0.021175384521484375; #-vertex:177, #-edge:2951\n",
      "[graph 289] time:0.034140586853027344; #-vertex:176, #-edge:3712\n",
      "[graph 290] time:0.012641429901123047; #-vertex:179, #-edge:1989\n",
      "[graph 291] time:0.008204460144042969; #-vertex:172, #-edge:1582\n",
      "[graph 292] time:0.00981903076171875; #-vertex:184, #-edge:1602\n",
      "[graph 293] time:0.00840306282043457; #-vertex:177, #-edge:1537\n",
      "[graph 294] time:0.02181386947631836; #-vertex:171, #-edge:3151\n",
      "[graph 295] time:0.028028011322021484; #-vertex:176, #-edge:3486\n",
      "[graph 296] time:0.017149686813354492; #-vertex:165, #-edge:2535\n",
      "[graph 297] time:0.03221273422241211; #-vertex:177, #-edge:3909\n",
      "[graph 298] time:0.03772139549255371; #-vertex:173, #-edge:4061\n",
      "[graph 299] time:0.023657560348510742; #-vertex:174, #-edge:3256\n",
      "[graph 300] time:0.03489041328430176; #-vertex:178, #-edge:4024\n",
      "[graph 301] time:0.012600898742675781; #-vertex:170, #-edge:2186\n",
      "[graph 302] time:0.020733356475830078; #-vertex:172, #-edge:3060\n",
      "[graph 303] time:0.017702817916870117; #-vertex:175, #-edge:2633\n",
      "[graph 304] time:0.01355743408203125; #-vertex:166, #-edge:2326\n",
      "[graph 305] time:0.016864776611328125; #-vertex:174, #-edge:2608\n",
      "[graph 306] time:0.022945880889892578; #-vertex:169, #-edge:2961\n",
      "[graph 307] time:0.012623071670532227; #-vertex:175, #-edge:2115\n",
      "[graph 308] time:0.007657527923583984; #-vertex:166, #-edge:1510\n",
      "[graph 309] time:0.02095341682434082; #-vertex:173, #-edge:3001\n",
      "[graph 310] time:0.024456262588500977; #-vertex:176, #-edge:3280\n",
      "[graph 311] time:0.044751644134521484; #-vertex:174, #-edge:4402\n",
      "[graph 312] time:0.028382539749145508; #-vertex:174, #-edge:3580\n",
      "[graph 313] time:0.030309677124023438; #-vertex:168, #-edge:3484\n",
      "[graph 314] time:0.013574838638305664; #-vertex:177, #-edge:2201\n",
      "[graph 315] time:0.08839964866638184; #-vertex:174, #-edge:3926\n",
      "[graph 316] time:0.01566314697265625; #-vertex:171, #-edge:2509\n",
      "[graph 317] time:0.010251760482788086; #-vertex:175, #-edge:1765\n",
      "[graph 318] time:0.018024682998657227; #-vertex:171, #-edge:2815\n",
      "[graph 319] time:0.03586864471435547; #-vertex:172, #-edge:3938\n",
      "[graph 320] time:0.02103281021118164; #-vertex:178, #-edge:2998\n",
      "[graph 321] time:0.012276411056518555; #-vertex:170, #-edge:2102\n",
      "[graph 322] time:0.03706216812133789; #-vertex:175, #-edge:3947\n",
      "[graph 323] time:0.026190757751464844; #-vertex:172, #-edge:3338\n",
      "[graph 324] time:0.03683924674987793; #-vertex:175, #-edge:4231\n",
      "[graph 325] time:0.01866316795349121; #-vertex:174, #-edge:2790\n",
      "[graph 326] time:0.04839468002319336; #-vertex:176, #-edge:4632\n",
      "[graph 327] time:0.03540349006652832; #-vertex:168, #-edge:4188\n",
      "[graph 328] time:0.023624897003173828; #-vertex:178, #-edge:3152\n",
      "[graph 329] time:0.011754035949707031; #-vertex:184, #-edge:1730\n",
      "[graph 330] time:0.010241985321044922; #-vertex:180, #-edge:1628\n",
      "[graph 331] time:0.013778924942016602; #-vertex:171, #-edge:2311\n",
      "[graph 332] time:0.010778427124023438; #-vertex:184, #-edge:1612\n",
      "[graph 333] time:0.03512120246887207; #-vertex:173, #-edge:4085\n",
      "[graph 334] time:0.039388418197631836; #-vertex:173, #-edge:4045\n",
      "[graph 335] time:0.01839900016784668; #-vertex:173, #-edge:2733\n",
      "[graph 336] time:0.012263298034667969; #-vertex:181, #-edge:1989\n",
      "[graph 337] time:0.010783672332763672; #-vertex:179, #-edge:1705\n",
      "[graph 338] time:0.008440971374511719; #-vertex:178, #-edge:1418\n",
      "[graph 339] time:0.01848459243774414; #-vertex:172, #-edge:2684\n",
      "[graph 340] time:0.04012751579284668; #-vertex:176, #-edge:4388\n",
      "[graph 341] time:0.034392356872558594; #-vertex:174, #-edge:3978\n",
      "[graph 342] time:0.029204130172729492; #-vertex:171, #-edge:3481\n",
      "[graph 343] time:0.017760276794433594; #-vertex:177, #-edge:2621\n",
      "[graph 344] time:0.028585195541381836; #-vertex:172, #-edge:3578\n",
      "[graph 345] time:0.03663969039916992; #-vertex:171, #-edge:4087\n",
      "[graph 346] time:0.10757231712341309; #-vertex:180, #-edge:3018\n",
      "[graph 347] time:0.020744800567626953; #-vertex:169, #-edge:2997\n",
      "[graph 348] time:0.024656295776367188; #-vertex:176, #-edge:3258\n",
      "[graph 349] time:0.010481834411621094; #-vertex:182, #-edge:1746\n",
      "[graph 350] time:0.0078008174896240234; #-vertex:166, #-edge:1468\n",
      "[graph 351] time:0.010596990585327148; #-vertex:173, #-edge:1853\n",
      "[graph 352] time:0.02039790153503418; #-vertex:165, #-edge:2705\n",
      "[graph 353] time:0.036188602447509766; #-vertex:179, #-edge:3771\n",
      "[graph 354] time:0.0370326042175293; #-vertex:174, #-edge:3976\n",
      "[graph 355] time:0.023743152618408203; #-vertex:171, #-edge:3151\n",
      "[graph 356] time:0.023842573165893555; #-vertex:172, #-edge:3186\n",
      "[graph 357] time:0.018085718154907227; #-vertex:176, #-edge:2694\n",
      "[graph 358] time:0.014194965362548828; #-vertex:170, #-edge:2328\n",
      "[graph 359] time:0.030643701553344727; #-vertex:172, #-edge:3560\n",
      "[graph 360] time:0.014442682266235352; #-vertex:171, #-edge:2213\n",
      "[graph 361] time:0.025338411331176758; #-vertex:173, #-edge:3371\n",
      "[graph 362] time:0.012011289596557617; #-vertex:184, #-edge:1884\n",
      "[graph 363] time:0.03519749641418457; #-vertex:170, #-edge:4166\n",
      "[graph 364] time:0.022220849990844727; #-vertex:176, #-edge:3066\n",
      "[graph 365] time:0.04470324516296387; #-vertex:171, #-edge:4695\n",
      "[graph 366] time:0.011925220489501953; #-vertex:179, #-edge:1903\n",
      "[graph 367] time:0.00895237922668457; #-vertex:176, #-edge:1576\n",
      "[graph 368] time:0.022472620010375977; #-vertex:178, #-edge:3146\n",
      "[graph 369] time:0.007888555526733398; #-vertex:168, #-edge:1470\n",
      "[graph 370] time:0.018047809600830078; #-vertex:168, #-edge:2814\n",
      "[graph 371] time:0.023969411849975586; #-vertex:178, #-edge:3216\n",
      "[graph 372] time:0.032738447189331055; #-vertex:170, #-edge:4022\n",
      "[graph 373] time:0.023343801498413086; #-vertex:179, #-edge:2919\n",
      "[graph 374] time:0.01159358024597168; #-vertex:179, #-edge:1893\n",
      "[graph 375] time:0.01707005500793457; #-vertex:177, #-edge:2619\n",
      "[graph 376] time:0.022687673568725586; #-vertex:173, #-edge:3147\n",
      "[graph 377] time:0.03085160255432129; #-vertex:170, #-edge:3854\n",
      "[graph 378] time:0.01413583755493164; #-vertex:175, #-edge:2261\n",
      "[graph 379] time:0.0305173397064209; #-vertex:172, #-edge:3830\n",
      "[graph 380] time:0.08849644660949707; #-vertex:178, #-edge:3198\n",
      "[graph 381] time:0.015700340270996094; #-vertex:172, #-edge:2014\n",
      "[graph 382] time:0.02403116226196289; #-vertex:174, #-edge:3332\n",
      "[graph 383] time:0.014081716537475586; #-vertex:171, #-edge:2217\n",
      "[graph 384] time:0.0356287956237793; #-vertex:175, #-edge:3757\n",
      "[graph 385] time:0.03817892074584961; #-vertex:171, #-edge:4167\n",
      "[graph 386] time:0.024619340896606445; #-vertex:176, #-edge:3274\n",
      "[graph 387] time:0.0423893928527832; #-vertex:176, #-edge:4268\n",
      "[graph 388] time:0.016477108001708984; #-vertex:166, #-edge:2606\n",
      "[graph 389] time:0.015099048614501953; #-vertex:176, #-edge:2402\n",
      "[graph 390] time:0.033275604248046875; #-vertex:172, #-edge:3992\n",
      "[graph 391] time:0.020727872848510742; #-vertex:175, #-edge:3005\n",
      "[graph 392] time:0.011540651321411133; #-vertex:169, #-edge:1439\n",
      "[graph 393] time:0.007122039794921875; #-vertex:173, #-edge:1353\n",
      "[graph 394] time:0.010589361190795898; #-vertex:179, #-edge:1835\n",
      "[graph 395] time:0.0279233455657959; #-vertex:169, #-edge:3665\n",
      "[graph 396] time:0.024973154067993164; #-vertex:171, #-edge:3383\n",
      "[graph 397] time:0.013659000396728516; #-vertex:178, #-edge:2158\n",
      "[graph 398] time:0.03687787055969238; #-vertex:176, #-edge:4160\n",
      "[graph 399] time:0.010804891586303711; #-vertex:169, #-edge:1973\n",
      "[graph 400] time:0.03900647163391113; #-vertex:175, #-edge:4139\n",
      "[graph 401] time:0.02627277374267578; #-vertex:171, #-edge:3459\n",
      "[graph 402] time:0.024542570114135742; #-vertex:175, #-edge:3257\n",
      "[graph 403] time:0.033453941345214844; #-vertex:177, #-edge:3945\n",
      "[graph 404] time:0.01595330238342285; #-vertex:174, #-edge:2478\n",
      "[graph 405] time:0.010714054107666016; #-vertex:181, #-edge:1821\n",
      "[graph 406] time:0.029046297073364258; #-vertex:174, #-edge:3726\n",
      "[graph 407] time:0.01936483383178711; #-vertex:172, #-edge:2712\n",
      "[graph 408] time:0.008056879043579102; #-vertex:180, #-edge:1336\n",
      "[graph 409] time:0.008861780166625977; #-vertex:171, #-edge:1565\n",
      "[graph 410] time:0.03086090087890625; #-vertex:170, #-edge:3786\n",
      "[graph 411] time:0.01981067657470703; #-vertex:177, #-edge:2857\n",
      "[graph 412] time:0.08041238784790039; #-vertex:166, #-edge:3534\n",
      "[graph 413] time:0.008638381958007812; #-vertex:188, #-edge:1350\n",
      "[graph 414] time:0.011474370956420898; #-vertex:180, #-edge:2020\n",
      "[graph 415] time:0.013118982315063477; #-vertex:169, #-edge:2151\n",
      "[graph 416] time:0.02884364128112793; #-vertex:177, #-edge:3681\n",
      "[graph 417] time:0.012727022171020508; #-vertex:185, #-edge:1581\n",
      "[graph 418] time:0.012386322021484375; #-vertex:173, #-edge:1893\n",
      "[graph 419] time:0.04170966148376465; #-vertex:169, #-edge:4541\n",
      "[graph 420] time:0.008392095565795898; #-vertex:186, #-edge:1336\n",
      "[graph 421] time:0.0232846736907959; #-vertex:175, #-edge:2929\n",
      "[graph 422] time:0.014966487884521484; #-vertex:170, #-edge:2410\n",
      "[graph 423] time:0.029013633728027344; #-vertex:167, #-edge:3687\n",
      "[graph 424] time:0.015428304672241211; #-vertex:180, #-edge:2372\n",
      "[graph 425] time:0.008293867111206055; #-vertex:169, #-edge:1543\n",
      "[graph 426] time:0.029602527618408203; #-vertex:173, #-edge:3557\n",
      "[graph 427] time:0.019281387329101562; #-vertex:169, #-edge:2785\n",
      "[graph 428] time:0.02450108528137207; #-vertex:166, #-edge:3372\n",
      "[graph 429] time:0.043557167053222656; #-vertex:173, #-edge:4475\n",
      "[graph 430] time:0.011776924133300781; #-vertex:178, #-edge:1826\n",
      "[graph 431] time:0.006970643997192383; #-vertex:167, #-edge:1207\n",
      "[graph 432] time:0.02402806282043457; #-vertex:169, #-edge:3349\n",
      "[graph 433] time:0.021003007888793945; #-vertex:178, #-edge:2960\n",
      "[graph 434] time:0.017169713973999023; #-vertex:162, #-edge:2520\n",
      "[graph 435] time:0.009149789810180664; #-vertex:173, #-edge:1627\n",
      "[graph 436] time:0.011065959930419922; #-vertex:182, #-edge:1914\n",
      "[graph 437] time:0.013101577758789062; #-vertex:171, #-edge:2279\n",
      "[graph 438] time:0.01629495620727539; #-vertex:176, #-edge:2484\n",
      "[graph 439] time:0.021399736404418945; #-vertex:171, #-edge:2663\n",
      "[graph 440] time:0.026379108428955078; #-vertex:170, #-edge:3466\n",
      "[graph 441] time:0.009488821029663086; #-vertex:175, #-edge:1585\n",
      "[graph 442] time:0.025171756744384766; #-vertex:175, #-edge:3371\n",
      "[graph 443] time:0.013122320175170898; #-vertex:174, #-edge:2202\n",
      "[graph 444] time:0.0061490535736083984; #-vertex:164, #-edge:1090\n",
      "[graph 445] time:0.017536163330078125; #-vertex:175, #-edge:2223\n",
      "[graph 446] time:0.01971602439880371; #-vertex:171, #-edge:2911\n",
      "[graph 447] time:0.021454811096191406; #-vertex:170, #-edge:3120\n",
      "[graph 448] time:0.028763771057128906; #-vertex:174, #-edge:3416\n",
      "[graph 449] time:0.07403278350830078; #-vertex:175, #-edge:2723\n",
      "[graph 450] time:0.011794567108154297; #-vertex:180, #-edge:2036\n",
      "[graph 451] time:0.01872730255126953; #-vertex:185, #-edge:2515\n",
      "[graph 452] time:0.011179208755493164; #-vertex:182, #-edge:1808\n",
      "[graph 453] time:0.018338441848754883; #-vertex:170, #-edge:2870\n",
      "[graph 454] time:0.021538257598876953; #-vertex:176, #-edge:3000\n",
      "[graph 455] time:0.021212100982666016; #-vertex:172, #-edge:2700\n",
      "[graph 456] time:0.01746964454650879; #-vertex:167, #-edge:2097\n",
      "[graph 457] time:0.020348310470581055; #-vertex:172, #-edge:2966\n",
      "[graph 458] time:0.014480113983154297; #-vertex:171, #-edge:2415\n",
      "[graph 459] time:0.016056537628173828; #-vertex:173, #-edge:2483\n",
      "[graph 460] time:0.016978025436401367; #-vertex:169, #-edge:2711\n",
      "[graph 461] time:0.023118257522583008; #-vertex:173, #-edge:3167\n",
      "[graph 462] time:0.012189626693725586; #-vertex:183, #-edge:1855\n",
      "[graph 463] time:0.012211799621582031; #-vertex:175, #-edge:2121\n",
      "[graph 464] time:0.02445363998413086; #-vertex:169, #-edge:3121\n",
      "[graph 465] time:0.02117300033569336; #-vertex:173, #-edge:3037\n",
      "[graph 466] time:0.019173860549926758; #-vertex:175, #-edge:2763\n",
      "[graph 467] time:0.009676456451416016; #-vertex:169, #-edge:1777\n",
      "[graph 468] time:0.0377810001373291; #-vertex:170, #-edge:4262\n",
      "[graph 469] time:0.030701160430908203; #-vertex:175, #-edge:3583\n",
      "[graph 470] time:0.029491662979125977; #-vertex:174, #-edge:3700\n",
      "[graph 471] time:0.013373851776123047; #-vertex:181, #-edge:1491\n",
      "[graph 472] time:0.012966156005859375; #-vertex:179, #-edge:2147\n",
      "[graph 473] time:0.033377647399902344; #-vertex:174, #-edge:3978\n",
      "[graph 474] time:0.012996673583984375; #-vertex:171, #-edge:2071\n",
      "[graph 475] time:0.014901161193847656; #-vertex:183, #-edge:2319\n",
      "[graph 476] time:0.03161120414733887; #-vertex:173, #-edge:3705\n",
      "[graph 477] time:0.028980731964111328; #-vertex:173, #-edge:3579\n",
      "[graph 478] time:0.028398990631103516; #-vertex:172, #-edge:3470\n",
      "[graph 479] time:0.024798870086669922; #-vertex:176, #-edge:3274\n",
      "[graph 480] time:0.027883052825927734; #-vertex:171, #-edge:3541\n",
      "[graph 481] time:0.018507003784179688; #-vertex:175, #-edge:2715\n",
      "[graph 482] time:0.12490606307983398; #-vertex:176, #-edge:4202\n",
      "[graph 483] time:0.01679229736328125; #-vertex:172, #-edge:2546\n",
      "[graph 484] time:0.017381906509399414; #-vertex:170, #-edge:2680\n",
      "[graph 485] time:0.01015329360961914; #-vertex:169, #-edge:1777\n",
      "[graph 486] time:0.014612436294555664; #-vertex:179, #-edge:2253\n",
      "[graph 487] time:0.009931325912475586; #-vertex:179, #-edge:1727\n",
      "[graph 488] time:0.02873516082763672; #-vertex:173, #-edge:3547\n",
      "[graph 489] time:0.044836997985839844; #-vertex:173, #-edge:4061\n",
      "[graph 490] time:0.03747868537902832; #-vertex:172, #-edge:3968\n",
      "[graph 491] time:0.010204792022705078; #-vertex:185, #-edge:1541\n",
      "[graph 492] time:0.032518625259399414; #-vertex:177, #-edge:3883\n",
      "[graph 493] time:0.008385419845581055; #-vertex:178, #-edge:1354\n",
      "[graph 494] time:0.019688129425048828; #-vertex:172, #-edge:2730\n",
      "[graph 495] time:0.03757214546203613; #-vertex:172, #-edge:4198\n",
      "[graph 496] time:0.01697540283203125; #-vertex:179, #-edge:2509\n",
      "[graph 497] time:0.01844167709350586; #-vertex:167, #-edge:2769\n",
      "[graph 498] time:0.011762857437133789; #-vertex:181, #-edge:1301\n",
      "[graph 499] time:0.008564233779907227; #-vertex:177, #-edge:1531\n",
      "[graph 500] time:0.01884770393371582; #-vertex:171, #-edge:2843\n",
      "[graph 501] time:0.03219437599182129; #-vertex:171, #-edge:3883\n",
      "[graph 502] time:0.011985301971435547; #-vertex:174, #-edge:1964\n",
      "[graph 503] time:0.03570103645324707; #-vertex:174, #-edge:3978\n",
      "[graph 504] time:0.022835731506347656; #-vertex:172, #-edge:3024\n",
      "[graph 505] time:0.020038366317749023; #-vertex:176, #-edge:2904\n",
      "[graph 506] time:0.020576000213623047; #-vertex:167, #-edge:2543\n",
      "[graph 507] time:0.015530109405517578; #-vertex:171, #-edge:2451\n",
      "[graph 508] time:0.030797719955444336; #-vertex:177, #-edge:3169\n",
      "[graph 509] time:0.024853229522705078; #-vertex:172, #-edge:2960\n",
      "[graph 510] time:0.03381943702697754; #-vertex:169, #-edge:3793\n",
      "[graph 511] time:0.010777950286865234; #-vertex:175, #-edge:1715\n",
      "[graph 512] time:0.014449119567871094; #-vertex:171, #-edge:2379\n",
      "[graph 513] time:0.019391298294067383; #-vertex:172, #-edge:2860\n",
      "[graph 514] time:0.027740955352783203; #-vertex:175, #-edge:3473\n",
      "[graph 515] time:0.024502038955688477; #-vertex:172, #-edge:3306\n",
      "[graph 516] time:0.012677907943725586; #-vertex:171, #-edge:2155\n",
      "[graph 517] time:0.11206674575805664; #-vertex:175, #-edge:3647\n",
      "[graph 518] time:0.023328065872192383; #-vertex:171, #-edge:3199\n",
      "[graph 519] time:0.02843308448791504; #-vertex:169, #-edge:3261\n",
      "[graph 520] time:0.02629995346069336; #-vertex:175, #-edge:3381\n",
      "[graph 521] time:0.029784202575683594; #-vertex:177, #-edge:3681\n",
      "[graph 522] time:0.02025604248046875; #-vertex:173, #-edge:2513\n",
      "[graph 523] time:0.020446300506591797; #-vertex:174, #-edge:2878\n",
      "[graph 524] time:0.025216341018676758; #-vertex:173, #-edge:3339\n",
      "[graph 525] time:0.012893438339233398; #-vertex:176, #-edge:1512\n",
      "[graph 526] time:0.017597198486328125; #-vertex:178, #-edge:2606\n",
      "[graph 527] time:0.031009435653686523; #-vertex:171, #-edge:3787\n",
      "[graph 528] time:0.023118019104003906; #-vertex:178, #-edge:3056\n",
      "[graph 529] time:0.03953671455383301; #-vertex:174, #-edge:4258\n",
      "[graph 530] time:0.012015342712402344; #-vertex:180, #-edge:1952\n",
      "[graph 531] time:0.033803462982177734; #-vertex:176, #-edge:4014\n",
      "[graph 532] time:0.021098852157592773; #-vertex:173, #-edge:3031\n",
      "[graph 533] time:0.03217291831970215; #-vertex:175, #-edge:3929\n",
      "[graph 534] time:0.028269529342651367; #-vertex:172, #-edge:3614\n",
      "[graph 535] time:0.03697347640991211; #-vertex:172, #-edge:4136\n",
      "[graph 536] time:0.02460455894470215; #-vertex:178, #-edge:3264\n",
      "[graph 537] time:0.011261463165283203; #-vertex:176, #-edge:1918\n",
      "[graph 538] time:0.036974191665649414; #-vertex:178, #-edge:4006\n",
      "[graph 539] time:0.027565479278564453; #-vertex:169, #-edge:3635\n",
      "[graph 540] time:0.014371395111083984; #-vertex:174, #-edge:2316\n",
      "[graph 541] time:0.03163337707519531; #-vertex:172, #-edge:3772\n",
      "[graph 542] time:0.036484479904174805; #-vertex:174, #-edge:4186\n",
      "[graph 543] time:0.011010885238647461; #-vertex:182, #-edge:1832\n",
      "[graph 544] time:0.012280702590942383; #-vertex:177, #-edge:2047\n",
      "[graph 545] time:0.014091968536376953; #-vertex:179, #-edge:2335\n",
      "[graph 546] time:0.014136314392089844; #-vertex:171, #-edge:2359\n",
      "[graph 547] time:0.031764984130859375; #-vertex:171, #-edge:3859\n",
      "[graph 548] time:0.08236002922058105; #-vertex:176, #-edge:2620\n",
      "[graph 549] time:0.03029489517211914; #-vertex:173, #-edge:3339\n",
      "[graph 550] time:0.0170896053314209; #-vertex:172, #-edge:2618\n",
      "[graph 551] time:0.026246309280395508; #-vertex:176, #-edge:3250\n",
      "[graph 552] time:0.012522697448730469; #-vertex:163, #-edge:2183\n",
      "[graph 553] time:0.029720067977905273; #-vertex:177, #-edge:3681\n",
      "[graph 554] time:0.027676820755004883; #-vertex:173, #-edge:3615\n",
      "[graph 555] time:0.0282590389251709; #-vertex:178, #-edge:3566\n",
      "[graph 556] time:0.03934955596923828; #-vertex:174, #-edge:4402\n",
      "[graph 557] time:0.013974189758300781; #-vertex:168, #-edge:2326\n",
      "[graph 558] time:0.025922060012817383; #-vertex:176, #-edge:3280\n",
      "[graph 559] time:0.03397536277770996; #-vertex:173, #-edge:3973\n",
      "[graph 560] time:0.008906364440917969; #-vertex:174, #-edge:1570\n",
      "[graph 561] time:0.007233858108520508; #-vertex:177, #-edge:1239\n",
      "[graph 562] time:0.022218704223632812; #-vertex:171, #-edge:2743\n",
      "[graph 563] time:0.008841514587402344; #-vertex:181, #-edge:1427\n",
      "[graph 564] time:0.008344650268554688; #-vertex:175, #-edge:1551\n",
      "[graph 565] time:0.03545570373535156; #-vertex:169, #-edge:4151\n",
      "[graph 566] time:0.008544921875; #-vertex:172, #-edge:1496\n",
      "[graph 567] time:0.022213459014892578; #-vertex:173, #-edge:2975\n",
      "[graph 568] time:0.03152608871459961; #-vertex:167, #-edge:3633\n",
      "[graph 569] time:0.03438115119934082; #-vertex:173, #-edge:4061\n",
      "[graph 570] time:0.021744728088378906; #-vertex:174, #-edge:2968\n",
      "[graph 571] time:0.027927875518798828; #-vertex:173, #-edge:3369\n",
      "[graph 572] time:0.024590015411376953; #-vertex:172, #-edge:3328\n",
      "[graph 573] time:0.03719043731689453; #-vertex:171, #-edge:4053\n",
      "[graph 574] time:0.009517431259155273; #-vertex:172, #-edge:1598\n",
      "[graph 575] time:0.008704900741577148; #-vertex:166, #-edge:1546\n",
      "[graph 576] time:0.030031442642211914; #-vertex:178, #-edge:3646\n",
      "[graph 577] time:0.03981590270996094; #-vertex:176, #-edge:4352\n",
      "[graph 578] time:0.02913379669189453; #-vertex:173, #-edge:3461\n",
      "[graph 579] time:0.11032509803771973; #-vertex:171, #-edge:3031\n",
      "[graph 580] time:0.021453380584716797; #-vertex:176, #-edge:2836\n",
      "[graph 581] time:0.019624948501586914; #-vertex:169, #-edge:2827\n",
      "[graph 582] time:0.019416093826293945; #-vertex:176, #-edge:2738\n",
      "[graph 583] time:0.028492450714111328; #-vertex:173, #-edge:3615\n",
      "[graph 584] time:0.0267484188079834; #-vertex:174, #-edge:3466\n",
      "[graph 585] time:0.029701709747314453; #-vertex:168, #-edge:3664\n",
      "[graph 586] time:0.007980108261108398; #-vertex:178, #-edge:1214\n",
      "[graph 587] time:0.01641082763671875; #-vertex:170, #-edge:2558\n",
      "[graph 588] time:0.011343717575073242; #-vertex:184, #-edge:1752\n",
      "[graph 589] time:0.0207211971282959; #-vertex:167, #-edge:2975\n",
      "[graph 590] time:0.022395849227905273; #-vertex:176, #-edge:3066\n",
      "[graph 591] time:0.025128602981567383; #-vertex:172, #-edge:3140\n",
      "[graph 592] time:0.01241922378540039; #-vertex:186, #-edge:1982\n",
      "[graph 593] time:0.012665271759033203; #-vertex:171, #-edge:2207\n",
      "[graph 594] time:0.013342857360839844; #-vertex:163, #-edge:2339\n",
      "[graph 595] time:0.013911247253417969; #-vertex:178, #-edge:2280\n",
      "[graph 596] time:0.016987085342407227; #-vertex:185, #-edge:2431\n",
      "[graph 597] time:0.01080465316772461; #-vertex:170, #-edge:1976\n",
      "[graph 598] time:0.019153356552124023; #-vertex:172, #-edge:2910\n",
      "[graph 599] time:0.029750585556030273; #-vertex:176, #-edge:3706\n",
      "[graph 600] time:0.01029348373413086; #-vertex:177, #-edge:1763\n",
      "[graph 601] time:0.010147571563720703; #-vertex:183, #-edge:1703\n",
      "[graph 602] time:0.02237105369567871; #-vertex:172, #-edge:2714\n",
      "[graph 603] time:0.014222145080566406; #-vertex:175, #-edge:2231\n",
      "[graph 604] time:0.016841888427734375; #-vertex:170, #-edge:2690\n",
      "[graph 605] time:0.0226900577545166; #-vertex:176, #-edge:3058\n",
      "[graph 606] time:0.03217792510986328; #-vertex:168, #-edge:3934\n",
      "[graph 607] time:0.0281374454498291; #-vertex:178, #-edge:3566\n",
      "[graph 608] time:0.01179647445678711; #-vertex:168, #-edge:2098\n",
      "[graph 609] time:0.02647233009338379; #-vertex:177, #-edge:3429\n",
      "[graph 610] time:0.01490473747253418; #-vertex:172, #-edge:2408\n",
      "[graph 611] time:0.03279447555541992; #-vertex:171, #-edge:3999\n",
      "[graph 612] time:0.024368762969970703; #-vertex:176, #-edge:2924\n",
      "[graph 613] time:0.018071889877319336; #-vertex:173, #-edge:2801\n",
      "[graph 614] time:0.08825373649597168; #-vertex:167, #-edge:3153\n",
      "[graph 615] time:0.030895709991455078; #-vertex:175, #-edge:3761\n",
      "[graph 616] time:0.021763086318969727; #-vertex:176, #-edge:3066\n",
      "[graph 617] time:0.03834414482116699; #-vertex:175, #-edge:4259\n",
      "[graph 618] time:0.03535199165344238; #-vertex:175, #-edge:4139\n",
      "[graph 619] time:0.0069811344146728516; #-vertex:167, #-edge:1269\n",
      "[graph 620] time:0.01345205307006836; #-vertex:174, #-edge:2266\n",
      "[graph 621] time:0.02999734878540039; #-vertex:169, #-edge:3671\n",
      "[graph 622] time:0.01803731918334961; #-vertex:177, #-edge:2709\n",
      "[graph 623] time:0.008633852005004883; #-vertex:173, #-edge:1555\n",
      "[graph 624] time:0.035855770111083984; #-vertex:173, #-edge:4085\n",
      "[graph 625] time:0.028945207595825195; #-vertex:169, #-edge:3583\n",
      "[graph 626] time:0.02641749382019043; #-vertex:175, #-edge:3461\n",
      "[graph 627] time:0.01240992546081543; #-vertex:182, #-edge:1810\n",
      "[graph 628] time:0.009383440017700195; #-vertex:178, #-edge:1694\n",
      "[graph 629] time:0.027498245239257812; #-vertex:169, #-edge:3635\n",
      "[graph 630] time:0.011153459548950195; #-vertex:175, #-edge:1863\n",
      "[graph 631] time:0.025379180908203125; #-vertex:172, #-edge:3488\n",
      "[graph 632] time:0.015200138092041016; #-vertex:173, #-edge:2297\n",
      "[graph 633] time:0.032561302185058594; #-vertex:171, #-edge:3991\n",
      "[graph 634] time:0.029374122619628906; #-vertex:177, #-edge:3671\n",
      "[graph 635] time:0.01233220100402832; #-vertex:172, #-edge:2120\n",
      "[graph 636] time:0.018096923828125; #-vertex:173, #-edge:2789\n",
      "[graph 637] time:0.03525400161743164; #-vertex:173, #-edge:4031\n",
      "[graph 638] time:0.013160943984985352; #-vertex:179, #-edge:2173\n",
      "[graph 639] time:0.024453163146972656; #-vertex:173, #-edge:3081\n",
      "[graph 640] time:0.03368115425109863; #-vertex:174, #-edge:3974\n",
      "[graph 641] time:0.013817548751831055; #-vertex:168, #-edge:2328\n",
      "[graph 642] time:0.009285926818847656; #-vertex:171, #-edge:1691\n",
      "[graph 643] time:0.01220083236694336; #-vertex:166, #-edge:2120\n",
      "[graph 644] time:0.023955821990966797; #-vertex:174, #-edge:3196\n",
      "[graph 645] time:0.027060270309448242; #-vertex:171, #-edge:3481\n",
      "[graph 646] time:0.09108567237854004; #-vertex:175, #-edge:4013\n",
      "[graph 647] time:0.008658409118652344; #-vertex:170, #-edge:1446\n",
      "[graph 648] time:0.029773473739624023; #-vertex:173, #-edge:3761\n",
      "[graph 649] time:0.009612798690795898; #-vertex:181, #-edge:1687\n",
      "[graph 650] time:0.009307861328125; #-vertex:173, #-edge:1739\n",
      "[graph 651] time:0.021038055419921875; #-vertex:172, #-edge:3086\n",
      "[graph 652] time:0.018563270568847656; #-vertex:169, #-edge:2839\n",
      "[graph 653] time:0.01573777198791504; #-vertex:175, #-edge:2485\n",
      "[graph 654] time:0.020259857177734375; #-vertex:175, #-edge:2805\n",
      "[graph 655] time:0.020308971405029297; #-vertex:173, #-edge:2889\n",
      "[graph 656] time:0.024002790451049805; #-vertex:176, #-edge:3308\n",
      "[graph 657] time:0.01458883285522461; #-vertex:181, #-edge:2053\n",
      "[graph 658] time:0.018251419067382812; #-vertex:184, #-edge:2520\n",
      "[graph 659] time:0.023939132690429688; #-vertex:173, #-edge:3147\n",
      "[graph 660] time:0.024415016174316406; #-vertex:174, #-edge:2886\n",
      "[graph 661] time:0.019135236740112305; #-vertex:172, #-edge:2776\n",
      "[graph 662] time:0.02916860580444336; #-vertex:175, #-edge:3605\n",
      "[graph 663] time:0.015858173370361328; #-vertex:172, #-edge:2452\n",
      "[graph 664] time:0.021989107131958008; #-vertex:171, #-edge:3085\n",
      "[graph 665] time:0.027040481567382812; #-vertex:176, #-edge:3490\n",
      "[graph 666] time:0.010088443756103516; #-vertex:180, #-edge:1640\n",
      "[graph 667] time:0.01614665985107422; #-vertex:172, #-edge:2576\n",
      "[graph 668] time:0.034027099609375; #-vertex:175, #-edge:3887\n",
      "[graph 669] time:0.0238039493560791; #-vertex:175, #-edge:3179\n",
      "[graph 670] time:0.010468244552612305; #-vertex:179, #-edge:1689\n",
      "[graph 671] time:0.024416446685791016; #-vertex:173, #-edge:3255\n",
      "[graph 672] time:0.026259183883666992; #-vertex:176, #-edge:3388\n",
      "[graph 673] time:0.017354965209960938; #-vertex:171, #-edge:2643\n",
      "[graph 674] time:0.03025531768798828; #-vertex:175, #-edge:3637\n",
      "[graph 675] time:0.017305612564086914; #-vertex:169, #-edge:2643\n",
      "[graph 676] time:0.027713537216186523; #-vertex:175, #-edge:3491\n",
      "[graph 677] time:0.027740955352783203; #-vertex:175, #-edge:3509\n",
      "[graph 678] time:0.026708364486694336; #-vertex:173, #-edge:3459\n",
      "[graph 679] time:0.03679847717285156; #-vertex:175, #-edge:4139\n",
      "[graph 680] time:0.11277127265930176; #-vertex:170, #-edge:2688\n",
      "[graph 681] time:0.037407875061035156; #-vertex:173, #-edge:4199\n",
      "[graph 682] time:0.016053199768066406; #-vertex:175, #-edge:2319\n",
      "[graph 683] time:0.01407003402709961; #-vertex:178, #-edge:2216\n",
      "[graph 684] time:0.008414745330810547; #-vertex:174, #-edge:1512\n",
      "[graph 685] time:0.018252134323120117; #-vertex:170, #-edge:2774\n",
      "[graph 686] time:0.012544631958007812; #-vertex:174, #-edge:2044\n",
      "[graph 687] time:0.018108606338500977; #-vertex:175, #-edge:2633\n",
      "[graph 688] time:0.03616189956665039; #-vertex:173, #-edge:3973\n",
      "[graph 689] time:0.015190362930297852; #-vertex:171, #-edge:2349\n",
      "[graph 690] time:0.009048700332641602; #-vertex:172, #-edge:1588\n",
      "[graph 691] time:0.021246910095214844; #-vertex:175, #-edge:3055\n",
      "[graph 692] time:0.009266376495361328; #-vertex:178, #-edge:1572\n",
      "[graph 693] time:0.013487577438354492; #-vertex:181, #-edge:2159\n",
      "[graph 694] time:0.03404998779296875; #-vertex:174, #-edge:3646\n",
      "[graph 695] time:0.009386777877807617; #-vertex:164, #-edge:1680\n",
      "[graph 696] time:0.01914668083190918; #-vertex:171, #-edge:2843\n",
      "[graph 697] time:0.02901434898376465; #-vertex:170, #-edge:3304\n",
      "[graph 698] time:0.018961429595947266; #-vertex:177, #-edge:2757\n",
      "[graph 699] time:0.02693772315979004; #-vertex:173, #-edge:3421\n",
      "[graph 700] time:0.035921573638916016; #-vertex:171, #-edge:4203\n",
      "[graph 701] time:0.03285384178161621; #-vertex:178, #-edge:3562\n",
      "[graph 702] time:0.018652915954589844; #-vertex:172, #-edge:2830\n",
      "[graph 703] time:0.03049755096435547; #-vertex:169, #-edge:3735\n",
      "[graph 704] time:0.010635614395141602; #-vertex:186, #-edge:1550\n",
      "[graph 705] time:0.009997844696044922; #-vertex:180, #-edge:1716\n",
      "[graph 706] time:0.014111757278442383; #-vertex:176, #-edge:2314\n",
      "[graph 707] time:0.029574871063232422; #-vertex:176, #-edge:3722\n",
      "[graph 708] time:0.008418560028076172; #-vertex:180, #-edge:1336\n",
      "[graph 709] time:0.024135351181030273; #-vertex:174, #-edge:3044\n",
      "[graph 710] time:0.018792152404785156; #-vertex:171, #-edge:2799\n",
      "[graph 711] time:0.02140355110168457; #-vertex:173, #-edge:3027\n",
      "[graph 712] time:0.026047229766845703; #-vertex:174, #-edge:3062\n",
      "[graph 713] time:0.031403303146362305; #-vertex:174, #-edge:3850\n",
      "[graph 714] time:0.02908492088317871; #-vertex:169, #-edge:2959\n",
      "[graph 715] time:0.09507274627685547; #-vertex:173, #-edge:1713\n",
      "[graph 716] time:0.012325763702392578; #-vertex:173, #-edge:2101\n",
      "[graph 717] time:0.02704930305480957; #-vertex:169, #-edge:2945\n",
      "[graph 718] time:0.021276235580444336; #-vertex:176, #-edge:2944\n",
      "[graph 719] time:0.02628159523010254; #-vertex:168, #-edge:3400\n",
      "[graph 720] time:0.023586034774780273; #-vertex:174, #-edge:3156\n",
      "[graph 721] time:0.037143707275390625; #-vertex:174, #-edge:3978\n",
      "[graph 722] time:0.018711328506469727; #-vertex:178, #-edge:2764\n",
      "[graph 723] time:0.009158849716186523; #-vertex:179, #-edge:1613\n",
      "[graph 724] time:0.010535717010498047; #-vertex:180, #-edge:1796\n",
      "[graph 725] time:0.02612757682800293; #-vertex:172, #-edge:3290\n",
      "[graph 726] time:0.014719247817993164; #-vertex:170, #-edge:2420\n",
      "[graph 727] time:0.011927366256713867; #-vertex:173, #-edge:2055\n",
      "[graph 728] time:0.021436691284179688; #-vertex:176, #-edge:3064\n",
      "[graph 729] time:0.008696317672729492; #-vertex:178, #-edge:1506\n",
      "[graph 730] time:0.010647773742675781; #-vertex:182, #-edge:1710\n",
      "[graph 731] time:0.010580062866210938; #-vertex:184, #-edge:1700\n",
      "[graph 732] time:0.010493040084838867; #-vertex:180, #-edge:1710\n",
      "[graph 733] time:0.019604206085205078; #-vertex:175, #-edge:2891\n",
      "[graph 734] time:0.015465736389160156; #-vertex:181, #-edge:2457\n",
      "[graph 735] time:0.014904499053955078; #-vertex:171, #-edge:2477\n",
      "[graph 736] time:0.030792951583862305; #-vertex:177, #-edge:3785\n",
      "[graph 737] time:0.02889847755432129; #-vertex:168, #-edge:3676\n",
      "[graph 738] time:0.018027067184448242; #-vertex:184, #-edge:2690\n",
      "[graph 739] time:0.006970882415771484; #-vertex:176, #-edge:1170\n",
      "[graph 740] time:0.05027890205383301; #-vertex:174, #-edge:4902\n",
      "[graph 741] time:0.012731790542602539; #-vertex:174, #-edge:2132\n",
      "[graph 742] time:0.013327360153198242; #-vertex:179, #-edge:2165\n",
      "[graph 743] time:0.04834103584289551; #-vertex:175, #-edge:4453\n",
      "[graph 744] time:0.01090860366821289; #-vertex:169, #-edge:1895\n",
      "[graph 745] time:0.027842044830322266; #-vertex:173, #-edge:3605\n",
      "[graph 746] time:0.02961444854736328; #-vertex:175, #-edge:3719\n",
      "[graph 747] time:0.035108327865600586; #-vertex:169, #-edge:4091\n",
      "[graph 748] time:0.020920276641845703; #-vertex:175, #-edge:2715\n",
      "[graph 749] time:0.027289390563964844; #-vertex:176, #-edge:3388\n",
      "[graph 750] time:0.07483196258544922; #-vertex:174, #-edge:1654\n",
      "[graph 751] time:0.017071247100830078; #-vertex:183, #-edge:2429\n",
      "[graph 752] time:0.012051582336425781; #-vertex:174, #-edge:1972\n",
      "[graph 753] time:0.01751232147216797; #-vertex:174, #-edge:2724\n",
      "[graph 754] time:0.028261423110961914; #-vertex:169, #-edge:3591\n",
      "[graph 755] time:0.02018570899963379; #-vertex:180, #-edge:2768\n",
      "[graph 756] time:0.033397674560546875; #-vertex:169, #-edge:3901\n",
      "[graph 757] time:0.020911216735839844; #-vertex:175, #-edge:2963\n",
      "[graph 758] time:0.02862834930419922; #-vertex:165, #-edge:3715\n",
      "[graph 759] time:0.015084266662597656; #-vertex:176, #-edge:2336\n",
      "[graph 760] time:0.030217647552490234; #-vertex:169, #-edge:3791\n",
      "[graph 761] time:0.02671504020690918; #-vertex:171, #-edge:3481\n",
      "[graph 762] time:0.009884119033813477; #-vertex:188, #-edge:1596\n",
      "[graph 763] time:0.014569997787475586; #-vertex:173, #-edge:2451\n",
      "[graph 764] time:0.030637741088867188; #-vertex:179, #-edge:3797\n",
      "[graph 765] time:0.015327215194702148; #-vertex:170, #-edge:2526\n",
      "[graph 766] time:0.01845240592956543; #-vertex:172, #-edge:2802\n",
      "[graph 767] time:0.03287220001220703; #-vertex:173, #-edge:3799\n",
      "[graph 768] time:0.01168060302734375; #-vertex:170, #-edge:2008\n",
      "[graph 769] time:0.028872251510620117; #-vertex:176, #-edge:3664\n",
      "[graph 770] time:0.010344505310058594; #-vertex:170, #-edge:1822\n",
      "[graph 771] time:0.03987312316894531; #-vertex:174, #-edge:4402\n",
      "[graph 772] time:0.020154237747192383; #-vertex:171, #-edge:2991\n",
      "[graph 773] time:0.024484872817993164; #-vertex:177, #-edge:3307\n",
      "[graph 774] time:0.031846046447753906; #-vertex:173, #-edge:3785\n",
      "[graph 775] time:0.01985001564025879; #-vertex:169, #-edge:2955\n",
      "[graph 776] time:0.02358865737915039; #-vertex:175, #-edge:2993\n",
      "[graph 777] time:0.026317119598388672; #-vertex:173, #-edge:3431\n",
      "[graph 778] time:0.012073278427124023; #-vertex:177, #-edge:2119\n",
      "[graph 779] time:0.007210254669189453; #-vertex:176, #-edge:1170\n",
      "[graph 780] time:0.04748678207397461; #-vertex:177, #-edge:4365\n",
      "[graph 781] time:0.03297924995422363; #-vertex:173, #-edge:3785\n",
      "[graph 782] time:0.016198396682739258; #-vertex:172, #-edge:2502\n",
      "[graph 783] time:0.09353518486022949; #-vertex:175, #-edge:4111\n",
      "[graph 784] time:0.016015052795410156; #-vertex:172, #-edge:2544\n",
      "[graph 785] time:0.026628971099853516; #-vertex:170, #-edge:3458\n",
      "[graph 786] time:0.02561807632446289; #-vertex:178, #-edge:3346\n",
      "[graph 787] time:0.02066969871520996; #-vertex:171, #-edge:2663\n",
      "[graph 788] time:0.031099557876586914; #-vertex:175, #-edge:3711\n",
      "[graph 789] time:0.0228729248046875; #-vertex:169, #-edge:3125\n",
      "[graph 790] time:0.02289128303527832; #-vertex:176, #-edge:2676\n",
      "[graph 791] time:0.04423785209655762; #-vertex:174, #-edge:4402\n",
      "[graph 792] time:0.015371561050415039; #-vertex:173, #-edge:2347\n",
      "[graph 793] time:0.02230358123779297; #-vertex:174, #-edge:3070\n",
      "[graph 794] time:0.01166677474975586; #-vertex:170, #-edge:1574\n",
      "[graph 795] time:0.016145944595336914; #-vertex:168, #-edge:2600\n",
      "[graph 796] time:0.01466989517211914; #-vertex:174, #-edge:2274\n",
      "[graph 797] time:0.034488677978515625; #-vertex:176, #-edge:3968\n",
      "[graph 798] time:0.031120777130126953; #-vertex:168, #-edge:3832\n",
      "[graph 799] time:0.016437768936157227; #-vertex:179, #-edge:2433\n",
      "[graph 800] time:0.021423816680908203; #-vertex:172, #-edge:3026\n",
      "[graph 801] time:0.009423017501831055; #-vertex:167, #-edge:1695\n",
      "[graph 802] time:0.05792665481567383; #-vertex:175, #-edge:3887\n",
      "[graph 803] time:0.044585227966308594; #-vertex:168, #-edge:3328\n",
      "[graph 804] time:0.010200738906860352; #-vertex:176, #-edge:1716\n",
      "[graph 805] time:0.027681350708007812; #-vertex:172, #-edge:3560\n",
      "[graph 806] time:0.03696560859680176; #-vertex:177, #-edge:4123\n",
      "[graph 807] time:0.010922431945800781; #-vertex:171, #-edge:1757\n",
      "[graph 808] time:0.02732563018798828; #-vertex:173, #-edge:3091\n",
      "[graph 809] time:0.023098468780517578; #-vertex:172, #-edge:3174\n",
      "[graph 810] time:0.031013011932373047; #-vertex:177, #-edge:3349\n",
      "[graph 811] time:0.03626275062561035; #-vertex:174, #-edge:4114\n",
      "[graph 812] time:0.01564931869506836; #-vertex:171, #-edge:2415\n",
      "[graph 813] time:0.019954442977905273; #-vertex:173, #-edge:2833\n",
      "[graph 814] time:0.026354074478149414; #-vertex:174, #-edge:3334\n",
      "[graph 815] time:0.10363149642944336; #-vertex:171, #-edge:2343\n",
      "[graph 816] time:0.013463973999023438; #-vertex:178, #-edge:2190\n",
      "[graph 817] time:0.008330821990966797; #-vertex:176, #-edge:1532\n",
      "[graph 818] time:0.021668672561645508; #-vertex:170, #-edge:3120\n",
      "[graph 819] time:0.01263427734375; #-vertex:170, #-edge:2110\n",
      "[graph 820] time:0.022099018096923828; #-vertex:173, #-edge:3109\n",
      "[graph 821] time:0.024933576583862305; #-vertex:170, #-edge:3080\n",
      "[graph 822] time:0.021208524703979492; #-vertex:174, #-edge:2968\n",
      "[graph 823] time:0.03333306312561035; #-vertex:173, #-edge:3605\n",
      "[graph 824] time:0.030544042587280273; #-vertex:175, #-edge:3195\n",
      "[graph 825] time:0.01199197769165039; #-vertex:177, #-edge:2013\n",
      "[graph 826] time:0.009523630142211914; #-vertex:177, #-edge:1739\n",
      "[graph 827] time:0.009991168975830078; #-vertex:180, #-edge:1686\n",
      "[graph 828] time:0.03749203681945801; #-vertex:167, #-edge:4293\n",
      "[graph 829] time:0.02216506004333496; #-vertex:181, #-edge:2989\n",
      "[graph 830] time:0.014302730560302734; #-vertex:171, #-edge:2375\n",
      "[graph 831] time:0.02754950523376465; #-vertex:169, #-edge:3275\n",
      "[graph 832] time:0.012393712997436523; #-vertex:171, #-edge:2167\n",
      "[graph 833] time:0.03616142272949219; #-vertex:176, #-edge:4128\n",
      "[graph 834] time:0.01338505744934082; #-vertex:178, #-edge:2166\n",
      "[graph 835] time:0.008810281753540039; #-vertex:178, #-edge:1638\n",
      "[graph 836] time:0.017613887786865234; #-vertex:171, #-edge:2663\n",
      "[graph 837] time:0.03523683547973633; #-vertex:171, #-edge:4099\n",
      "[graph 838] time:0.020458459854125977; #-vertex:176, #-edge:2958\n",
      "[graph 839] time:0.016569137573242188; #-vertex:171, #-edge:2099\n",
      "[graph 840] time:0.03103041648864746; #-vertex:171, #-edge:3685\n",
      "[graph 841] time:0.010024547576904297; #-vertex:173, #-edge:1421\n",
      "[graph 842] time:0.012460708618164062; #-vertex:171, #-edge:2183\n",
      "[graph 843] time:0.030655384063720703; #-vertex:174, #-edge:3780\n",
      "[graph 844] time:0.010977745056152344; #-vertex:182, #-edge:1768\n",
      "[graph 845] time:0.014989376068115234; #-vertex:171, #-edge:2415\n",
      "[graph 846] time:0.016515493392944336; #-vertex:165, #-edge:2643\n",
      "[graph 847] time:0.01714611053466797; #-vertex:178, #-edge:2652\n",
      "[graph 848] time:0.021425485610961914; #-vertex:174, #-edge:2868\n",
      "[graph 849] time:0.03623151779174805; #-vertex:176, #-edge:4106\n",
      "[graph 850] time:0.09060239791870117; #-vertex:170, #-edge:3546\n",
      "[graph 851] time:0.010990142822265625; #-vertex:184, #-edge:1824\n",
      "[graph 852] time:0.009114265441894531; #-vertex:180, #-edge:1556\n",
      "[graph 853] time:0.014057159423828125; #-vertex:175, #-edge:1739\n",
      "[graph 854] time:0.013153791427612305; #-vertex:170, #-edge:2220\n",
      "[graph 855] time:0.008156538009643555; #-vertex:168, #-edge:1452\n",
      "[graph 856] time:0.029328107833862305; #-vertex:175, #-edge:3757\n",
      "[graph 857] time:0.03744935989379883; #-vertex:174, #-edge:4258\n",
      "[graph 858] time:0.012218236923217773; #-vertex:179, #-edge:1831\n",
      "[graph 859] time:0.0170440673828125; #-vertex:176, #-edge:2618\n",
      "[graph 860] time:0.016279935836791992; #-vertex:172, #-edge:2628\n",
      "[graph 861] time:0.024824142456054688; #-vertex:171, #-edge:3179\n",
      "[graph 862] time:0.02444934844970703; #-vertex:177, #-edge:3327\n",
      "[graph 863] time:0.02411031723022461; #-vertex:175, #-edge:3257\n",
      "[graph 864] time:0.02438044548034668; #-vertex:174, #-edge:3346\n",
      "[graph 865] time:0.04274249076843262; #-vertex:172, #-edge:4478\n",
      "[graph 866] time:0.02051258087158203; #-vertex:167, #-edge:2995\n",
      "[graph 867] time:0.025458574295043945; #-vertex:177, #-edge:3105\n",
      "[graph 868] time:0.03455638885498047; #-vertex:172, #-edge:3922\n",
      "[graph 869] time:0.020720720291137695; #-vertex:173, #-edge:2959\n",
      "[graph 870] time:0.009934186935424805; #-vertex:184, #-edge:1686\n",
      "[graph 871] time:0.010595083236694336; #-vertex:170, #-edge:1966\n",
      "[graph 872] time:0.024544477462768555; #-vertex:168, #-edge:3136\n",
      "[graph 873] time:0.02462029457092285; #-vertex:179, #-edge:3237\n",
      "[graph 874] time:0.02646160125732422; #-vertex:172, #-edge:3362\n",
      "[graph 875] time:0.031938791275024414; #-vertex:171, #-edge:3311\n",
      "[graph 876] time:0.020966768264770508; #-vertex:179, #-edge:2941\n",
      "[graph 877] time:0.032065629959106445; #-vertex:172, #-edge:3928\n",
      "[graph 878] time:0.03693056106567383; #-vertex:170, #-edge:4168\n",
      "[graph 879] time:0.027657508850097656; #-vertex:178, #-edge:3458\n",
      "[graph 880] time:0.014680624008178711; #-vertex:171, #-edge:2385\n",
      "[graph 881] time:0.041795969009399414; #-vertex:169, #-edge:4277\n",
      "[graph 882] time:0.09745454788208008; #-vertex:173, #-edge:2845\n",
      "[graph 883] time:0.01191568374633789; #-vertex:173, #-edge:1969\n",
      "[graph 884] time:0.024013757705688477; #-vertex:175, #-edge:3257\n",
      "[graph 885] time:0.015021085739135742; #-vertex:172, #-edge:2346\n",
      "[graph 886] time:0.02356886863708496; #-vertex:171, #-edge:2955\n",
      "[graph 887] time:0.030566930770874023; #-vertex:175, #-edge:3763\n",
      "[graph 888] time:0.0261228084564209; #-vertex:168, #-edge:3388\n",
      "[graph 889] time:0.017871618270874023; #-vertex:173, #-edge:2733\n",
      "[graph 890] time:0.030379295349121094; #-vertex:175, #-edge:3645\n",
      "[graph 891] time:0.015176534652709961; #-vertex:173, #-edge:2389\n",
      "[graph 892] time:0.027231454849243164; #-vertex:172, #-edge:3536\n",
      "[graph 893] time:0.02662515640258789; #-vertex:175, #-edge:3059\n",
      "[graph 894] time:0.0313870906829834; #-vertex:167, #-edge:3865\n",
      "[graph 895] time:0.022240877151489258; #-vertex:177, #-edge:2959\n",
      "[graph 896] time:0.029046297073364258; #-vertex:167, #-edge:3663\n",
      "[graph 897] time:0.027568578720092773; #-vertex:181, #-edge:3309\n",
      "[graph 898] time:0.03410458564758301; #-vertex:168, #-edge:4020\n",
      "[graph 899] time:0.007515907287597656; #-vertex:176, #-edge:1176\n",
      "[graph 900] time:0.020465612411499023; #-vertex:172, #-edge:2620\n",
      "[graph 901] time:0.01602005958557129; #-vertex:174, #-edge:2522\n",
      "[graph 902] time:0.008106708526611328; #-vertex:168, #-edge:1446\n",
      "[graph 903] time:0.03313493728637695; #-vertex:174, #-edge:3978\n",
      "[graph 904] time:0.039266347885131836; #-vertex:172, #-edge:3826\n",
      "[graph 905] time:0.0195920467376709; #-vertex:175, #-edge:2805\n",
      "[graph 906] time:0.018885374069213867; #-vertex:172, #-edge:2776\n",
      "[graph 907] time:0.023043155670166016; #-vertex:171, #-edge:2749\n",
      "[graph 908] time:0.025363683700561523; #-vertex:178, #-edge:3134\n",
      "[graph 909] time:0.026536226272583008; #-vertex:173, #-edge:3459\n",
      "[graph 910] time:0.01919269561767578; #-vertex:172, #-edge:2802\n",
      "[graph 911] time:0.028457164764404297; #-vertex:175, #-edge:3461\n",
      "[graph 912] time:0.028568029403686523; #-vertex:173, #-edge:3647\n",
      "[graph 913] time:0.10712885856628418; #-vertex:173, #-edge:3117\n",
      "[graph 914] time:0.04380059242248535; #-vertex:175, #-edge:4547\n",
      "[graph 915] time:0.014572620391845703; #-vertex:170, #-edge:2328\n",
      "[graph 916] time:0.0222930908203125; #-vertex:170, #-edge:3130\n",
      "[graph 917] time:0.0355069637298584; #-vertex:180, #-edge:3868\n",
      "[graph 918] time:0.02961564064025879; #-vertex:173, #-edge:3667\n",
      "[graph 919] time:0.015464544296264648; #-vertex:177, #-edge:2305\n",
      "[graph 920] time:0.010105609893798828; #-vertex:178, #-edge:1490\n",
      "[graph 921] time:0.012135028839111328; #-vertex:172, #-edge:2112\n",
      "[graph 922] time:0.014348506927490234; #-vertex:167, #-edge:2379\n",
      "[graph 923] time:0.010746240615844727; #-vertex:177, #-edge:1777\n",
      "[graph 924] time:0.019301176071166992; #-vertex:174, #-edge:2882\n",
      "[graph 925] time:0.007816553115844727; #-vertex:170, #-edge:1392\n",
      "[graph 926] time:0.011611700057983398; #-vertex:183, #-edge:1733\n",
      "[graph 927] time:0.019173860549926758; #-vertex:171, #-edge:2553\n",
      "[graph 928] time:0.021656513214111328; #-vertex:172, #-edge:3098\n",
      "[graph 929] time:0.01668691635131836; #-vertex:185, #-edge:2431\n",
      "[graph 930] time:0.010302066802978516; #-vertex:177, #-edge:1363\n",
      "[graph 931] time:0.018064022064208984; #-vertex:171, #-edge:2729\n",
      "[graph 932] time:0.026626110076904297; #-vertex:172, #-edge:3458\n",
      "[graph 933] time:0.011405706405639648; #-vertex:175, #-edge:1903\n",
      "[graph 934] time:0.008581161499023438; #-vertex:176, #-edge:1454\n",
      "[graph 935] time:0.021643400192260742; #-vertex:178, #-edge:2934\n",
      "[graph 936] time:0.013445138931274414; #-vertex:181, #-edge:2177\n",
      "[graph 937] time:0.009684562683105469; #-vertex:174, #-edge:1730\n",
      "[graph 938] time:0.03877115249633789; #-vertex:175, #-edge:3847\n",
      "[graph 939] time:0.011834859848022461; #-vertex:166, #-edge:2066\n",
      "[graph 940] time:0.028751373291015625; #-vertex:175, #-edge:3603\n",
      "[graph 941] time:0.027092933654785156; #-vertex:176, #-edge:3424\n",
      "[graph 942] time:0.03288912773132324; #-vertex:168, #-edge:3830\n",
      "[graph 943] time:0.013125419616699219; #-vertex:175, #-edge:2087\n",
      "[graph 944] time:0.03684878349304199; #-vertex:177, #-edge:4171\n",
      "[graph 945] time:0.04228067398071289; #-vertex:176, #-edge:4232\n",
      "[graph 946] time:0.009450197219848633; #-vertex:175, #-edge:1453\n",
      "[graph 947] time:0.02684640884399414; #-vertex:182, #-edge:3398\n",
      "[graph 948] time:0.008736133575439453; #-vertex:174, #-edge:1506\n",
      "[graph 949] time:0.013057708740234375; #-vertex:178, #-edge:2120\n",
      "[graph 950] time:0.09647154808044434; #-vertex:181, #-edge:1871\n",
      "[graph 951] time:0.011890411376953125; #-vertex:175, #-edge:2049\n",
      "[graph 952] time:0.02793717384338379; #-vertex:172, #-edge:3328\n",
      "[graph 953] time:0.0338902473449707; #-vertex:177, #-edge:3889\n",
      "[graph 954] time:0.025272607803344727; #-vertex:171, #-edge:3349\n",
      "[graph 955] time:0.022723913192749023; #-vertex:175, #-edge:3057\n",
      "[graph 956] time:0.011368751525878906; #-vertex:170, #-edge:1922\n",
      "[graph 957] time:0.024361371994018555; #-vertex:175, #-edge:3273\n",
      "[graph 958] time:0.014685869216918945; #-vertex:171, #-edge:2415\n",
      "[graph 959] time:0.03953385353088379; #-vertex:175, #-edge:4139\n",
      "[graph 960] time:0.017152070999145508; #-vertex:174, #-edge:2404\n",
      "[graph 961] time:0.0338892936706543; #-vertex:172, #-edge:3704\n",
      "[graph 962] time:0.020855426788330078; #-vertex:179, #-edge:2281\n",
      "[graph 963] time:0.008338689804077148; #-vertex:178, #-edge:1468\n",
      "[graph 964] time:0.03703570365905762; #-vertex:169, #-edge:4201\n",
      "[graph 965] time:0.041629791259765625; #-vertex:177, #-edge:3673\n",
      "[graph 966] time:0.030745506286621094; #-vertex:175, #-edge:3687\n",
      "[graph 967] time:0.015700578689575195; #-vertex:178, #-edge:1926\n",
      "[graph 968] time:0.027524709701538086; #-vertex:172, #-edge:3338\n",
      "[graph 969] time:0.016780376434326172; #-vertex:177, #-edge:2529\n",
      "[graph 970] time:0.03613162040710449; #-vertex:168, #-edge:4184\n",
      "[graph 971] time:0.015650033950805664; #-vertex:179, #-edge:2403\n",
      "[graph 972] time:0.032924652099609375; #-vertex:170, #-edge:3540\n",
      "[graph 973] time:0.011656999588012695; #-vertex:184, #-edge:1856\n",
      "[graph 974] time:0.013647079467773438; #-vertex:168, #-edge:2300\n",
      "[graph 975] time:0.03180384635925293; #-vertex:175, #-edge:3887\n",
      "[graph 976] time:0.015858173370361328; #-vertex:172, #-edge:2386\n",
      "[graph 977] time:0.013815641403198242; #-vertex:174, #-edge:2044\n",
      "[graph 978] time:0.02149796485900879; #-vertex:172, #-edge:3104\n",
      "[graph 979] time:0.03231167793273926; #-vertex:172, #-edge:3806\n",
      "[graph 980] time:0.023732423782348633; #-vertex:178, #-edge:2762\n",
      "[graph 981] time:0.009863615036010742; #-vertex:176, #-edge:1680\n",
      "[graph 982] time:0.026730775833129883; #-vertex:179, #-edge:3443\n",
      "[graph 983] time:0.09885239601135254; #-vertex:178, #-edge:1868\n",
      "[graph 984] time:0.01416468620300293; #-vertex:180, #-edge:2292\n",
      "[graph 985] time:0.019365310668945312; #-vertex:166, #-edge:2892\n",
      "[graph 986] time:0.011973142623901367; #-vertex:174, #-edge:2034\n",
      "[graph 987] time:0.033580780029296875; #-vertex:173, #-edge:3939\n",
      "[graph 988] time:0.028313159942626953; #-vertex:176, #-edge:3586\n",
      "[graph 989] time:0.027045249938964844; #-vertex:173, #-edge:3383\n",
      "[graph 990] time:0.029345035552978516; #-vertex:176, #-edge:3688\n",
      "[graph 991] time:0.021626710891723633; #-vertex:169, #-edge:3017\n",
      "[graph 992] time:0.032840728759765625; #-vertex:175, #-edge:3705\n",
      "[graph 993] time:0.02147364616394043; #-vertex:174, #-edge:2996\n",
      "[graph 994] time:0.01206827163696289; #-vertex:181, #-edge:1821\n",
      "[graph 995] time:0.007894515991210938; #-vertex:171, #-edge:1451\n",
      "[graph 996] time:0.028210878372192383; #-vertex:172, #-edge:3560\n",
      "[graph 997] time:0.019014358520507812; #-vertex:174, #-edge:2468\n",
      "[graph 998] time:0.013097286224365234; #-vertex:171, #-edge:2111\n",
      "[graph 999] time:0.024584531784057617; #-vertex:172, #-edge:2868\n",
      "Total Time:31.933175325393677\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-75e5ee7c5352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprocess_graph_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./tpcc_10_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprocess_graph_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./tpcc_10_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprocess_graph_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./tpcc_10_4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprocess_graph_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./tpcc_10_8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprocess_graph_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./tpcc_10_16'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-792c1bbd910f>\u001b[0m in \u001b[0;36mprocess_graph_data\u001b[0;34m(data_path, num_graphs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{data_path}/graph/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{data_path}/graph/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mwid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_graphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# lstat()/open()/fstat() trick.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tpcc_10_2/graph/'"
     ]
    }
   ],
   "source": [
    "process_graph_data('./tpcc_10_1')\n",
    "process_graph_data('./tpcc_10_2')\n",
    "process_graph_data('./tpcc_10_4')\n",
    "process_graph_data('./tpcc_10_8')\n",
    "process_graph_data('./tpcc_10_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0tmyQfrNcQb",
    "outputId": "fc9133e6-469a-4f74-a21b-80d2cec7dec1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generated Graph] 3181\n"
     ]
    }
   ],
   "source": [
    "graphs = glob.glob(\"./pmodel_data/job/graph/sample-plan-*\")\n",
    "num_graphs = int(len(graphs)/2)\n",
    "print(\"[Generated Graph]\", num_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiE327YNNcQb"
   },
   "source": [
    "# Graph Embedding Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gD8OudoJNcQb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFb359noNcQc"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1VYITrmNcQc"
   },
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YrKUdpicNcQc"
   },
   "outputs": [],
   "source": [
    "def load_data(dataset, path=data_path):\n",
    "    \n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "    \n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    \n",
    "    # encode vertices\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    \n",
    "    # encode labels\n",
    "    # labels = encode_onehot(idx_features_labels[:, -2])\n",
    "    labels = idx_features_labels[:, -1]\n",
    "    lables = labels.astype(float)\n",
    "    \n",
    "    # encode edges\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "\n",
    "    \n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.float32)[:, :-1]\n",
    "    edges_value = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.float32)[:, -1:]\n",
    "    # print(list(map(idx_map.get, edges_unordered.flatten())))\n",
    "    # print(edges_unordered.flatten())\n",
    "    \n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    \n",
    "    # edges (weights are computed in gcn)\n",
    "    # adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "    adj = sp.coo_matrix((edges_value[:, 0], (edges[:, 0], edges[:, 1])),    \n",
    "                        shape=(node_dim, node_dim),\n",
    "                        dtype=np.float32)\n",
    "    \n",
    "    # print(adj.shape)\n",
    "    # build symmetric adjacency matrix\n",
    "    # adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    \n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "    \n",
    "    operator_num = adj.shape[0]\n",
    "    idx_train = range(int(0.8 * operator_num))\n",
    "    # print(\"idx_train\", idx_train)\n",
    "    idx_val = range(int(0.8 * operator_num), int(0.9 * operator_num))\n",
    "    idx_test = range(int(0.9 * operator_num), int(operator_num))\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    # labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    \n",
    "    # padding to the same size\n",
    "    print(features.shape)\n",
    "    print(node_dim - features.shape[0])\n",
    "    dim=(0, 0, 0,  node_dim - features.shape[0])\n",
    "    features=F.pad(features, dim, \"constant\", value=0)\n",
    "\n",
    "    labels = labels.astype(np.float32)\n",
    "    labels = torch.from_numpy(labels)\n",
    "    # print(labels[idx_train].dtype)\n",
    "    labels.unsqueeze(1)\n",
    "    lables = labels * 10\n",
    "    labels=F.pad(labels, [0, node_dim - labels.shape[0]], \"constant\", value=0)    \n",
    "    \n",
    "    # print(\"features\", features.shape)\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "# adj, features, labels, idx_train, idx_val, idx_test = \n",
    "# load_data(path = \"/Users/xuanhe/Documents/mypaper/workload-performance/pygcn-master/data/cora/\", dataset = \"cora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndTfOb-9NcQd",
    "outputId": "7cf238d9-3b48-4058-e57e-5a1502589c05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[0., 1., 2., 0., 0., 0.],\n",
      "        [0., 3., 4., 0., 0., 0.]])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "x=np.asarray([[1,2], [3, 4]])\n",
    "X=torch.Tensor(x)\n",
    "print(X.shape)\n",
    "pad_dims = (1, 3)\n",
    "X=F.pad(X,pad_dims,\"constant\")\n",
    "print(X)\n",
    "print(X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QZhEqvnNcQd"
   },
   "source": [
    "## GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "veh_4r-tNcQd"
   },
   "outputs": [],
   "source": [
    "class arguments():\n",
    "    def __init__(self):\n",
    "        self.cuda = False\n",
    "        self.fastmode = False\n",
    "        self.seed = 42\n",
    "        self.epochs = 200\n",
    "        self.lr = 0.01\n",
    "        self.weight_decay = 5e-4\n",
    "        self.hidden = 16\n",
    "        self.dropout = 0.5\n",
    "        \n",
    "args = arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDbyy3RMNcQd",
    "outputId": "3d568b71-3b37-4e1e-eaac-dc2fa84b47a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "print(Path().resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCViohpdNcQd"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSU_vgCaNcQf"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.fc=nn.Linear(nclass,1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        x=self.fc(x)\n",
    "        \n",
    "#        return F.log_softmax(x, dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXblpH8pNcQf",
    "outputId": "a5bc85be-a5e4-4fd1-800c-7071a69a52ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[training samples]:8\n",
      "[graph 0]\n",
      "Loading sample-plan-0 dataset...\n",
      "torch.Size([170, 3])\n",
      "130\n",
      "Epoch: 0001 loss_train: 0.0008 loss_val: 0.0556 time: 0.0110s\n",
      "Epoch: 0002 loss_train: 0.1404 loss_val: 0.0059 time: 0.0130s\n",
      "Epoch: 0003 loss_train: 0.0091 loss_val: 0.0031 time: 0.0078s\n",
      "Epoch: 0004 loss_train: 0.0123 loss_val: 0.0125 time: 0.0085s\n",
      "Epoch: 0005 loss_train: 0.0255 loss_val: 0.0102 time: 0.0056s\n",
      "Epoch: 0006 loss_train: 0.0140 loss_val: 0.0029 time: 0.0064s\n",
      "Epoch: 0007 loss_train: 0.0024 loss_val: 0.0001 time: 0.0064s\n",
      "Epoch: 0008 loss_train: 0.0015 loss_val: 0.0036 time: 0.0064s\n",
      "Epoch: 0009 loss_train: 0.0152 loss_val: 0.0056 time: 0.0057s\n",
      "Epoch: 0010 loss_train: 0.0121 loss_val: 0.0039 time: 0.0076s\n",
      "Epoch: 0011 loss_train: 0.0078 loss_val: 0.0012 time: 0.0057s\n",
      "Epoch: 0012 loss_train: 0.0024 loss_val: 0.0000 time: 0.0059s\n",
      "Epoch: 0013 loss_train: 0.0016 loss_val: 0.0007 time: 0.0079s\n",
      "Epoch: 0014 loss_train: 0.0027 loss_val: 0.0022 time: 0.0110s\n",
      "Epoch: 0015 loss_train: 0.0030 loss_val: 0.0029 time: 0.0113s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: Using a target size (torch.Size([240])) that is different to the input size (torch.Size([240, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0016 loss_train: 0.0042 loss_val: 0.0026 time: 0.0150s\n",
      "Epoch: 0017 loss_train: 0.0040 loss_val: 0.0015 time: 0.0107s\n",
      "Epoch: 0018 loss_train: 0.0025 loss_val: 0.0005 time: 0.0084s\n",
      "Epoch: 0019 loss_train: 0.0009 loss_val: 0.0000 time: 0.0058s\n",
      "Epoch: 0020 loss_train: 0.0004 loss_val: 0.0001 time: 0.0057s\n",
      "Epoch: 0021 loss_train: 0.0004 loss_val: 0.0006 time: 0.0080s\n",
      "Epoch: 0022 loss_train: 0.0008 loss_val: 0.0008 time: 0.0086s\n",
      "Epoch: 0023 loss_train: 0.0013 loss_val: 0.0008 time: 0.0061s\n",
      "Epoch: 0024 loss_train: 0.0012 loss_val: 0.0005 time: 0.0081s\n",
      "Epoch: 0025 loss_train: 0.0010 loss_val: 0.0002 time: 0.0069s\n",
      "Epoch: 0026 loss_train: 0.0007 loss_val: 0.0000 time: 0.0085s\n",
      "Epoch: 0027 loss_train: 0.0003 loss_val: 0.0001 time: 0.0062s\n",
      "Epoch: 0028 loss_train: 0.0002 loss_val: 0.0004 time: 0.0067s\n",
      "Epoch: 0029 loss_train: 0.0003 loss_val: 0.0008 time: 0.0067s\n",
      "Epoch: 0030 loss_train: 0.0005 loss_val: 0.0010 time: 0.0058s\n",
      "Epoch: 0031 loss_train: 0.0006 loss_val: 0.0010 time: 0.0058s\n",
      "Epoch: 0032 loss_train: 0.0007 loss_val: 0.0009 time: 0.0082s\n",
      "Epoch: 0033 loss_train: 0.0006 loss_val: 0.0006 time: 0.0056s\n",
      "Epoch: 0034 loss_train: 0.0003 loss_val: 0.0003 time: 0.0082s\n",
      "Epoch: 0035 loss_train: 0.0002 loss_val: 0.0001 time: 0.0082s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.3340s\n",
      "Test set results: loss= 0.0001\n",
      "[graph 1]\n",
      "Loading sample-plan-1 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: Using a target size (torch.Size([240])) that is different to the input size (torch.Size([240, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([180, 3])\n",
      "120\n",
      "Epoch: 0001 loss_train: 3852.8625 loss_val: 0.0028 time: 0.0061s\n",
      "Epoch: 0002 loss_train: 4016.8120 loss_val: 0.0025 time: 0.0050s\n",
      "Epoch: 0003 loss_train: 1550.7604 loss_val: 0.0015 time: 0.0060s\n",
      "Epoch: 0004 loss_train: 189.5931 loss_val: 0.0011 time: 0.0069s\n",
      "Epoch: 0005 loss_train: 1652.5204 loss_val: 0.0018 time: 0.0070s\n",
      "Epoch: 0006 loss_train: 345.6100 loss_val: 0.0031 time: 0.0045s\n",
      "Epoch: 0007 loss_train: 41.3339 loss_val: 0.0048 time: 0.0062s\n",
      "Epoch: 0008 loss_train: 524.2206 loss_val: 0.0053 time: 0.0063s\n",
      "Epoch: 0009 loss_train: 642.5016 loss_val: 0.0044 time: 0.0062s\n",
      "Epoch: 0010 loss_train: 388.0983 loss_val: 0.0030 time: 0.0067s\n",
      "Epoch: 0011 loss_train: 45.3967 loss_val: 0.0018 time: 0.0048s\n",
      "Epoch: 0012 loss_train: 41.3015 loss_val: 0.0011 time: 0.0075s\n",
      "Epoch: 0013 loss_train: 184.4072 loss_val: 0.0008 time: 0.0059s\n",
      "Epoch: 0014 loss_train: 129.2592 loss_val: 0.0008 time: 0.0134s\n",
      "Epoch: 0015 loss_train: 163.6704 loss_val: 0.0011 time: 0.0053s\n",
      "Epoch: 0016 loss_train: 88.5283 loss_val: 0.0015 time: 0.0060s\n",
      "Epoch: 0017 loss_train: 35.2077 loss_val: 0.0022 time: 0.0063s\n",
      "Epoch: 0018 loss_train: 3.7099 loss_val: 0.0030 time: 0.0068s\n",
      "Epoch: 0019 loss_train: 5.2946 loss_val: 0.0036 time: 0.0067s\n",
      "Epoch: 0020 loss_train: 25.3125 loss_val: 0.0041 time: 0.0064s\n",
      "Epoch: 0021 loss_train: 38.8665 loss_val: 0.0043 time: 0.0050s\n",
      "Epoch: 0022 loss_train: 72.1654 loss_val: 0.0041 time: 0.0082s\n",
      "Epoch: 0023 loss_train: 47.7392 loss_val: 0.0036 time: 0.0054s\n",
      "Epoch: 0024 loss_train: 28.5195 loss_val: 0.0030 time: 0.0058s\n",
      "Epoch: 0025 loss_train: 36.7988 loss_val: 0.0023 time: 0.0050s\n",
      "Epoch: 0026 loss_train: 6.1423 loss_val: 0.0017 time: 0.0100s\n",
      "Epoch: 0027 loss_train: 4.1539 loss_val: 0.0011 time: 0.0058s\n",
      "Epoch: 0028 loss_train: 0.0356 loss_val: 0.0008 time: 0.0054s\n",
      "Epoch: 0029 loss_train: 0.9944 loss_val: 0.0005 time: 0.0044s\n",
      "Epoch: 0030 loss_train: 1.9452 loss_val: 0.0003 time: 0.0041s\n",
      "Epoch: 0031 loss_train: 2.8033 loss_val: 0.0002 time: 0.0045s\n",
      "Epoch: 0032 loss_train: 5.0330 loss_val: 0.0001 time: 0.0040s\n",
      "Epoch: 0033 loss_train: 8.1820 loss_val: 0.0001 time: 0.0067s\n",
      "Epoch: 0034 loss_train: 13.1522 loss_val: 0.0000 time: 0.0041s\n",
      "Epoch: 0035 loss_train: 9.9476 loss_val: 0.0000 time: 0.0051s\n",
      "Epoch: 0036 loss_train: 6.1703 loss_val: 0.0001 time: 0.0100s\n",
      "Epoch: 0037 loss_train: 4.5497 loss_val: 0.0001 time: 0.0055s\n",
      "Epoch: 0038 loss_train: 2.8915 loss_val: 0.0001 time: 0.0060s\n",
      "Epoch: 0039 loss_train: 2.8827 loss_val: 0.0001 time: 0.0041s\n",
      "Epoch: 0040 loss_train: 2.5763 loss_val: 0.0002 time: 0.0061s\n",
      "Epoch: 0041 loss_train: 0.9372 loss_val: 0.0002 time: 0.0049s\n",
      "Epoch: 0042 loss_train: 0.8032 loss_val: 0.0003 time: 0.0061s\n",
      "Epoch: 0043 loss_train: 0.2634 loss_val: 0.0004 time: 0.0042s\n",
      "Epoch: 0044 loss_train: 0.0798 loss_val: 0.0005 time: 0.0065s\n",
      "Epoch: 0045 loss_train: 0.1756 loss_val: 0.0005 time: 0.0061s\n",
      "Epoch: 0046 loss_train: 0.5664 loss_val: 0.0006 time: 0.0048s\n",
      "Epoch: 0047 loss_train: 0.3195 loss_val: 0.0006 time: 0.0046s\n",
      "Epoch: 0048 loss_train: 1.0449 loss_val: 0.0006 time: 0.0065s\n",
      "Epoch: 0049 loss_train: 0.7600 loss_val: 0.0006 time: 0.0041s\n",
      "Epoch: 0050 loss_train: 0.6656 loss_val: 0.0006 time: 0.0061s\n",
      "Epoch: 0051 loss_train: 0.8002 loss_val: 0.0005 time: 0.0048s\n",
      "Epoch: 0052 loss_train: 0.9773 loss_val: 0.0005 time: 0.0059s\n",
      "Epoch: 0053 loss_train: 0.7906 loss_val: 0.0004 time: 0.0052s\n",
      "Epoch: 0054 loss_train: 1.8638 loss_val: 0.0004 time: 0.0106s\n",
      "Epoch: 0055 loss_train: 0.1966 loss_val: 0.0003 time: 0.0087s\n",
      "Epoch: 0056 loss_train: 0.9480 loss_val: 0.0003 time: 0.0079s\n",
      "Epoch: 0057 loss_train: 0.2258 loss_val: 0.0002 time: 0.0049s\n",
      "Epoch: 0058 loss_train: 0.5201 loss_val: 0.0002 time: 0.0047s\n",
      "Epoch: 0059 loss_train: 0.5779 loss_val: 0.0001 time: 0.0048s\n",
      "Epoch: 0060 loss_train: 0.1129 loss_val: 0.0001 time: 0.0050s\n",
      "Epoch: 0061 loss_train: 0.1367 loss_val: 0.0000 time: 0.0066s\n",
      "Epoch: 0062 loss_train: 0.1028 loss_val: 0.0000 time: 0.0068s\n",
      "Epoch: 0063 loss_train: 0.1287 loss_val: 0.0000 time: 0.0069s\n",
      "Epoch: 0064 loss_train: 0.0783 loss_val: 0.0000 time: 0.0055s\n",
      "Epoch: 0065 loss_train: 0.0747 loss_val: 0.0000 time: 0.0044s\n",
      "Epoch: 0066 loss_train: 0.1145 loss_val: 0.0000 time: 0.0067s\n",
      "Epoch: 0067 loss_train: 0.1435 loss_val: 0.0000 time: 0.0071s\n",
      "Epoch: 0068 loss_train: 0.2586 loss_val: 0.0000 time: 0.0055s\n",
      "Epoch: 0069 loss_train: 0.1457 loss_val: 0.0000 time: 0.0049s\n",
      "Epoch: 0070 loss_train: 0.1776 loss_val: 0.0000 time: 0.0051s\n",
      "Epoch: 0071 loss_train: 0.1795 loss_val: 0.0000 time: 0.0056s\n",
      "Epoch: 0072 loss_train: 0.1703 loss_val: 0.0000 time: 0.0052s\n",
      "Epoch: 0073 loss_train: 0.2168 loss_val: 0.0000 time: 0.0062s\n",
      "Epoch: 0074 loss_train: 0.1785 loss_val: 0.0000 time: 0.0060s\n",
      "Epoch: 0075 loss_train: 0.1885 loss_val: 0.0000 time: 0.0049s\n",
      "Epoch: 0076 loss_train: 0.2089 loss_val: 0.0000 time: 0.0057s\n",
      "Epoch: 0077 loss_train: 0.0994 loss_val: 0.0000 time: 0.0095s\n",
      "Epoch: 0078 loss_train: 0.0889 loss_val: 0.0000 time: 0.0057s\n",
      "Epoch: 0079 loss_train: 0.0679 loss_val: 0.0000 time: 0.0065s\n",
      "Epoch: 0080 loss_train: 0.0551 loss_val: 0.0000 time: 0.0047s\n",
      "Epoch: 0081 loss_train: 0.0443 loss_val: 0.0000 time: 0.0043s\n",
      "Epoch: 0082 loss_train: 0.0358 loss_val: 0.0000 time: 0.0117s\n",
      "Epoch: 0083 loss_train: 0.0431 loss_val: 0.0000 time: 0.0072s\n",
      "Epoch: 0084 loss_train: 0.0244 loss_val: 0.0000 time: 0.0044s\n",
      "Epoch: 0085 loss_train: 0.1034 loss_val: 0.0000 time: 0.0043s\n",
      "Epoch: 0086 loss_train: 0.0832 loss_val: 0.0000 time: 0.0050s\n",
      "Epoch: 0087 loss_train: 0.0752 loss_val: 0.0000 time: 0.0068s\n",
      "Epoch: 0088 loss_train: 0.0838 loss_val: 0.0000 time: 0.0044s\n",
      "Epoch: 0089 loss_train: 0.1184 loss_val: 0.0000 time: 0.0053s\n",
      "Epoch: 0090 loss_train: 0.0265 loss_val: 0.0000 time: 0.0042s\n",
      "Epoch: 0091 loss_train: 0.0169 loss_val: 0.0000 time: 0.0041s\n",
      "Epoch: 0092 loss_train: 0.0477 loss_val: 0.0000 time: 0.0065s\n",
      "Epoch: 0093 loss_train: 0.0261 loss_val: 0.0000 time: 0.0042s\n",
      "Epoch: 0094 loss_train: 0.0416 loss_val: 0.0000 time: 0.0048s\n",
      "Epoch: 0095 loss_train: 0.0232 loss_val: 0.0000 time: 0.0040s\n",
      "Epoch: 0096 loss_train: 0.0220 loss_val: 0.0000 time: 0.0047s\n",
      "Epoch: 0097 loss_train: 0.0218 loss_val: 0.0000 time: 0.0066s\n",
      "Epoch: 0098 loss_train: 0.0224 loss_val: 0.0000 time: 0.0042s\n",
      "Epoch: 0099 loss_train: 0.0302 loss_val: 0.0000 time: 0.0044s\n",
      "Epoch: 0100 loss_train: 0.0213 loss_val: 0.0000 time: 0.0056s\n",
      "Epoch: 0101 loss_train: 0.0287 loss_val: 0.0000 time: 0.0068s\n",
      "Epoch: 0102 loss_train: 0.0196 loss_val: 0.0000 time: 0.0067s\n",
      "Epoch: 0103 loss_train: 0.0231 loss_val: 0.0000 time: 0.0063s\n",
      "Epoch: 0104 loss_train: 0.0207 loss_val: 0.0000 time: 0.0050s\n",
      "Epoch: 0105 loss_train: 0.0289 loss_val: 0.0000 time: 0.0050s\n",
      "Epoch: 0106 loss_train: 0.0218 loss_val: 0.0000 time: 0.0094s\n",
      "Epoch: 0107 loss_train: 0.0198 loss_val: 0.0000 time: 0.0084s\n",
      "Epoch: 0108 loss_train: 0.0204 loss_val: 0.0000 time: 0.0099s\n",
      "Epoch: 0109 loss_train: 0.0249 loss_val: 0.0001 time: 0.0068s\n",
      "Epoch: 0110 loss_train: 0.0403 loss_val: 0.0001 time: 0.0106s\n",
      "Epoch: 0111 loss_train: 0.0195 loss_val: 0.0001 time: 0.0049s\n",
      "Epoch: 0112 loss_train: 0.0215 loss_val: 0.0001 time: 0.0066s\n",
      "Epoch: 0113 loss_train: 0.0257 loss_val: 0.0001 time: 0.0051s\n",
      "Epoch: 0114 loss_train: 0.0393 loss_val: 0.0001 time: 0.0080s\n",
      "Epoch: 0115 loss_train: 0.0281 loss_val: 0.0001 time: 0.0047s\n",
      "Epoch: 0116 loss_train: 0.0319 loss_val: 0.0001 time: 0.0045s\n",
      "Epoch: 0117 loss_train: 0.0187 loss_val: 0.0001 time: 0.0067s\n",
      "Epoch: 0118 loss_train: 0.0317 loss_val: 0.0001 time: 0.0075s\n",
      "Epoch: 0119 loss_train: 0.0187 loss_val: 0.0001 time: 0.0062s\n",
      "Epoch: 0120 loss_train: 0.0205 loss_val: 0.0001 time: 0.0073s\n",
      "Epoch: 0121 loss_train: 0.0183 loss_val: 0.0001 time: 0.0059s\n",
      "Epoch: 0122 loss_train: 0.0177 loss_val: 0.0001 time: 0.0069s\n",
      "Epoch: 0123 loss_train: 0.0184 loss_val: 0.0001 time: 0.0072s\n",
      "Epoch: 0124 loss_train: 0.0194 loss_val: 0.0001 time: 0.0046s\n",
      "Epoch: 0125 loss_train: 0.0172 loss_val: 0.0001 time: 0.0066s\n",
      "Epoch: 0126 loss_train: 0.0274 loss_val: 0.0001 time: 0.0065s\n",
      "Epoch: 0127 loss_train: 0.0243 loss_val: 0.0001 time: 0.0115s\n",
      "Epoch: 0128 loss_train: 0.0214 loss_val: 0.0001 time: 0.0044s\n",
      "Epoch: 0129 loss_train: 0.0171 loss_val: 0.0001 time: 0.0061s\n",
      "Epoch: 0130 loss_train: 0.0222 loss_val: 0.0002 time: 0.0041s\n",
      "Epoch: 0131 loss_train: 0.0163 loss_val: 0.0002 time: 0.0043s\n",
      "Epoch: 0132 loss_train: 0.0256 loss_val: 0.0002 time: 0.0040s\n",
      "Epoch: 0133 loss_train: 0.0246 loss_val: 0.0002 time: 0.0042s\n",
      "Epoch: 0134 loss_train: 0.0348 loss_val: 0.0002 time: 0.0060s\n",
      "Epoch: 0135 loss_train: 0.0240 loss_val: 0.0002 time: 0.0059s\n",
      "Epoch: 0136 loss_train: 0.0274 loss_val: 0.0002 time: 0.0045s\n",
      "Epoch: 0137 loss_train: 0.0179 loss_val: 0.0002 time: 0.0109s\n",
      "Epoch: 0138 loss_train: 0.0171 loss_val: 0.0002 time: 0.0064s\n",
      "Epoch: 0139 loss_train: 0.0184 loss_val: 0.0002 time: 0.0044s\n",
      "Epoch: 0140 loss_train: 0.0182 loss_val: 0.0002 time: 0.0065s\n",
      "Epoch: 0141 loss_train: 0.0198 loss_val: 0.0002 time: 0.0041s\n",
      "Epoch: 0142 loss_train: 0.0170 loss_val: 0.0002 time: 0.0062s\n",
      "Epoch: 0143 loss_train: 0.0166 loss_val: 0.0002 time: 0.0043s\n",
      "Epoch: 0144 loss_train: 0.0196 loss_val: 0.0002 time: 0.0052s\n",
      "Epoch: 0145 loss_train: 0.0169 loss_val: 0.0002 time: 0.0068s\n",
      "Epoch: 0146 loss_train: 0.0162 loss_val: 0.0002 time: 0.0042s\n",
      "Epoch: 0147 loss_train: 0.0234 loss_val: 0.0002 time: 0.0042s\n",
      "Epoch: 0148 loss_train: 0.0166 loss_val: 0.0002 time: 0.0066s\n",
      "Epoch: 0149 loss_train: 0.0321 loss_val: 0.0003 time: 0.0057s\n",
      "Epoch: 0150 loss_train: 0.0355 loss_val: 0.0003 time: 0.0041s\n",
      "Epoch: 0151 loss_train: 0.0214 loss_val: 0.0003 time: 0.0064s\n",
      "Epoch: 0152 loss_train: 0.0166 loss_val: 0.0003 time: 0.0043s\n",
      "Epoch: 0153 loss_train: 0.0291 loss_val: 0.0003 time: 0.0040s\n",
      "Epoch: 0154 loss_train: 0.0179 loss_val: 0.0003 time: 0.0057s\n",
      "Epoch: 0155 loss_train: 0.0205 loss_val: 0.0003 time: 0.0054s\n",
      "Epoch: 0156 loss_train: 0.0176 loss_val: 0.0003 time: 0.0053s\n",
      "Epoch: 0157 loss_train: 0.0168 loss_val: 0.0003 time: 0.0039s\n",
      "Epoch: 0158 loss_train: 0.0161 loss_val: 0.0003 time: 0.0044s\n",
      "Epoch: 0159 loss_train: 0.0167 loss_val: 0.0003 time: 0.0050s\n",
      "Epoch: 0160 loss_train: 0.0163 loss_val: 0.0003 time: 0.0039s\n",
      "Epoch: 0161 loss_train: 0.0194 loss_val: 0.0003 time: 0.0041s\n",
      "Epoch: 0162 loss_train: 0.0158 loss_val: 0.0003 time: 0.0040s\n",
      "Epoch: 0163 loss_train: 0.0176 loss_val: 0.0003 time: 0.0052s\n",
      "Epoch: 0164 loss_train: 0.0263 loss_val: 0.0003 time: 0.0063s\n",
      "Epoch: 0165 loss_train: 0.0164 loss_val: 0.0003 time: 0.0107s\n",
      "Epoch: 0166 loss_train: 0.0279 loss_val: 0.0004 time: 0.0077s\n",
      "Epoch: 0167 loss_train: 0.0153 loss_val: 0.0004 time: 0.0041s\n",
      "Epoch: 0168 loss_train: 0.0170 loss_val: 0.0004 time: 0.0044s\n",
      "Epoch: 0169 loss_train: 0.0182 loss_val: 0.0004 time: 0.0046s\n",
      "Epoch: 0170 loss_train: 0.0198 loss_val: 0.0004 time: 0.0040s\n",
      "Epoch: 0171 loss_train: 0.0154 loss_val: 0.0004 time: 0.0051s\n",
      "Epoch: 0172 loss_train: 0.0156 loss_val: 0.0004 time: 0.0047s\n",
      "Epoch: 0173 loss_train: 0.0245 loss_val: 0.0004 time: 0.0047s\n",
      "Epoch: 0174 loss_train: 0.0203 loss_val: 0.0004 time: 0.0053s\n",
      "Epoch: 0175 loss_train: 0.0208 loss_val: 0.0004 time: 0.0053s\n",
      "Epoch: 0176 loss_train: 0.0152 loss_val: 0.0004 time: 0.0050s\n",
      "Epoch: 0177 loss_train: 0.0153 loss_val: 0.0004 time: 0.0068s\n",
      "Epoch: 0178 loss_train: 0.0155 loss_val: 0.0004 time: 0.0042s\n",
      "Epoch: 0179 loss_train: 0.0153 loss_val: 0.0004 time: 0.0042s\n",
      "Epoch: 0180 loss_train: 0.0184 loss_val: 0.0004 time: 0.0043s\n",
      "Epoch: 0181 loss_train: 0.0152 loss_val: 0.0004 time: 0.0043s\n",
      "Epoch: 0182 loss_train: 0.0277 loss_val: 0.0004 time: 0.0057s\n",
      "Epoch: 0183 loss_train: 0.0186 loss_val: 0.0004 time: 0.0051s\n",
      "Epoch: 0184 loss_train: 0.0161 loss_val: 0.0004 time: 0.0047s\n",
      "Epoch: 0185 loss_train: 0.0158 loss_val: 0.0005 time: 0.0063s\n",
      "Epoch: 0186 loss_train: 0.0165 loss_val: 0.0005 time: 0.0041s\n",
      "Epoch: 0187 loss_train: 0.0160 loss_val: 0.0005 time: 0.0042s\n",
      "Epoch: 0188 loss_train: 0.0182 loss_val: 0.0005 time: 0.0066s\n",
      "Epoch: 0189 loss_train: 0.0149 loss_val: 0.0005 time: 0.0043s\n",
      "Epoch: 0190 loss_train: 0.0176 loss_val: 0.0005 time: 0.0070s\n",
      "Epoch: 0191 loss_train: 0.0171 loss_val: 0.0005 time: 0.0066s\n",
      "Epoch: 0192 loss_train: 0.0157 loss_val: 0.0005 time: 0.0054s\n",
      "Epoch: 0193 loss_train: 0.0178 loss_val: 0.0005 time: 0.0049s\n",
      "Epoch: 0194 loss_train: 0.0169 loss_val: 0.0005 time: 0.0101s\n",
      "Epoch: 0195 loss_train: 0.0194 loss_val: 0.0005 time: 0.0070s\n",
      "Epoch: 0196 loss_train: 0.0164 loss_val: 0.0005 time: 0.0042s\n",
      "Epoch: 0197 loss_train: 0.0180 loss_val: 0.0005 time: 0.0053s\n",
      "Epoch: 0198 loss_train: 0.0164 loss_val: 0.0005 time: 0.0047s\n",
      "Epoch: 0199 loss_train: 0.0154 loss_val: 0.0005 time: 0.0065s\n",
      "Epoch: 0200 loss_train: 0.0154 loss_val: 0.0005 time: 0.0045s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1.5102s\n",
      "Test set results: loss= 0.0005\n",
      "[graph 2]\n",
      "Loading sample-plan-2 dataset...\n",
      "torch.Size([174, 3])\n",
      "126\n",
      "Epoch: 0001 loss_train: 0.0141 loss_val: 0.0005 time: 0.0054s\n",
      "Epoch: 0002 loss_train: 0.0133 loss_val: 0.0005 time: 0.0067s\n",
      "Epoch: 0003 loss_train: 0.0136 loss_val: 0.0005 time: 0.0047s\n",
      "Epoch: 0004 loss_train: 0.0125 loss_val: 0.0005 time: 0.0055s\n",
      "Epoch: 0005 loss_train: 0.0134 loss_val: 0.0005 time: 0.0042s\n",
      "Epoch: 0006 loss_train: 0.0127 loss_val: 0.0005 time: 0.0044s\n",
      "Epoch: 0007 loss_train: 0.0137 loss_val: 0.0005 time: 0.0090s\n",
      "Epoch: 0008 loss_train: 0.0122 loss_val: 0.0005 time: 0.0082s\n",
      "Epoch: 0009 loss_train: 0.0111 loss_val: 0.0005 time: 0.0067s\n",
      "Epoch: 0010 loss_train: 0.0127 loss_val: 0.0005 time: 0.0042s\n",
      "Epoch: 0011 loss_train: 0.0120 loss_val: 0.0005 time: 0.0064s\n",
      "Epoch: 0012 loss_train: 0.0115 loss_val: 0.0005 time: 0.0069s\n",
      "Epoch: 0013 loss_train: 0.0130 loss_val: 0.0005 time: 0.0065s\n",
      "Epoch: 0014 loss_train: 0.0137 loss_val: 0.0005 time: 0.0042s\n",
      "Epoch: 0015 loss_train: 0.0110 loss_val: 0.0005 time: 0.0045s\n",
      "Epoch: 0016 loss_train: 0.0128 loss_val: 0.0005 time: 0.0048s\n",
      "Epoch: 0017 loss_train: 0.0128 loss_val: 0.0005 time: 0.0058s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: Using a target size (torch.Size([240])) that is different to the input size (torch.Size([240, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0018 loss_train: 0.0141 loss_val: 0.0005 time: 0.0072s\n",
      "Epoch: 0019 loss_train: 0.0144 loss_val: 0.0005 time: 0.0091s\n",
      "Epoch: 0020 loss_train: 0.0125 loss_val: 0.0005 time: 0.0041s\n",
      "Epoch: 0021 loss_train: 0.0124 loss_val: 0.0005 time: 0.0048s\n",
      "Epoch: 0022 loss_train: 0.0125 loss_val: 0.0005 time: 0.0042s\n",
      "Epoch: 0023 loss_train: 0.0152 loss_val: 0.0005 time: 0.0066s\n",
      "Epoch: 0024 loss_train: 0.0115 loss_val: 0.0005 time: 0.0075s\n",
      "Epoch: 0025 loss_train: 0.0146 loss_val: 0.0004 time: 0.0080s\n",
      "Epoch: 0026 loss_train: 0.0126 loss_val: 0.0004 time: 0.0049s\n",
      "Epoch: 0027 loss_train: 0.0124 loss_val: 0.0004 time: 0.0042s\n",
      "Epoch: 0028 loss_train: 0.0128 loss_val: 0.0004 time: 0.0042s\n",
      "Epoch: 0029 loss_train: 0.0136 loss_val: 0.0004 time: 0.0048s\n",
      "Epoch: 0030 loss_train: 0.0139 loss_val: 0.0004 time: 0.0066s\n",
      "Epoch: 0031 loss_train: 0.0138 loss_val: 0.0004 time: 0.0046s\n",
      "Epoch: 0032 loss_train: 0.0123 loss_val: 0.0004 time: 0.0056s\n",
      "Epoch: 0033 loss_train: 0.0125 loss_val: 0.0004 time: 0.0061s\n",
      "Epoch: 0034 loss_train: 0.0140 loss_val: 0.0004 time: 0.0069s\n",
      "Epoch: 0035 loss_train: 0.0124 loss_val: 0.0004 time: 0.0049s\n",
      "Epoch: 0036 loss_train: 0.0136 loss_val: 0.0004 time: 0.0071s\n",
      "Epoch: 0037 loss_train: 0.0151 loss_val: 0.0004 time: 0.0101s\n",
      "Epoch: 0038 loss_train: 0.0122 loss_val: 0.0004 time: 0.0065s\n",
      "Epoch: 0039 loss_train: 0.0158 loss_val: 0.0004 time: 0.0065s\n",
      "Epoch: 0040 loss_train: 0.0127 loss_val: 0.0004 time: 0.0090s\n",
      "Epoch: 0041 loss_train: 0.0121 loss_val: 0.0004 time: 0.0085s\n",
      "Epoch: 0042 loss_train: 0.0134 loss_val: 0.0004 time: 0.0043s\n",
      "Epoch: 0043 loss_train: 0.0136 loss_val: 0.0004 time: 0.0056s\n",
      "Epoch: 0044 loss_train: 0.0120 loss_val: 0.0004 time: 0.0080s\n",
      "Epoch: 0045 loss_train: 0.0132 loss_val: 0.0004 time: 0.0091s\n",
      "Epoch: 0046 loss_train: 0.0128 loss_val: 0.0004 time: 0.0041s\n",
      "Epoch: 0047 loss_train: 0.0127 loss_val: 0.0004 time: 0.0040s\n",
      "Epoch: 0048 loss_train: 0.0117 loss_val: 0.0004 time: 0.0049s\n",
      "Epoch: 0049 loss_train: 0.0130 loss_val: 0.0004 time: 0.0065s\n",
      "Epoch: 0050 loss_train: 0.0127 loss_val: 0.0004 time: 0.0064s\n",
      "Epoch: 0051 loss_train: 0.0131 loss_val: 0.0004 time: 0.0041s\n",
      "Epoch: 0052 loss_train: 0.0108 loss_val: 0.0004 time: 0.0053s\n",
      "Epoch: 0053 loss_train: 0.0127 loss_val: 0.0004 time: 0.0041s\n",
      "Epoch: 0054 loss_train: 0.0127 loss_val: 0.0004 time: 0.0062s\n",
      "Epoch: 0055 loss_train: 0.0125 loss_val: 0.0004 time: 0.0064s\n",
      "Epoch: 0056 loss_train: 0.0119 loss_val: 0.0004 time: 0.0071s\n",
      "Epoch: 0057 loss_train: 0.0146 loss_val: 0.0004 time: 0.0065s\n",
      "Epoch: 0058 loss_train: 0.0114 loss_val: 0.0004 time: 0.0055s\n",
      "Epoch: 0059 loss_train: 0.0140 loss_val: 0.0004 time: 0.0042s\n",
      "Epoch: 0060 loss_train: 0.0128 loss_val: 0.0004 time: 0.0066s\n",
      "Epoch: 0061 loss_train: 0.0129 loss_val: 0.0004 time: 0.0042s\n",
      "Epoch: 0062 loss_train: 0.0137 loss_val: 0.0004 time: 0.0065s\n",
      "Epoch: 0063 loss_train: 0.0122 loss_val: 0.0004 time: 0.0047s\n",
      "Epoch: 0064 loss_train: 0.0119 loss_val: 0.0004 time: 0.0058s\n",
      "Epoch: 0065 loss_train: 0.0117 loss_val: 0.0004 time: 0.0041s\n",
      "Epoch: 0066 loss_train: 0.0116 loss_val: 0.0004 time: 0.0066s\n",
      "Epoch: 0067 loss_train: 0.0135 loss_val: 0.0004 time: 0.0048s\n",
      "Epoch: 0068 loss_train: 0.0117 loss_val: 0.0004 time: 0.0040s\n",
      "Epoch: 0069 loss_train: 0.0121 loss_val: 0.0004 time: 0.0066s\n",
      "Epoch: 0070 loss_train: 0.0135 loss_val: 0.0004 time: 0.0048s\n",
      "Epoch: 0071 loss_train: 0.0130 loss_val: 0.0004 time: 0.0069s\n",
      "Epoch: 0072 loss_train: 0.0126 loss_val: 0.0004 time: 0.0047s\n",
      "Epoch: 0073 loss_train: 0.0133 loss_val: 0.0004 time: 0.0072s\n",
      "Epoch: 0074 loss_train: 0.0107 loss_val: 0.0004 time: 0.0081s\n",
      "Epoch: 0075 loss_train: 0.0117 loss_val: 0.0004 time: 0.0057s\n",
      "Epoch: 0076 loss_train: 0.0124 loss_val: 0.0004 time: 0.0075s\n",
      "Epoch: 0077 loss_train: 0.0126 loss_val: 0.0004 time: 0.0072s\n",
      "Epoch: 0078 loss_train: 0.0108 loss_val: 0.0004 time: 0.0044s\n",
      "Epoch: 0079 loss_train: 0.0130 loss_val: 0.0004 time: 0.0060s\n",
      "Epoch: 0080 loss_train: 0.0132 loss_val: 0.0004 time: 0.0054s\n",
      "Epoch: 0081 loss_train: 0.0116 loss_val: 0.0004 time: 0.0053s\n",
      "Epoch: 0082 loss_train: 0.0111 loss_val: 0.0004 time: 0.0064s\n",
      "Epoch: 0083 loss_train: 0.0122 loss_val: 0.0004 time: 0.0066s\n",
      "Epoch: 0084 loss_train: 0.0134 loss_val: 0.0004 time: 0.0051s\n",
      "Epoch: 0085 loss_train: 0.0112 loss_val: 0.0004 time: 0.0046s\n",
      "Epoch: 0086 loss_train: 0.0117 loss_val: 0.0004 time: 0.0051s\n",
      "Epoch: 0087 loss_train: 0.0109 loss_val: 0.0004 time: 0.0062s\n",
      "Epoch: 0088 loss_train: 0.0134 loss_val: 0.0004 time: 0.0044s\n",
      "Epoch: 0089 loss_train: 0.0125 loss_val: 0.0004 time: 0.0040s\n",
      "Epoch: 0090 loss_train: 0.0128 loss_val: 0.0004 time: 0.0066s\n",
      "Epoch: 0091 loss_train: 0.0116 loss_val: 0.0004 time: 0.0043s\n",
      "Epoch: 0092 loss_train: 0.0127 loss_val: 0.0004 time: 0.0071s\n",
      "Epoch: 0093 loss_train: 0.0126 loss_val: 0.0004 time: 0.0041s\n",
      "Epoch: 0094 loss_train: 0.0125 loss_val: 0.0004 time: 0.0066s\n",
      "Epoch: 0095 loss_train: 0.0131 loss_val: 0.0004 time: 0.0055s\n",
      "Epoch: 0096 loss_train: 0.0126 loss_val: 0.0004 time: 0.0059s\n",
      "Epoch: 0097 loss_train: 0.0119 loss_val: 0.0004 time: 0.0046s\n",
      "Epoch: 0098 loss_train: 0.0130 loss_val: 0.0004 time: 0.0042s\n",
      "Epoch: 0099 loss_train: 0.0111 loss_val: 0.0004 time: 0.0092s\n",
      "Epoch: 0100 loss_train: 0.0111 loss_val: 0.0004 time: 0.0068s\n",
      "Epoch: 0101 loss_train: 0.0135 loss_val: 0.0004 time: 0.0043s\n",
      "Epoch: 0102 loss_train: 0.0124 loss_val: 0.0004 time: 0.0085s\n",
      "Epoch: 0103 loss_train: 0.0116 loss_val: 0.0004 time: 0.0087s\n",
      "Epoch: 0104 loss_train: 0.0117 loss_val: 0.0004 time: 0.0083s\n",
      "Epoch: 0105 loss_train: 0.0117 loss_val: 0.0004 time: 0.0088s\n",
      "Epoch: 0106 loss_train: 0.0137 loss_val: 0.0004 time: 0.0064s\n",
      "Epoch: 0107 loss_train: 0.0126 loss_val: 0.0004 time: 0.0049s\n",
      "Epoch: 0108 loss_train: 0.0139 loss_val: 0.0004 time: 0.0042s\n",
      "Epoch: 0109 loss_train: 0.0134 loss_val: 0.0004 time: 0.0066s\n",
      "Epoch: 0110 loss_train: 0.0127 loss_val: 0.0004 time: 0.0048s\n",
      "Epoch: 0111 loss_train: 0.0118 loss_val: 0.0004 time: 0.0066s\n",
      "Epoch: 0112 loss_train: 0.0124 loss_val: 0.0004 time: 0.0047s\n",
      "Epoch: 0113 loss_train: 0.0119 loss_val: 0.0004 time: 0.0047s\n",
      "Epoch: 0114 loss_train: 0.0127 loss_val: 0.0004 time: 0.0040s\n",
      "Epoch: 0115 loss_train: 0.0116 loss_val: 0.0004 time: 0.0072s\n",
      "Epoch: 0116 loss_train: 0.0123 loss_val: 0.0004 time: 0.0068s\n",
      "Epoch: 0117 loss_train: 0.0130 loss_val: 0.0004 time: 0.0047s\n",
      "Epoch: 0118 loss_train: 0.0132 loss_val: 0.0004 time: 0.0043s\n",
      "Epoch: 0119 loss_train: 0.0124 loss_val: 0.0004 time: 0.0040s\n",
      "Epoch: 0120 loss_train: 0.0121 loss_val: 0.0004 time: 0.0043s\n",
      "Epoch: 0121 loss_train: 0.0120 loss_val: 0.0004 time: 0.0065s\n",
      "Epoch: 0122 loss_train: 0.0141 loss_val: 0.0004 time: 0.0043s\n",
      "Epoch: 0123 loss_train: 0.0113 loss_val: 0.0004 time: 0.0060s\n",
      "Epoch: 0124 loss_train: 0.0115 loss_val: 0.0004 time: 0.0072s\n",
      "Epoch: 0125 loss_train: 0.0116 loss_val: 0.0004 time: 0.0052s\n",
      "Epoch: 0126 loss_train: 0.0141 loss_val: 0.0004 time: 0.0039s\n",
      "Epoch: 0127 loss_train: 0.0129 loss_val: 0.0004 time: 0.0060s\n",
      "Epoch: 0128 loss_train: 0.0117 loss_val: 0.0004 time: 0.0066s\n",
      "Epoch: 0129 loss_train: 0.0139 loss_val: 0.0004 time: 0.0047s\n",
      "Epoch: 0130 loss_train: 0.0130 loss_val: 0.0004 time: 0.0060s\n",
      "Epoch: 0131 loss_train: 0.0141 loss_val: 0.0004 time: 0.0095s\n",
      "Epoch: 0132 loss_train: 0.0116 loss_val: 0.0004 time: 0.0042s\n",
      "Epoch: 0133 loss_train: 0.0137 loss_val: 0.0004 time: 0.0065s\n",
      "Epoch: 0134 loss_train: 0.0107 loss_val: 0.0004 time: 0.0063s\n",
      "Epoch: 0135 loss_train: 0.0115 loss_val: 0.0004 time: 0.0045s\n",
      "Epoch: 0136 loss_train: 0.0118 loss_val: 0.0004 time: 0.0048s\n",
      "Epoch: 0137 loss_train: 0.0126 loss_val: 0.0004 time: 0.0068s\n",
      "Epoch: 0138 loss_train: 0.0136 loss_val: 0.0004 time: 0.0058s\n",
      "Epoch: 0139 loss_train: 0.0130 loss_val: 0.0004 time: 0.0041s\n",
      "Epoch: 0140 loss_train: 0.0131 loss_val: 0.0004 time: 0.0044s\n",
      "Epoch: 0141 loss_train: 0.0125 loss_val: 0.0004 time: 0.0073s\n",
      "Epoch: 0142 loss_train: 0.0110 loss_val: 0.0004 time: 0.0049s\n",
      "Epoch: 0143 loss_train: 0.0123 loss_val: 0.0004 time: 0.0059s\n",
      "Epoch: 0144 loss_train: 0.0123 loss_val: 0.0004 time: 0.0059s\n",
      "Epoch: 0145 loss_train: 0.0117 loss_val: 0.0004 time: 0.0059s\n",
      "Epoch: 0146 loss_train: 0.0129 loss_val: 0.0004 time: 0.0065s\n",
      "Epoch: 0147 loss_train: 0.0122 loss_val: 0.0004 time: 0.0044s\n",
      "Epoch: 0148 loss_train: 0.0122 loss_val: 0.0004 time: 0.0065s\n",
      "Epoch: 0149 loss_train: 0.0120 loss_val: 0.0004 time: 0.0065s\n",
      "Epoch: 0150 loss_train: 0.0116 loss_val: 0.0004 time: 0.0077s\n",
      "Epoch: 0151 loss_train: 0.0130 loss_val: 0.0004 time: 0.0043s\n",
      "Epoch: 0152 loss_train: 0.0108 loss_val: 0.0004 time: 0.0070s\n",
      "Epoch: 0153 loss_train: 0.0118 loss_val: 0.0004 time: 0.0045s\n",
      "Epoch: 0154 loss_train: 0.0112 loss_val: 0.0004 time: 0.0065s\n",
      "Epoch: 0155 loss_train: 0.0141 loss_val: 0.0004 time: 0.0055s\n",
      "Epoch: 0156 loss_train: 0.0143 loss_val: 0.0004 time: 0.0042s\n",
      "Epoch: 0157 loss_train: 0.0142 loss_val: 0.0004 time: 0.0064s\n",
      "Epoch: 0158 loss_train: 0.0127 loss_val: 0.0004 time: 0.0065s\n",
      "Epoch: 0159 loss_train: 0.0120 loss_val: 0.0004 time: 0.0078s\n",
      "Epoch: 0160 loss_train: 0.0122 loss_val: 0.0004 time: 0.0094s\n",
      "Epoch: 0161 loss_train: 0.0113 loss_val: 0.0004 time: 0.0050s\n",
      "Epoch: 0162 loss_train: 0.0111 loss_val: 0.0004 time: 0.0048s\n",
      "Epoch: 0163 loss_train: 0.0117 loss_val: 0.0004 time: 0.0043s\n",
      "Epoch: 0164 loss_train: 0.0118 loss_val: 0.0004 time: 0.0041s\n",
      "Epoch: 0165 loss_train: 0.0121 loss_val: 0.0004 time: 0.0044s\n",
      "Epoch: 0166 loss_train: 0.0133 loss_val: 0.0004 time: 0.0046s\n",
      "Epoch: 0167 loss_train: 0.0109 loss_val: 0.0004 time: 0.0050s\n",
      "Epoch: 0168 loss_train: 0.0116 loss_val: 0.0004 time: 0.0044s\n",
      "Epoch: 0169 loss_train: 0.0115 loss_val: 0.0004 time: 0.0061s\n",
      "Epoch: 0170 loss_train: 0.0121 loss_val: 0.0004 time: 0.0054s\n",
      "Epoch: 0171 loss_train: 0.0110 loss_val: 0.0004 time: 0.0058s\n",
      "Epoch: 0172 loss_train: 0.0115 loss_val: 0.0004 time: 0.0070s\n",
      "Epoch: 0173 loss_train: 0.0119 loss_val: 0.0004 time: 0.0049s\n",
      "Epoch: 0174 loss_train: 0.0110 loss_val: 0.0004 time: 0.0054s\n",
      "Epoch: 0175 loss_train: 0.0120 loss_val: 0.0004 time: 0.0046s\n",
      "Epoch: 0176 loss_train: 0.0109 loss_val: 0.0004 time: 0.0067s\n",
      "Epoch: 0177 loss_train: 0.0117 loss_val: 0.0004 time: 0.0052s\n",
      "Epoch: 0178 loss_train: 0.0124 loss_val: 0.0005 time: 0.0069s\n",
      "Epoch: 0179 loss_train: 0.0118 loss_val: 0.0005 time: 0.0060s\n",
      "Epoch: 0180 loss_train: 0.0123 loss_val: 0.0005 time: 0.0041s\n",
      "Epoch: 0181 loss_train: 0.0114 loss_val: 0.0005 time: 0.0040s\n",
      "Epoch: 0182 loss_train: 0.0117 loss_val: 0.0005 time: 0.0040s\n",
      "Epoch: 0183 loss_train: 0.0129 loss_val: 0.0005 time: 0.0039s\n",
      "Epoch: 0184 loss_train: 0.0139 loss_val: 0.0005 time: 0.0047s\n",
      "Epoch: 0185 loss_train: 0.0120 loss_val: 0.0005 time: 0.0060s\n",
      "Epoch: 0186 loss_train: 0.0130 loss_val: 0.0005 time: 0.0070s\n",
      "Epoch: 0187 loss_train: 0.0116 loss_val: 0.0005 time: 0.0099s\n",
      "Epoch: 0188 loss_train: 0.0134 loss_val: 0.0005 time: 0.0050s\n",
      "Epoch: 0189 loss_train: 0.0127 loss_val: 0.0005 time: 0.0054s\n",
      "Epoch: 0190 loss_train: 0.0128 loss_val: 0.0005 time: 0.0041s\n",
      "Epoch: 0191 loss_train: 0.0124 loss_val: 0.0005 time: 0.0041s\n",
      "Epoch: 0192 loss_train: 0.0127 loss_val: 0.0005 time: 0.0041s\n",
      "Epoch: 0193 loss_train: 0.0124 loss_val: 0.0005 time: 0.0069s\n",
      "Epoch: 0194 loss_train: 0.0114 loss_val: 0.0005 time: 0.0050s\n",
      "Epoch: 0195 loss_train: 0.0116 loss_val: 0.0005 time: 0.0041s\n",
      "Epoch: 0196 loss_train: 0.0126 loss_val: 0.0005 time: 0.0040s\n",
      "Epoch: 0197 loss_train: 0.0123 loss_val: 0.0005 time: 0.0066s\n",
      "Epoch: 0198 loss_train: 0.0103 loss_val: 0.0005 time: 0.0069s\n",
      "Epoch: 0199 loss_train: 0.0109 loss_val: 0.0005 time: 0.0044s\n",
      "Epoch: 0200 loss_train: 0.0116 loss_val: 0.0005 time: 0.0058s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1.4751s\n",
      "Test set results: loss= 0.0005\n",
      "[graph 3]\n",
      "Loading sample-plan-3 dataset...\n",
      "torch.Size([176, 3])\n",
      "124\n",
      "Epoch: 0001 loss_train: 0.0050 loss_val: 0.0005 time: 0.0075s\n",
      "Epoch: 0002 loss_train: 0.0050 loss_val: 0.0005 time: 0.0071s\n",
      "Epoch: 0003 loss_train: 0.0049 loss_val: 0.0005 time: 0.0056s\n",
      "Epoch: 0004 loss_train: 0.0050 loss_val: 0.0005 time: 0.0073s\n",
      "Epoch: 0005 loss_train: 0.0050 loss_val: 0.0004 time: 0.0090s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: Using a target size (torch.Size([240])) that is different to the input size (torch.Size([240, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0006 loss_train: 0.0049 loss_val: 0.0004 time: 0.0106s\n",
      "Epoch: 0007 loss_train: 0.0050 loss_val: 0.0004 time: 0.0095s\n",
      "Epoch: 0008 loss_train: 0.0050 loss_val: 0.0004 time: 0.0056s\n",
      "Epoch: 0009 loss_train: 0.0050 loss_val: 0.0004 time: 0.0055s\n",
      "Epoch: 0010 loss_train: 0.0050 loss_val: 0.0004 time: 0.0073s\n",
      "Epoch: 0011 loss_train: 0.0050 loss_val: 0.0004 time: 0.0063s\n",
      "Epoch: 0012 loss_train: 0.0050 loss_val: 0.0004 time: 0.0060s\n",
      "Epoch: 0013 loss_train: 0.0050 loss_val: 0.0004 time: 0.0054s\n",
      "Epoch: 0014 loss_train: 0.0050 loss_val: 0.0004 time: 0.0080s\n",
      "Epoch: 0015 loss_train: 0.0050 loss_val: 0.0004 time: 0.0055s\n",
      "Epoch: 0016 loss_train: 0.0050 loss_val: 0.0004 time: 0.0053s\n",
      "Epoch: 0017 loss_train: 0.0050 loss_val: 0.0004 time: 0.0083s\n",
      "Epoch: 0018 loss_train: 0.0049 loss_val: 0.0004 time: 0.0093s\n",
      "Epoch: 0019 loss_train: 0.0050 loss_val: 0.0004 time: 0.0066s\n",
      "Epoch: 0020 loss_train: 0.0050 loss_val: 0.0004 time: 0.0057s\n",
      "Epoch: 0021 loss_train: 0.0050 loss_val: 0.0004 time: 0.0060s\n",
      "Epoch: 0022 loss_train: 0.0049 loss_val: 0.0004 time: 0.0076s\n",
      "Epoch: 0023 loss_train: 0.0050 loss_val: 0.0004 time: 0.0120s\n",
      "Epoch: 0024 loss_train: 0.0049 loss_val: 0.0004 time: 0.0083s\n",
      "Epoch: 0025 loss_train: 0.0050 loss_val: 0.0004 time: 0.0062s\n",
      "Epoch: 0026 loss_train: 0.0049 loss_val: 0.0004 time: 0.0055s\n",
      "Epoch: 0027 loss_train: 0.0049 loss_val: 0.0003 time: 0.0056s\n",
      "Epoch: 0028 loss_train: 0.0050 loss_val: 0.0003 time: 0.0063s\n",
      "Epoch: 0029 loss_train: 0.0049 loss_val: 0.0003 time: 0.0060s\n",
      "Epoch: 0030 loss_train: 0.0049 loss_val: 0.0003 time: 0.0105s\n",
      "Epoch: 0031 loss_train: 0.0051 loss_val: 0.0003 time: 0.0093s\n",
      "Epoch: 0032 loss_train: 0.0049 loss_val: 0.0003 time: 0.0078s\n",
      "Epoch: 0033 loss_train: 0.0049 loss_val: 0.0003 time: 0.0057s\n",
      "Epoch: 0034 loss_train: 0.0050 loss_val: 0.0003 time: 0.0068s\n",
      "Epoch: 0035 loss_train: 0.0050 loss_val: 0.0003 time: 0.0058s\n",
      "Epoch: 0036 loss_train: 0.0050 loss_val: 0.0003 time: 0.0057s\n",
      "Epoch: 0037 loss_train: 0.0050 loss_val: 0.0003 time: 0.0068s\n",
      "Epoch: 0038 loss_train: 0.0050 loss_val: 0.0003 time: 0.0101s\n",
      "Epoch: 0039 loss_train: 0.0050 loss_val: 0.0003 time: 0.0099s\n",
      "Epoch: 0040 loss_train: 0.0050 loss_val: 0.0003 time: 0.0086s\n",
      "Epoch: 0041 loss_train: 0.0050 loss_val: 0.0003 time: 0.0063s\n",
      "Epoch: 0042 loss_train: 0.0050 loss_val: 0.0003 time: 0.0055s\n",
      "Epoch: 0043 loss_train: 0.0049 loss_val: 0.0003 time: 0.0069s\n",
      "Epoch: 0044 loss_train: 0.0050 loss_val: 0.0003 time: 0.0054s\n",
      "Epoch: 0045 loss_train: 0.0050 loss_val: 0.0003 time: 0.0070s\n",
      "Epoch: 0046 loss_train: 0.0050 loss_val: 0.0003 time: 0.0066s\n",
      "Epoch: 0047 loss_train: 0.0050 loss_val: 0.0003 time: 0.0067s\n",
      "Epoch: 0048 loss_train: 0.0050 loss_val: 0.0003 time: 0.0053s\n",
      "Epoch: 0049 loss_train: 0.0049 loss_val: 0.0003 time: 0.0067s\n",
      "Epoch: 0050 loss_train: 0.0050 loss_val: 0.0003 time: 0.0067s\n",
      "Epoch: 0051 loss_train: 0.0050 loss_val: 0.0003 time: 0.0055s\n",
      "Epoch: 0052 loss_train: 0.0049 loss_val: 0.0003 time: 0.0054s\n",
      "Epoch: 0053 loss_train: 0.0050 loss_val: 0.0003 time: 0.0053s\n",
      "Epoch: 0054 loss_train: 0.0050 loss_val: 0.0003 time: 0.0141s\n",
      "Epoch: 0055 loss_train: 0.0050 loss_val: 0.0003 time: 0.0104s\n",
      "Epoch: 0056 loss_train: 0.0049 loss_val: 0.0003 time: 0.0099s\n",
      "Epoch: 0057 loss_train: 0.0050 loss_val: 0.0003 time: 0.0069s\n",
      "Epoch: 0058 loss_train: 0.0050 loss_val: 0.0003 time: 0.0058s\n",
      "Epoch: 0059 loss_train: 0.0050 loss_val: 0.0003 time: 0.0054s\n",
      "Epoch: 0060 loss_train: 0.0051 loss_val: 0.0003 time: 0.0080s\n",
      "Epoch: 0061 loss_train: 0.0050 loss_val: 0.0003 time: 0.0074s\n",
      "Epoch: 0062 loss_train: 0.0049 loss_val: 0.0003 time: 0.0056s\n",
      "Epoch: 0063 loss_train: 0.0050 loss_val: 0.0003 time: 0.0058s\n",
      "Epoch: 0064 loss_train: 0.0049 loss_val: 0.0003 time: 0.0061s\n",
      "Epoch: 0065 loss_train: 0.0049 loss_val: 0.0003 time: 0.0078s\n",
      "Epoch: 0066 loss_train: 0.0050 loss_val: 0.0003 time: 0.0061s\n",
      "Epoch: 0067 loss_train: 0.0049 loss_val: 0.0003 time: 0.0067s\n",
      "Epoch: 0068 loss_train: 0.0050 loss_val: 0.0003 time: 0.0078s\n",
      "Epoch: 0069 loss_train: 0.0050 loss_val: 0.0003 time: 0.0054s\n",
      "Epoch: 0070 loss_train: 0.0049 loss_val: 0.0003 time: 0.0054s\n",
      "Epoch: 0071 loss_train: 0.0050 loss_val: 0.0003 time: 0.0061s\n",
      "Epoch: 0072 loss_train: 0.0049 loss_val: 0.0003 time: 0.0067s\n",
      "Epoch: 0073 loss_train: 0.0050 loss_val: 0.0003 time: 0.0095s\n",
      "Epoch: 0074 loss_train: 0.0050 loss_val: 0.0003 time: 0.0068s\n",
      "Epoch: 0075 loss_train: 0.0049 loss_val: 0.0003 time: 0.0076s\n",
      "Epoch: 0076 loss_train: 0.0049 loss_val: 0.0003 time: 0.0059s\n",
      "Epoch: 0077 loss_train: 0.0049 loss_val: 0.0003 time: 0.0061s\n",
      "Epoch: 0078 loss_train: 0.0050 loss_val: 0.0003 time: 0.0076s\n",
      "Epoch: 0079 loss_train: 0.0049 loss_val: 0.0003 time: 0.0115s\n",
      "Epoch: 0080 loss_train: 0.0050 loss_val: 0.0003 time: 0.0075s\n",
      "Epoch: 0081 loss_train: 0.0050 loss_val: 0.0003 time: 0.0073s\n",
      "Epoch: 0082 loss_train: 0.0050 loss_val: 0.0003 time: 0.0080s\n",
      "Epoch: 0083 loss_train: 0.0049 loss_val: 0.0003 time: 0.0084s\n",
      "Epoch: 0084 loss_train: 0.0050 loss_val: 0.0003 time: 0.0081s\n",
      "Epoch: 0085 loss_train: 0.0049 loss_val: 0.0003 time: 0.0055s\n",
      "Epoch: 0086 loss_train: 0.0050 loss_val: 0.0003 time: 0.0057s\n",
      "Epoch: 0087 loss_train: 0.0050 loss_val: 0.0003 time: 0.0077s\n",
      "Epoch: 0088 loss_train: 0.0049 loss_val: 0.0003 time: 0.0071s\n",
      "Epoch: 0089 loss_train: 0.0049 loss_val: 0.0003 time: 0.0073s\n",
      "Epoch: 0090 loss_train: 0.0050 loss_val: 0.0003 time: 0.0080s\n",
      "Epoch: 0091 loss_train: 0.0049 loss_val: 0.0003 time: 0.0108s\n",
      "Epoch: 0092 loss_train: 0.0050 loss_val: 0.0003 time: 0.0077s\n",
      "Epoch: 0093 loss_train: 0.0051 loss_val: 0.0003 time: 0.0070s\n",
      "Epoch: 0094 loss_train: 0.0050 loss_val: 0.0003 time: 0.0056s\n",
      "Epoch: 0095 loss_train: 0.0050 loss_val: 0.0003 time: 0.0053s\n",
      "Epoch: 0096 loss_train: 0.0049 loss_val: 0.0003 time: 0.0053s\n",
      "Epoch: 0097 loss_train: 0.0050 loss_val: 0.0003 time: 0.0061s\n",
      "Epoch: 0098 loss_train: 0.0050 loss_val: 0.0003 time: 0.0067s\n",
      "Epoch: 0099 loss_train: 0.0049 loss_val: 0.0003 time: 0.0061s\n",
      "Epoch: 0100 loss_train: 0.0050 loss_val: 0.0003 time: 0.0055s\n",
      "Epoch: 0101 loss_train: 0.0050 loss_val: 0.0003 time: 0.0079s\n",
      "Epoch: 0102 loss_train: 0.0050 loss_val: 0.0003 time: 0.0083s\n",
      "Epoch: 0103 loss_train: 0.0049 loss_val: 0.0003 time: 0.0099s\n",
      "Epoch: 0104 loss_train: 0.0049 loss_val: 0.0003 time: 0.0055s\n",
      "Epoch: 0105 loss_train: 0.0050 loss_val: 0.0003 time: 0.0061s\n",
      "Epoch: 0106 loss_train: 0.0050 loss_val: 0.0003 time: 0.0081s\n",
      "Epoch: 0107 loss_train: 0.0051 loss_val: 0.0003 time: 0.0077s\n",
      "Epoch: 0108 loss_train: 0.0049 loss_val: 0.0003 time: 0.0079s\n",
      "Epoch: 0109 loss_train: 0.0050 loss_val: 0.0003 time: 0.0066s\n",
      "Epoch: 0110 loss_train: 0.0050 loss_val: 0.0003 time: 0.0054s\n",
      "Epoch: 0111 loss_train: 0.0050 loss_val: 0.0003 time: 0.0061s\n",
      "Epoch: 0112 loss_train: 0.0050 loss_val: 0.0003 time: 0.0079s\n",
      "Epoch: 0113 loss_train: 0.0050 loss_val: 0.0003 time: 0.0061s\n",
      "Epoch: 0114 loss_train: 0.0051 loss_val: 0.0003 time: 0.0054s\n",
      "Epoch: 0115 loss_train: 0.0050 loss_val: 0.0003 time: 0.0062s\n",
      "Epoch: 0116 loss_train: 0.0050 loss_val: 0.0003 time: 0.0068s\n",
      "Epoch: 0117 loss_train: 0.0050 loss_val: 0.0003 time: 0.0073s\n",
      "Epoch: 0118 loss_train: 0.0050 loss_val: 0.0003 time: 0.0106s\n",
      "Epoch: 0119 loss_train: 0.0050 loss_val: 0.0003 time: 0.0058s\n",
      "Epoch: 0120 loss_train: 0.0049 loss_val: 0.0003 time: 0.0056s\n",
      "Epoch: 0121 loss_train: 0.0049 loss_val: 0.0003 time: 0.0065s\n",
      "Epoch: 0122 loss_train: 0.0050 loss_val: 0.0003 time: 0.0086s\n",
      "Epoch: 0123 loss_train: 0.0049 loss_val: 0.0003 time: 0.0063s\n",
      "Epoch: 0124 loss_train: 0.0049 loss_val: 0.0003 time: 0.0076s\n",
      "Epoch: 0125 loss_train: 0.0049 loss_val: 0.0003 time: 0.0085s\n",
      "Epoch: 0126 loss_train: 0.0050 loss_val: 0.0003 time: 0.0123s\n",
      "Epoch: 0127 loss_train: 0.0050 loss_val: 0.0003 time: 0.0089s\n",
      "Epoch: 0128 loss_train: 0.0049 loss_val: 0.0003 time: 0.0060s\n",
      "Epoch: 0129 loss_train: 0.0049 loss_val: 0.0003 time: 0.0086s\n",
      "Epoch: 0130 loss_train: 0.0049 loss_val: 0.0003 time: 0.0059s\n",
      "Epoch: 0131 loss_train: 0.0049 loss_val: 0.0003 time: 0.0074s\n",
      "Epoch: 0132 loss_train: 0.0049 loss_val: 0.0003 time: 0.0056s\n",
      "Epoch: 0133 loss_train: 0.0049 loss_val: 0.0003 time: 0.0079s\n",
      "Epoch: 0134 loss_train: 0.0050 loss_val: 0.0003 time: 0.0070s\n",
      "Epoch: 0135 loss_train: 0.0049 loss_val: 0.0003 time: 0.0086s\n",
      "Epoch: 0136 loss_train: 0.0049 loss_val: 0.0003 time: 0.0076s\n",
      "Epoch: 0137 loss_train: 0.0050 loss_val: 0.0003 time: 0.0088s\n",
      "Epoch: 0138 loss_train: 0.0049 loss_val: 0.0003 time: 0.0083s\n",
      "Epoch: 0139 loss_train: 0.0049 loss_val: 0.0003 time: 0.0058s\n",
      "Epoch: 0140 loss_train: 0.0049 loss_val: 0.0003 time: 0.0069s\n",
      "Epoch: 0141 loss_train: 0.0050 loss_val: 0.0003 time: 0.0086s\n",
      "Epoch: 0142 loss_train: 0.0049 loss_val: 0.0003 time: 0.0121s\n",
      "Epoch: 0143 loss_train: 0.0050 loss_val: 0.0003 time: 0.0088s\n",
      "Epoch: 0144 loss_train: 0.0051 loss_val: 0.0003 time: 0.0092s\n",
      "Epoch: 0145 loss_train: 0.0050 loss_val: 0.0003 time: 0.0089s\n",
      "Epoch: 0146 loss_train: 0.0049 loss_val: 0.0003 time: 0.0060s\n",
      "Epoch: 0147 loss_train: 0.0049 loss_val: 0.0003 time: 0.0085s\n",
      "Epoch: 0148 loss_train: 0.0049 loss_val: 0.0003 time: 0.0097s\n",
      "Epoch: 0149 loss_train: 0.0049 loss_val: 0.0003 time: 0.0108s\n",
      "Epoch: 0150 loss_train: 0.0050 loss_val: 0.0003 time: 0.0112s\n",
      "Epoch: 0151 loss_train: 0.0050 loss_val: 0.0003 time: 0.0082s\n",
      "Epoch: 0152 loss_train: 0.0049 loss_val: 0.0003 time: 0.0097s\n",
      "Epoch: 0153 loss_train: 0.0049 loss_val: 0.0003 time: 0.0089s\n",
      "Epoch: 0154 loss_train: 0.0050 loss_val: 0.0003 time: 0.0063s\n",
      "Epoch: 0155 loss_train: 0.0050 loss_val: 0.0003 time: 0.0078s\n",
      "Epoch: 0156 loss_train: 0.0049 loss_val: 0.0003 time: 0.0060s\n",
      "Epoch: 0157 loss_train: 0.0050 loss_val: 0.0003 time: 0.0062s\n",
      "Epoch: 0158 loss_train: 0.0049 loss_val: 0.0003 time: 0.0076s\n",
      "Epoch: 0159 loss_train: 0.0049 loss_val: 0.0003 time: 0.0083s\n",
      "Epoch: 0160 loss_train: 0.0050 loss_val: 0.0003 time: 0.0064s\n",
      "Epoch: 0161 loss_train: 0.0050 loss_val: 0.0003 time: 0.0056s\n",
      "Epoch: 0162 loss_train: 0.0050 loss_val: 0.0003 time: 0.0069s\n",
      "Epoch: 0163 loss_train: 0.0049 loss_val: 0.0003 time: 0.0084s\n",
      "Epoch: 0164 loss_train: 0.0050 loss_val: 0.0003 time: 0.0095s\n",
      "Epoch: 0165 loss_train: 0.0050 loss_val: 0.0003 time: 0.0058s\n",
      "Epoch: 0166 loss_train: 0.0050 loss_val: 0.0003 time: 0.0055s\n",
      "Epoch: 0167 loss_train: 0.0050 loss_val: 0.0003 time: 0.0056s\n",
      "Epoch: 0168 loss_train: 0.0049 loss_val: 0.0003 time: 0.0055s\n",
      "Epoch: 0169 loss_train: 0.0049 loss_val: 0.0003 time: 0.0076s\n",
      "Epoch: 0170 loss_train: 0.0049 loss_val: 0.0003 time: 0.0058s\n",
      "Epoch: 0171 loss_train: 0.0050 loss_val: 0.0003 time: 0.0124s\n",
      "Epoch: 0172 loss_train: 0.0051 loss_val: 0.0003 time: 0.0083s\n",
      "Epoch: 0173 loss_train: 0.0050 loss_val: 0.0003 time: 0.0081s\n",
      "Epoch: 0174 loss_train: 0.0050 loss_val: 0.0003 time: 0.0067s\n",
      "Epoch: 0175 loss_train: 0.0050 loss_val: 0.0003 time: 0.0065s\n",
      "Epoch: 0176 loss_train: 0.0049 loss_val: 0.0003 time: 0.0069s\n",
      "Epoch: 0177 loss_train: 0.0050 loss_val: 0.0003 time: 0.0058s\n",
      "Epoch: 0178 loss_train: 0.0050 loss_val: 0.0003 time: 0.0057s\n",
      "Epoch: 0179 loss_train: 0.0050 loss_val: 0.0003 time: 0.0064s\n",
      "Epoch: 0180 loss_train: 0.0050 loss_val: 0.0003 time: 0.0084s\n",
      "Epoch: 0181 loss_train: 0.0050 loss_val: 0.0003 time: 0.0064s\n",
      "Epoch: 0182 loss_train: 0.0049 loss_val: 0.0003 time: 0.0070s\n",
      "Epoch: 0183 loss_train: 0.0050 loss_val: 0.0003 time: 0.0070s\n",
      "Epoch: 0184 loss_train: 0.0050 loss_val: 0.0003 time: 0.0062s\n",
      "Epoch: 0185 loss_train: 0.0049 loss_val: 0.0003 time: 0.0066s\n",
      "Epoch: 0186 loss_train: 0.0049 loss_val: 0.0003 time: 0.0075s\n",
      "Epoch: 0187 loss_train: 0.0049 loss_val: 0.0003 time: 0.0084s\n",
      "Epoch: 0188 loss_train: 0.0050 loss_val: 0.0003 time: 0.0061s\n",
      "Epoch: 0189 loss_train: 0.0049 loss_val: 0.0003 time: 0.0064s\n",
      "Epoch: 0190 loss_train: 0.0050 loss_val: 0.0003 time: 0.0063s\n",
      "Epoch: 0191 loss_train: 0.0049 loss_val: 0.0003 time: 0.0058s\n",
      "Epoch: 0192 loss_train: 0.0050 loss_val: 0.0003 time: 0.0056s\n",
      "Epoch: 0193 loss_train: 0.0050 loss_val: 0.0003 time: 0.0056s\n",
      "Epoch: 0194 loss_train: 0.0050 loss_val: 0.0003 time: 0.0079s\n",
      "Epoch: 0195 loss_train: 0.0049 loss_val: 0.0003 time: 0.0127s\n",
      "Epoch: 0196 loss_train: 0.0050 loss_val: 0.0003 time: 0.0088s\n",
      "Epoch: 0197 loss_train: 0.0050 loss_val: 0.0003 time: 0.0060s\n",
      "Epoch: 0198 loss_train: 0.0049 loss_val: 0.0003 time: 0.0079s\n",
      "Epoch: 0199 loss_train: 0.0049 loss_val: 0.0003 time: 0.0056s\n",
      "Epoch: 0200 loss_train: 0.0050 loss_val: 0.0003 time: 0.0060s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1.7870s\n",
      "Test set results: loss= 0.0003\n",
      "[graph 4]\n",
      "Loading sample-plan-4 dataset...\n",
      "torch.Size([170, 3])\n",
      "130\n",
      "Epoch: 0001 loss_train: 0.0002 loss_val: 0.0003 time: 0.0074s\n",
      "Epoch: 0002 loss_train: 0.0002 loss_val: 0.0003 time: 0.0075s\n",
      "Epoch: 0003 loss_train: 0.0002 loss_val: 0.0003 time: 0.0082s\n",
      "Epoch: 0004 loss_train: 0.0002 loss_val: 0.0002 time: 0.0073s\n",
      "Epoch: 0005 loss_train: 0.0002 loss_val: 0.0002 time: 0.0075s\n",
      "Epoch: 0006 loss_train: 0.0002 loss_val: 0.0002 time: 0.0082s\n",
      "Epoch: 0007 loss_train: 0.0002 loss_val: 0.0002 time: 0.0080s\n",
      "Epoch: 0008 loss_train: 0.0002 loss_val: 0.0002 time: 0.0069s\n",
      "Epoch: 0009 loss_train: 0.0002 loss_val: 0.0002 time: 0.0103s\n",
      "Epoch: 0010 loss_train: 0.0001 loss_val: 0.0002 time: 0.0079s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: Using a target size (torch.Size([240])) that is different to the input size (torch.Size([240, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0011 loss_train: 0.0002 loss_val: 0.0002 time: 0.0124s\n",
      "Epoch: 0012 loss_train: 0.0002 loss_val: 0.0002 time: 0.0085s\n",
      "Epoch: 0013 loss_train: 0.0002 loss_val: 0.0002 time: 0.0055s\n",
      "Epoch: 0014 loss_train: 0.0002 loss_val: 0.0002 time: 0.0068s\n",
      "Epoch: 0015 loss_train: 0.0002 loss_val: 0.0002 time: 0.0055s\n",
      "Epoch: 0016 loss_train: 0.0001 loss_val: 0.0002 time: 0.0070s\n",
      "Epoch: 0017 loss_train: 0.0001 loss_val: 0.0002 time: 0.0116s\n",
      "Epoch: 0018 loss_train: 0.0002 loss_val: 0.0002 time: 0.0084s\n",
      "Epoch: 0019 loss_train: 0.0002 loss_val: 0.0002 time: 0.0062s\n",
      "Epoch: 0020 loss_train: 0.0002 loss_val: 0.0001 time: 0.0070s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.1887s\n",
      "Test set results: loss= 0.0001\n",
      "[graph 5]\n",
      "Loading sample-plan-5 dataset...\n",
      "torch.Size([174, 3])\n",
      "126\n",
      "Epoch: 0001 loss_train: 0.0048 loss_val: 0.0001 time: 0.0080s\n",
      "Epoch: 0002 loss_train: 0.0048 loss_val: 0.0001 time: 0.0074s\n",
      "Epoch: 0003 loss_train: 0.0048 loss_val: 0.0001 time: 0.0066s\n",
      "Epoch: 0004 loss_train: 0.0048 loss_val: 0.0001 time: 0.0064s\n",
      "Epoch: 0005 loss_train: 0.0048 loss_val: 0.0001 time: 0.0048s\n",
      "Epoch: 0006 loss_train: 0.0048 loss_val: 0.0001 time: 0.0048s\n",
      "Epoch: 0007 loss_train: 0.0048 loss_val: 0.0001 time: 0.0053s\n",
      "Epoch: 0008 loss_train: 0.0048 loss_val: 0.0001 time: 0.0081s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: Using a target size (torch.Size([240])) that is different to the input size (torch.Size([240, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0009 loss_train: 0.0048 loss_val: 0.0001 time: 0.0131s\n",
      "Epoch: 0010 loss_train: 0.0048 loss_val: 0.0001 time: 0.0077s\n",
      "Epoch: 0011 loss_train: 0.0048 loss_val: 0.0001 time: 0.0052s\n",
      "Epoch: 0012 loss_train: 0.0048 loss_val: 0.0001 time: 0.0062s\n",
      "Epoch: 0013 loss_train: 0.0048 loss_val: 0.0001 time: 0.0065s\n",
      "Epoch: 0014 loss_train: 0.0048 loss_val: 0.0001 time: 0.0053s\n",
      "Epoch: 0015 loss_train: 0.0048 loss_val: 0.0001 time: 0.0073s\n",
      "Epoch: 0016 loss_train: 0.0048 loss_val: 0.0001 time: 0.0081s\n",
      "Epoch: 0017 loss_train: 0.0048 loss_val: 0.0002 time: 0.0071s\n",
      "Epoch: 0018 loss_train: 0.0048 loss_val: 0.0002 time: 0.0096s\n",
      "Epoch: 0019 loss_train: 0.0048 loss_val: 0.0002 time: 0.0112s\n",
      "Epoch: 0020 loss_train: 0.0048 loss_val: 0.0002 time: 0.0080s\n",
      "Epoch: 0021 loss_train: 0.0048 loss_val: 0.0002 time: 0.0057s\n",
      "Epoch: 0022 loss_train: 0.0048 loss_val: 0.0002 time: 0.0052s\n",
      "Epoch: 0023 loss_train: 0.0048 loss_val: 0.0002 time: 0.0078s\n",
      "Epoch: 0024 loss_train: 0.0048 loss_val: 0.0002 time: 0.0054s\n",
      "Epoch: 0025 loss_train: 0.0048 loss_val: 0.0002 time: 0.0070s\n",
      "Epoch: 0026 loss_train: 0.0048 loss_val: 0.0002 time: 0.0050s\n",
      "Epoch: 0027 loss_train: 0.0048 loss_val: 0.0002 time: 0.0062s\n",
      "Epoch: 0028 loss_train: 0.0048 loss_val: 0.0002 time: 0.0054s\n",
      "Epoch: 0029 loss_train: 0.0048 loss_val: 0.0002 time: 0.0059s\n",
      "Epoch: 0030 loss_train: 0.0048 loss_val: 0.0002 time: 0.0049s\n",
      "Epoch: 0031 loss_train: 0.0048 loss_val: 0.0002 time: 0.0056s\n",
      "Epoch: 0032 loss_train: 0.0048 loss_val: 0.0002 time: 0.0056s\n",
      "Epoch: 0033 loss_train: 0.0048 loss_val: 0.0002 time: 0.0048s\n",
      "Epoch: 0034 loss_train: 0.0048 loss_val: 0.0002 time: 0.0113s\n",
      "Epoch: 0035 loss_train: 0.0048 loss_val: 0.0002 time: 0.0076s\n",
      "Epoch: 0036 loss_train: 0.0048 loss_val: 0.0002 time: 0.0087s\n",
      "Epoch: 0037 loss_train: 0.0048 loss_val: 0.0002 time: 0.0050s\n",
      "Epoch: 0038 loss_train: 0.0048 loss_val: 0.0002 time: 0.0075s\n",
      "Epoch: 0039 loss_train: 0.0048 loss_val: 0.0002 time: 0.0066s\n",
      "Epoch: 0040 loss_train: 0.0048 loss_val: 0.0002 time: 0.0062s\n",
      "Epoch: 0041 loss_train: 0.0048 loss_val: 0.0002 time: 0.0066s\n",
      "Epoch: 0042 loss_train: 0.0048 loss_val: 0.0002 time: 0.0050s\n",
      "Epoch: 0043 loss_train: 0.0048 loss_val: 0.0002 time: 0.0051s\n",
      "Epoch: 0044 loss_train: 0.0048 loss_val: 0.0002 time: 0.0080s\n",
      "Epoch: 0045 loss_train: 0.0048 loss_val: 0.0002 time: 0.0054s\n",
      "Epoch: 0046 loss_train: 0.0048 loss_val: 0.0002 time: 0.0050s\n",
      "Epoch: 0047 loss_train: 0.0048 loss_val: 0.0002 time: 0.0062s\n",
      "Epoch: 0048 loss_train: 0.0048 loss_val: 0.0002 time: 0.0058s\n",
      "Epoch: 0049 loss_train: 0.0048 loss_val: 0.0002 time: 0.0056s\n",
      "Epoch: 0050 loss_train: 0.0048 loss_val: 0.0002 time: 0.0051s\n",
      "Epoch: 0051 loss_train: 0.0048 loss_val: 0.0002 time: 0.0067s\n",
      "Epoch: 0052 loss_train: 0.0048 loss_val: 0.0002 time: 0.0076s\n",
      "Epoch: 0053 loss_train: 0.0048 loss_val: 0.0002 time: 0.0059s\n",
      "Epoch: 0054 loss_train: 0.0048 loss_val: 0.0002 time: 0.0064s\n",
      "Epoch: 0055 loss_train: 0.0048 loss_val: 0.0002 time: 0.0058s\n",
      "Epoch: 0056 loss_train: 0.0048 loss_val: 0.0002 time: 0.0094s\n",
      "Epoch: 0057 loss_train: 0.0048 loss_val: 0.0002 time: 0.0050s\n",
      "Epoch: 0058 loss_train: 0.0048 loss_val: 0.0002 time: 0.0073s\n",
      "Epoch: 0059 loss_train: 0.0048 loss_val: 0.0002 time: 0.0073s\n",
      "Epoch: 0060 loss_train: 0.0048 loss_val: 0.0002 time: 0.0123s\n",
      "Epoch: 0061 loss_train: 0.0048 loss_val: 0.0002 time: 0.0074s\n",
      "Epoch: 0062 loss_train: 0.0048 loss_val: 0.0002 time: 0.0063s\n",
      "Epoch: 0063 loss_train: 0.0048 loss_val: 0.0002 time: 0.0062s\n",
      "Epoch: 0064 loss_train: 0.0048 loss_val: 0.0002 time: 0.0076s\n",
      "Epoch: 0065 loss_train: 0.0048 loss_val: 0.0002 time: 0.0055s\n",
      "Epoch: 0066 loss_train: 0.0048 loss_val: 0.0002 time: 0.0078s\n",
      "Epoch: 0067 loss_train: 0.0048 loss_val: 0.0002 time: 0.0076s\n",
      "Epoch: 0068 loss_train: 0.0048 loss_val: 0.0002 time: 0.0059s\n",
      "Epoch: 0069 loss_train: 0.0048 loss_val: 0.0002 time: 0.0057s\n",
      "Epoch: 0070 loss_train: 0.0048 loss_val: 0.0002 time: 0.0076s\n",
      "Epoch: 0071 loss_train: 0.0048 loss_val: 0.0002 time: 0.0070s\n",
      "Epoch: 0072 loss_train: 0.0048 loss_val: 0.0002 time: 0.0054s\n",
      "Epoch: 0073 loss_train: 0.0048 loss_val: 0.0002 time: 0.0066s\n",
      "Epoch: 0074 loss_train: 0.0048 loss_val: 0.0002 time: 0.0085s\n",
      "Epoch: 0075 loss_train: 0.0048 loss_val: 0.0002 time: 0.0077s\n",
      "Epoch: 0076 loss_train: 0.0048 loss_val: 0.0002 time: 0.0071s\n",
      "Epoch: 0077 loss_train: 0.0048 loss_val: 0.0002 time: 0.0065s\n",
      "Epoch: 0078 loss_train: 0.0048 loss_val: 0.0002 time: 0.0060s\n",
      "Epoch: 0079 loss_train: 0.0048 loss_val: 0.0002 time: 0.0050s\n",
      "Epoch: 0080 loss_train: 0.0048 loss_val: 0.0002 time: 0.0050s\n",
      "Epoch: 0081 loss_train: 0.0048 loss_val: 0.0002 time: 0.0063s\n",
      "Epoch: 0082 loss_train: 0.0048 loss_val: 0.0002 time: 0.0049s\n",
      "Epoch: 0083 loss_train: 0.0048 loss_val: 0.0002 time: 0.0080s\n",
      "Epoch: 0084 loss_train: 0.0048 loss_val: 0.0002 time: 0.0072s\n",
      "Epoch: 0085 loss_train: 0.0048 loss_val: 0.0002 time: 0.0121s\n",
      "Epoch: 0086 loss_train: 0.0048 loss_val: 0.0002 time: 0.0067s\n",
      "Epoch: 0087 loss_train: 0.0048 loss_val: 0.0002 time: 0.0053s\n",
      "Epoch: 0088 loss_train: 0.0048 loss_val: 0.0002 time: 0.0050s\n",
      "Epoch: 0089 loss_train: 0.0048 loss_val: 0.0002 time: 0.0076s\n",
      "Epoch: 0090 loss_train: 0.0048 loss_val: 0.0002 time: 0.0093s\n",
      "Epoch: 0091 loss_train: 0.0048 loss_val: 0.0002 time: 0.0079s\n",
      "Epoch: 0092 loss_train: 0.0048 loss_val: 0.0002 time: 0.0058s\n",
      "Epoch: 0093 loss_train: 0.0048 loss_val: 0.0002 time: 0.0052s\n",
      "Epoch: 0094 loss_train: 0.0048 loss_val: 0.0002 time: 0.0073s\n",
      "Epoch: 0095 loss_train: 0.0048 loss_val: 0.0002 time: 0.0052s\n",
      "Epoch: 0096 loss_train: 0.0048 loss_val: 0.0002 time: 0.0053s\n",
      "Epoch: 0097 loss_train: 0.0048 loss_val: 0.0002 time: 0.0083s\n",
      "Epoch: 0098 loss_train: 0.0048 loss_val: 0.0002 time: 0.0054s\n",
      "Epoch: 0099 loss_train: 0.0048 loss_val: 0.0002 time: 0.0105s\n",
      "Epoch: 0100 loss_train: 0.0048 loss_val: 0.0002 time: 0.0097s\n",
      "Epoch: 0101 loss_train: 0.0048 loss_val: 0.0002 time: 0.0064s\n",
      "Epoch: 0102 loss_train: 0.0048 loss_val: 0.0002 time: 0.0051s\n",
      "Epoch: 0103 loss_train: 0.0048 loss_val: 0.0002 time: 0.0076s\n",
      "Epoch: 0104 loss_train: 0.0048 loss_val: 0.0002 time: 0.0070s\n",
      "Epoch: 0105 loss_train: 0.0048 loss_val: 0.0002 time: 0.0062s\n",
      "Epoch: 0106 loss_train: 0.0048 loss_val: 0.0002 time: 0.0073s\n",
      "Epoch: 0107 loss_train: 0.0048 loss_val: 0.0002 time: 0.0069s\n",
      "Epoch: 0108 loss_train: 0.0048 loss_val: 0.0002 time: 0.0080s\n",
      "Epoch: 0109 loss_train: 0.0048 loss_val: 0.0002 time: 0.0102s\n",
      "Epoch: 0110 loss_train: 0.0048 loss_val: 0.0002 time: 0.0064s\n",
      "Epoch: 0111 loss_train: 0.0048 loss_val: 0.0002 time: 0.0080s\n",
      "Epoch: 0112 loss_train: 0.0048 loss_val: 0.0002 time: 0.0062s\n",
      "Epoch: 0113 loss_train: 0.0048 loss_val: 0.0002 time: 0.0091s\n",
      "Epoch: 0114 loss_train: 0.0048 loss_val: 0.0002 time: 0.0074s\n",
      "Epoch: 0115 loss_train: 0.0048 loss_val: 0.0002 time: 0.0072s\n",
      "Epoch: 0116 loss_train: 0.0048 loss_val: 0.0002 time: 0.0052s\n",
      "Epoch: 0117 loss_train: 0.0048 loss_val: 0.0002 time: 0.0074s\n",
      "Epoch: 0118 loss_train: 0.0048 loss_val: 0.0002 time: 0.0058s\n",
      "Epoch: 0119 loss_train: 0.0048 loss_val: 0.0002 time: 0.0050s\n",
      "Epoch: 0120 loss_train: 0.0048 loss_val: 0.0002 time: 0.0076s\n",
      "Epoch: 0121 loss_train: 0.0048 loss_val: 0.0002 time: 0.0074s\n",
      "Epoch: 0122 loss_train: 0.0048 loss_val: 0.0002 time: 0.0067s\n",
      "Epoch: 0123 loss_train: 0.0048 loss_val: 0.0002 time: 0.0068s\n",
      "Epoch: 0124 loss_train: 0.0048 loss_val: 0.0002 time: 0.0066s\n",
      "Epoch: 0125 loss_train: 0.0048 loss_val: 0.0002 time: 0.0074s\n",
      "Epoch: 0126 loss_train: 0.0048 loss_val: 0.0002 time: 0.0067s\n",
      "Epoch: 0127 loss_train: 0.0048 loss_val: 0.0002 time: 0.0048s\n",
      "Epoch: 0128 loss_train: 0.0048 loss_val: 0.0002 time: 0.0078s\n",
      "Epoch: 0129 loss_train: 0.0048 loss_val: 0.0002 time: 0.0047s\n",
      "Epoch: 0130 loss_train: 0.0048 loss_val: 0.0002 time: 0.0064s\n",
      "Epoch: 0131 loss_train: 0.0048 loss_val: 0.0002 time: 0.0074s\n",
      "Epoch: 0132 loss_train: 0.0048 loss_val: 0.0002 time: 0.0049s\n",
      "Epoch: 0133 loss_train: 0.0048 loss_val: 0.0002 time: 0.0051s\n",
      "Epoch: 0134 loss_train: 0.0048 loss_val: 0.0002 time: 0.0111s\n",
      "Epoch: 0135 loss_train: 0.0048 loss_val: 0.0002 time: 0.0080s\n",
      "Epoch: 0136 loss_train: 0.0048 loss_val: 0.0002 time: 0.0075s\n",
      "Epoch: 0137 loss_train: 0.0048 loss_val: 0.0002 time: 0.0049s\n",
      "Epoch: 0138 loss_train: 0.0048 loss_val: 0.0002 time: 0.0062s\n",
      "Epoch: 0139 loss_train: 0.0048 loss_val: 0.0002 time: 0.0087s\n",
      "Epoch: 0140 loss_train: 0.0048 loss_val: 0.0002 time: 0.0065s\n",
      "Epoch: 0141 loss_train: 0.0048 loss_val: 0.0002 time: 0.0050s\n",
      "Epoch: 0142 loss_train: 0.0048 loss_val: 0.0002 time: 0.0058s\n",
      "Epoch: 0143 loss_train: 0.0048 loss_val: 0.0002 time: 0.0075s\n",
      "Epoch: 0144 loss_train: 0.0048 loss_val: 0.0002 time: 0.0050s\n",
      "Epoch: 0145 loss_train: 0.0048 loss_val: 0.0002 time: 0.0060s\n",
      "Epoch: 0146 loss_train: 0.0048 loss_val: 0.0002 time: 0.0048s\n",
      "Epoch: 0147 loss_train: 0.0048 loss_val: 0.0002 time: 0.0054s\n",
      "Epoch: 0148 loss_train: 0.0048 loss_val: 0.0002 time: 0.0050s\n",
      "Epoch: 0149 loss_train: 0.0048 loss_val: 0.0002 time: 0.0062s\n",
      "Epoch: 0150 loss_train: 0.0048 loss_val: 0.0002 time: 0.0099s\n",
      "Epoch: 0151 loss_train: 0.0048 loss_val: 0.0002 time: 0.0079s\n",
      "Epoch: 0152 loss_train: 0.0048 loss_val: 0.0002 time: 0.0053s\n",
      "Epoch: 0153 loss_train: 0.0048 loss_val: 0.0002 time: 0.0048s\n",
      "Epoch: 0154 loss_train: 0.0048 loss_val: 0.0002 time: 0.0072s\n",
      "Epoch: 0155 loss_train: 0.0048 loss_val: 0.0002 time: 0.0051s\n",
      "Epoch: 0156 loss_train: 0.0048 loss_val: 0.0002 time: 0.0050s\n",
      "Epoch: 0157 loss_train: 0.0048 loss_val: 0.0002 time: 0.0073s\n",
      "Epoch: 0158 loss_train: 0.0048 loss_val: 0.0002 time: 0.0050s\n",
      "Epoch: 0159 loss_train: 0.0048 loss_val: 0.0002 time: 0.0047s\n",
      "Epoch: 0160 loss_train: 0.0048 loss_val: 0.0002 time: 0.0134s\n",
      "Epoch: 0161 loss_train: 0.0048 loss_val: 0.0002 time: 0.0086s\n",
      "Epoch: 0162 loss_train: 0.0048 loss_val: 0.0002 time: 0.0048s\n",
      "Epoch: 0163 loss_train: 0.0048 loss_val: 0.0002 time: 0.0069s\n",
      "Epoch: 0164 loss_train: 0.0048 loss_val: 0.0002 time: 0.0046s\n",
      "Epoch: 0165 loss_train: 0.0048 loss_val: 0.0002 time: 0.0046s\n",
      "Epoch: 0166 loss_train: 0.0048 loss_val: 0.0002 time: 0.0080s\n",
      "Epoch: 0167 loss_train: 0.0048 loss_val: 0.0002 time: 0.0063s\n",
      "Epoch: 0168 loss_train: 0.0048 loss_val: 0.0002 time: 0.0059s\n",
      "Epoch: 0169 loss_train: 0.0048 loss_val: 0.0002 time: 0.0047s\n",
      "Epoch: 0170 loss_train: 0.0048 loss_val: 0.0002 time: 0.0055s\n",
      "Epoch: 0171 loss_train: 0.0048 loss_val: 0.0002 time: 0.0058s\n",
      "Epoch: 0172 loss_train: 0.0048 loss_val: 0.0002 time: 0.0054s\n",
      "Epoch: 0173 loss_train: 0.0048 loss_val: 0.0002 time: 0.0046s\n",
      "Epoch: 0174 loss_train: 0.0048 loss_val: 0.0002 time: 0.0053s\n",
      "Epoch: 0175 loss_train: 0.0048 loss_val: 0.0002 time: 0.0078s\n",
      "Epoch: 0176 loss_train: 0.0048 loss_val: 0.0002 time: 0.0077s\n",
      "Epoch: 0177 loss_train: 0.0048 loss_val: 0.0002 time: 0.0054s\n",
      "Epoch: 0178 loss_train: 0.0048 loss_val: 0.0002 time: 0.0071s\n",
      "Epoch: 0179 loss_train: 0.0048 loss_val: 0.0002 time: 0.0077s\n",
      "Epoch: 0180 loss_train: 0.0048 loss_val: 0.0002 time: 0.0046s\n",
      "Epoch: 0181 loss_train: 0.0048 loss_val: 0.0002 time: 0.0069s\n",
      "Epoch: 0182 loss_train: 0.0048 loss_val: 0.0002 time: 0.0072s\n",
      "Epoch: 0183 loss_train: 0.0048 loss_val: 0.0002 time: 0.0057s\n",
      "Epoch: 0184 loss_train: 0.0048 loss_val: 0.0002 time: 0.0051s\n",
      "Epoch: 0185 loss_train: 0.0048 loss_val: 0.0002 time: 0.0070s\n",
      "Epoch: 0186 loss_train: 0.0048 loss_val: 0.0002 time: 0.0078s\n",
      "Epoch: 0187 loss_train: 0.0048 loss_val: 0.0002 time: 0.0105s\n",
      "Epoch: 0188 loss_train: 0.0048 loss_val: 0.0002 time: 0.0082s\n",
      "Epoch: 0189 loss_train: 0.0048 loss_val: 0.0002 time: 0.0049s\n",
      "Epoch: 0190 loss_train: 0.0048 loss_val: 0.0002 time: 0.0066s\n",
      "Epoch: 0191 loss_train: 0.0048 loss_val: 0.0002 time: 0.0064s\n",
      "Epoch: 0192 loss_train: 0.0048 loss_val: 0.0002 time: 0.0047s\n",
      "Epoch: 0193 loss_train: 0.0048 loss_val: 0.0002 time: 0.0055s\n",
      "Epoch: 0194 loss_train: 0.0048 loss_val: 0.0002 time: 0.0046s\n",
      "Epoch: 0195 loss_train: 0.0048 loss_val: 0.0002 time: 0.0046s\n",
      "Epoch: 0196 loss_train: 0.0048 loss_val: 0.0002 time: 0.0046s\n",
      "Epoch: 0197 loss_train: 0.0048 loss_val: 0.0002 time: 0.0065s\n",
      "Epoch: 0198 loss_train: 0.0048 loss_val: 0.0002 time: 0.0051s\n",
      "Epoch: 0199 loss_train: 0.0048 loss_val: 0.0002 time: 0.0072s\n",
      "Epoch: 0200 loss_train: 0.0048 loss_val: 0.0002 time: 0.0050s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1.6563s\n",
      "Test set results: loss= 0.0002\n",
      "[graph 6]\n",
      "Loading sample-plan-6 dataset...\n",
      "torch.Size([174, 3])\n",
      "126\n",
      "Epoch: 0001 loss_train: 0.0008 loss_val: 0.0002 time: 0.0057s\n",
      "Epoch: 0002 loss_train: 0.0008 loss_val: 0.0002 time: 0.0060s\n",
      "Epoch: 0003 loss_train: 0.0008 loss_val: 0.0002 time: 0.0054s\n",
      "Epoch: 0004 loss_train: 0.0008 loss_val: 0.0002 time: 0.0044s\n",
      "Epoch: 0005 loss_train: 0.0008 loss_val: 0.0002 time: 0.0041s\n",
      "Epoch: 0006 loss_train: 0.0008 loss_val: 0.0002 time: 0.0041s\n",
      "Epoch: 0007 loss_train: 0.0008 loss_val: 0.0002 time: 0.0041s\n",
      "Epoch: 0008 loss_train: 0.0008 loss_val: 0.0002 time: 0.0041s\n",
      "Epoch: 0009 loss_train: 0.0008 loss_val: 0.0002 time: 0.0040s\n",
      "Epoch: 0010 loss_train: 0.0008 loss_val: 0.0003 time: 0.0052s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: Using a target size (torch.Size([240])) that is different to the input size (torch.Size([240, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0011 loss_train: 0.0008 loss_val: 0.0003 time: 0.0105s\n",
      "Epoch: 0012 loss_train: 0.0008 loss_val: 0.0003 time: 0.0087s\n",
      "Epoch: 0013 loss_train: 0.0008 loss_val: 0.0003 time: 0.0092s\n",
      "Epoch: 0014 loss_train: 0.0008 loss_val: 0.0003 time: 0.0107s\n",
      "Epoch: 0015 loss_train: 0.0008 loss_val: 0.0003 time: 0.0057s\n",
      "Epoch: 0016 loss_train: 0.0008 loss_val: 0.0003 time: 0.0076s\n",
      "Epoch: 0017 loss_train: 0.0008 loss_val: 0.0003 time: 0.0054s\n",
      "Epoch: 0018 loss_train: 0.0008 loss_val: 0.0003 time: 0.0053s\n",
      "Epoch: 0019 loss_train: 0.0007 loss_val: 0.0003 time: 0.0054s\n",
      "Epoch: 0020 loss_train: 0.0008 loss_val: 0.0003 time: 0.0058s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.1547s\n",
      "Test set results: loss= 0.0003\n",
      "[graph 7]\n",
      "Loading sample-plan-7 dataset...\n",
      "torch.Size([188, 3])\n",
      "112\n",
      "Epoch: 0001 loss_train: 1662.0377 loss_val: 0.0014 time: 0.0061s\n",
      "Epoch: 0002 loss_train: 31096.1738 loss_val: 0.0054 time: 0.0066s\n",
      "Epoch: 0003 loss_train: 11824.1689 loss_val: 0.0174 time: 0.0069s\n",
      "Epoch: 0004 loss_train: 3275.6399 loss_val: 0.0692 time: 0.0066s\n",
      "Epoch: 0005 loss_train: 6424.6831 loss_val: 0.0367 time: 0.0069s\n",
      "Epoch: 0006 loss_train: 148.5026 loss_val: 0.0069 time: 0.0040s\n",
      "Epoch: 0007 loss_train: 0.4686 loss_val: 0.0014 time: 0.0054s\n",
      "Epoch: 0008 loss_train: 0.9131 loss_val: 0.0273 time: 0.0046s\n",
      "Epoch: 0009 loss_train: 1.8895 loss_val: 0.0827 time: 0.0051s\n",
      "Epoch: 0010 loss_train: 8.4632 loss_val: 0.1484 time: 0.0048s\n",
      "Epoch: 0011 loss_train: 4.0927 loss_val: 0.2210 time: 0.0069s\n",
      "Epoch: 0012 loss_train: 8.5987 loss_val: 0.2727 time: 0.0040s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: Using a target size (torch.Size([240])) that is different to the input size (torch.Size([240, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0013 loss_train: 9.4484 loss_val: 0.2952 time: 0.0080s\n",
      "Epoch: 0014 loss_train: 11.6230 loss_val: 0.2809 time: 0.0053s\n",
      "Epoch: 0015 loss_train: 15.3619 loss_val: 0.2204 time: 0.0068s\n",
      "Epoch: 0016 loss_train: 20.4200 loss_val: 0.1296 time: 0.0052s\n",
      "Epoch: 0017 loss_train: 15.6287 loss_val: 0.0494 time: 0.0048s\n",
      "Epoch: 0018 loss_train: 13.4499 loss_val: 0.0083 time: 0.0046s\n",
      "Epoch: 0019 loss_train: 18.0887 loss_val: 0.0049 time: 0.0063s\n",
      "Epoch: 0020 loss_train: 11.4838 loss_val: 0.0389 time: 0.0039s\n",
      "Epoch: 0021 loss_train: 11.0122 loss_val: 0.0748 time: 0.0038s\n",
      "Epoch: 0022 loss_train: 9.0103 loss_val: 0.1192 time: 0.0063s\n",
      "Epoch: 0023 loss_train: 5.2801 loss_val: 0.1619 time: 0.0050s\n",
      "Epoch: 0024 loss_train: 2.2547 loss_val: 0.1865 time: 0.0039s\n",
      "Epoch: 0025 loss_train: 1.4388 loss_val: 0.1790 time: 0.0042s\n",
      "Epoch: 0026 loss_train: 1.1031 loss_val: 0.1474 time: 0.0092s\n",
      "Epoch: 0027 loss_train: 0.4837 loss_val: 0.1028 time: 0.0074s\n",
      "Epoch: 0028 loss_train: 0.1646 loss_val: 0.0570 time: 0.0068s\n",
      "Epoch: 0029 loss_train: 0.0493 loss_val: 0.0217 time: 0.0083s\n",
      "Epoch: 0030 loss_train: 0.1187 loss_val: 0.0029 time: 0.0043s\n",
      "Epoch: 0031 loss_train: 0.3025 loss_val: 0.0012 time: 0.0066s\n",
      "Epoch: 0032 loss_train: 0.5739 loss_val: 0.0120 time: 0.0063s\n",
      "Epoch: 0033 loss_train: 0.7769 loss_val: 0.0287 time: 0.0043s\n",
      "Epoch: 0034 loss_train: 1.2411 loss_val: 0.0413 time: 0.0041s\n",
      "Epoch: 0035 loss_train: 1.6344 loss_val: 0.0425 time: 0.0063s\n",
      "Epoch: 0036 loss_train: 2.1328 loss_val: 0.0288 time: 0.0072s\n",
      "Epoch: 0037 loss_train: 1.8054 loss_val: 0.0125 time: 0.0042s\n",
      "Epoch: 0038 loss_train: 1.5346 loss_val: 0.0021 time: 0.0041s\n",
      "Epoch: 0039 loss_train: 2.0479 loss_val: 0.0012 time: 0.0043s\n",
      "Epoch: 0040 loss_train: 1.4743 loss_val: 0.0131 time: 0.0065s\n",
      "Epoch: 0041 loss_train: 1.4286 loss_val: 0.0368 time: 0.0058s\n",
      "Epoch: 0042 loss_train: 0.9936 loss_val: 0.0652 time: 0.0096s\n",
      "Epoch: 0043 loss_train: 0.8577 loss_val: 0.0861 time: 0.0072s\n",
      "Epoch: 0044 loss_train: 0.6995 loss_val: 0.0917 time: 0.0044s\n",
      "Epoch: 0045 loss_train: 0.5777 loss_val: 0.0859 time: 0.0042s\n",
      "Epoch: 0046 loss_train: 0.3878 loss_val: 0.0696 time: 0.0048s\n",
      "Epoch: 0047 loss_train: 0.2751 loss_val: 0.0482 time: 0.0069s\n",
      "Epoch: 0048 loss_train: 0.1532 loss_val: 0.0267 time: 0.0055s\n",
      "Epoch: 0049 loss_train: 0.0839 loss_val: 0.0104 time: 0.0054s\n",
      "Epoch: 0050 loss_train: 0.0277 loss_val: 0.0017 time: 0.0061s\n",
      "Epoch: 0051 loss_train: 0.0038 loss_val: 0.0003 time: 0.0068s\n",
      "Epoch: 0052 loss_train: 0.0026 loss_val: 0.0041 time: 0.0046s\n",
      "Epoch: 0053 loss_train: 0.0204 loss_val: 0.0105 time: 0.0042s\n",
      "Epoch: 0054 loss_train: 0.0407 loss_val: 0.0158 time: 0.0041s\n",
      "Epoch: 0055 loss_train: 0.0696 loss_val: 0.0184 time: 0.0049s\n",
      "Epoch: 0056 loss_train: 0.0917 loss_val: 0.0176 time: 0.0041s\n",
      "Epoch: 0057 loss_train: 0.1088 loss_val: 0.0137 time: 0.0047s\n",
      "Epoch: 0058 loss_train: 0.1219 loss_val: 0.0086 time: 0.0062s\n",
      "Epoch: 0059 loss_train: 0.1273 loss_val: 0.0039 time: 0.0052s\n",
      "Epoch: 0060 loss_train: 0.1462 loss_val: 0.0010 time: 0.0040s\n",
      "Epoch: 0061 loss_train: 0.1052 loss_val: 0.0000 time: 0.0040s\n",
      "Epoch: 0062 loss_train: 0.1598 loss_val: 0.0010 time: 0.0056s\n",
      "Epoch: 0063 loss_train: 0.1611 loss_val: 0.0027 time: 0.0045s\n",
      "Epoch: 0064 loss_train: 0.0932 loss_val: 0.0048 time: 0.0039s\n",
      "Epoch: 0065 loss_train: 0.1222 loss_val: 0.0059 time: 0.0039s\n",
      "Epoch: 0066 loss_train: 0.1486 loss_val: 0.0053 time: 0.0062s\n",
      "Epoch: 0067 loss_train: 0.1125 loss_val: 0.0035 time: 0.0039s\n",
      "Epoch: 0068 loss_train: 0.0543 loss_val: 0.0018 time: 0.0038s\n",
      "Epoch: 0069 loss_train: 0.0513 loss_val: 0.0005 time: 0.0039s\n",
      "Epoch: 0070 loss_train: 0.0366 loss_val: 0.0000 time: 0.0095s\n",
      "Epoch: 0071 loss_train: 0.0135 loss_val: 0.0004 time: 0.0100s\n",
      "Epoch: 0072 loss_train: 0.0107 loss_val: 0.0012 time: 0.0071s\n",
      "Epoch: 0073 loss_train: 0.0043 loss_val: 0.0020 time: 0.0042s\n",
      "Epoch: 0074 loss_train: 0.0044 loss_val: 0.0023 time: 0.0042s\n",
      "Epoch: 0075 loss_train: 0.0060 loss_val: 0.0020 time: 0.0041s\n",
      "Epoch: 0076 loss_train: 0.0080 loss_val: 0.0012 time: 0.0074s\n",
      "Epoch: 0077 loss_train: 0.0082 loss_val: 0.0005 time: 0.0041s\n",
      "Epoch: 0078 loss_train: 0.0094 loss_val: 0.0000 time: 0.0055s\n",
      "Epoch: 0079 loss_train: 0.0156 loss_val: 0.0001 time: 0.0072s\n",
      "Epoch: 0080 loss_train: 0.0162 loss_val: 0.0007 time: 0.0046s\n",
      "Epoch: 0081 loss_train: 0.0168 loss_val: 0.0018 time: 0.0049s\n",
      "Epoch: 0082 loss_train: 0.0152 loss_val: 0.0028 time: 0.0047s\n",
      "Epoch: 0083 loss_train: 0.0246 loss_val: 0.0038 time: 0.0050s\n",
      "Epoch: 0084 loss_train: 0.0224 loss_val: 0.0044 time: 0.0040s\n",
      "Epoch: 0085 loss_train: 0.0162 loss_val: 0.0043 time: 0.0053s\n",
      "Epoch: 0086 loss_train: 0.0168 loss_val: 0.0038 time: 0.0050s\n",
      "Epoch: 0087 loss_train: 0.0149 loss_val: 0.0030 time: 0.0058s\n",
      "Epoch: 0088 loss_train: 0.0107 loss_val: 0.0020 time: 0.0057s\n",
      "Epoch: 0089 loss_train: 0.0111 loss_val: 0.0012 time: 0.0050s\n",
      "Epoch: 0090 loss_train: 0.0052 loss_val: 0.0006 time: 0.0041s\n",
      "Epoch: 0091 loss_train: 0.0105 loss_val: 0.0002 time: 0.0061s\n",
      "Epoch: 0092 loss_train: 0.0049 loss_val: 0.0001 time: 0.0044s\n",
      "Epoch: 0093 loss_train: 0.0033 loss_val: 0.0000 time: 0.0089s\n",
      "Epoch: 0094 loss_train: 0.0027 loss_val: 0.0000 time: 0.0055s\n",
      "Epoch: 0095 loss_train: 0.0020 loss_val: 0.0000 time: 0.0040s\n",
      "Epoch: 0096 loss_train: 0.0011 loss_val: 0.0000 time: 0.0046s\n",
      "Epoch: 0097 loss_train: 0.0009 loss_val: 0.0000 time: 0.0039s\n",
      "Epoch: 0098 loss_train: 0.0008 loss_val: 0.0001 time: 0.0154s\n",
      "Epoch: 0099 loss_train: 0.0009 loss_val: 0.0001 time: 0.0051s\n",
      "Epoch: 0100 loss_train: 0.0014 loss_val: 0.0002 time: 0.0040s\n",
      "Epoch: 0101 loss_train: 0.0013 loss_val: 0.0003 time: 0.0056s\n",
      "Epoch: 0102 loss_train: 0.0016 loss_val: 0.0004 time: 0.0064s\n",
      "Epoch: 0103 loss_train: 0.0020 loss_val: 0.0005 time: 0.0050s\n",
      "Epoch: 0104 loss_train: 0.0019 loss_val: 0.0006 time: 0.0042s\n",
      "Epoch: 0105 loss_train: 0.0024 loss_val: 0.0005 time: 0.0044s\n",
      "Epoch: 0106 loss_train: 0.0029 loss_val: 0.0005 time: 0.0041s\n",
      "Epoch: 0107 loss_train: 0.0020 loss_val: 0.0004 time: 0.0050s\n",
      "Epoch: 0108 loss_train: 0.0027 loss_val: 0.0003 time: 0.0045s\n",
      "Epoch: 0109 loss_train: 0.0027 loss_val: 0.0002 time: 0.0063s\n",
      "Epoch: 0110 loss_train: 0.0027 loss_val: 0.0002 time: 0.0054s\n",
      "Epoch: 0111 loss_train: 0.0020 loss_val: 0.0001 time: 0.0069s\n",
      "Epoch: 0112 loss_train: 0.0013 loss_val: 0.0001 time: 0.0045s\n",
      "Epoch: 0113 loss_train: 0.0012 loss_val: 0.0001 time: 0.0052s\n",
      "Epoch: 0114 loss_train: 0.0013 loss_val: 0.0001 time: 0.0041s\n",
      "Epoch: 0115 loss_train: 0.0011 loss_val: 0.0001 time: 0.0039s\n",
      "Epoch: 0116 loss_train: 0.0010 loss_val: 0.0001 time: 0.0039s\n",
      "Epoch: 0117 loss_train: 0.0008 loss_val: 0.0001 time: 0.0053s\n",
      "Epoch: 0118 loss_train: 0.0007 loss_val: 0.0002 time: 0.0103s\n",
      "Epoch: 0119 loss_train: 0.0007 loss_val: 0.0002 time: 0.0076s\n",
      "Epoch: 0120 loss_train: 0.0006 loss_val: 0.0003 time: 0.0059s\n",
      "Epoch: 0121 loss_train: 0.0006 loss_val: 0.0004 time: 0.0074s\n",
      "Epoch: 0122 loss_train: 0.0007 loss_val: 0.0005 time: 0.0056s\n",
      "Epoch: 0123 loss_train: 0.0007 loss_val: 0.0005 time: 0.0041s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.9230s\n",
      "Test set results: loss= 0.0005\n",
      "Loading sample-plan-8 dataset...\n",
      "torch.Size([173, 3])\n",
      "127\n",
      "Test set results: loss= 0.0005\n",
      "Testing Finished!\n",
      "Total time elapsed: 0.0029s\n",
      "Loading sample-plan-9 dataset...\n",
      "torch.Size([175, 3])\n",
      "125\n",
      "Test set results: loss= 0.0005\n",
      "Testing Finished!\n",
      "Total time elapsed: 0.0026s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train(epoch, labels, features, adj, idx_train):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    # print(output[idx_train])\n",
    "    \n",
    "    loss_train = F.mse_loss(output[idx_train], labels[idx_train])\n",
    "    \n",
    "    # loss_train = nn.CrossEntropyLoss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    # https://www.cnblogs.com/52dxer/p/13793911.html\n",
    "    loss_val = F.mse_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    \n",
    "    return round(loss_train.item(), 4)\n",
    "\n",
    "def test(labels, features, adj, idx_test):\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    \n",
    "    loss_test = F.mse_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()))\n",
    "    \n",
    "    return loss_test.item()\n",
    "\n",
    "# Step-3:     \n",
    "data_path = 'tpcc_10_1/'\n",
    "feature_num = 3\n",
    "num_graphs = 10\n",
    "# graphs = glob.glob(\"./pmodel_data/job/sample-plan-*\")\n",
    "# num_graphs = len(graphs)\n",
    "iteration_num = int(round(0.8 * num_graphs, 0)) \n",
    "print(\"[training samples]:{}\".format(iteration_num))\n",
    "\n",
    "model = GCN(nfeat=feature_num,\n",
    "            nhid=args.hidden,\n",
    "            nclass=node_dim, \n",
    "            dropout=args.dropout)    \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "for wid in range(iteration_num):\n",
    "    print(\"[graph {}]\".format(wid))\n",
    "    # Load data \n",
    "    adj, features, labels, idx_train, idx_val, idx_test = load_data(path = data_path + \"graph/\", dataset = \"sample-plan-\" + str(wid))\n",
    "    # print(adj.shape)\n",
    "    \n",
    "    # Model Training\n",
    "    ok_times = 0\n",
    "    t_total = time.time()\n",
    "    for epoch in range(args.epochs):\n",
    "        # print(features.shape, adj.shape)\n",
    "        loss_train = train(epoch, labels, features, adj, idx_train)\n",
    "        if loss_train < 0.002:\n",
    "            ok_times += 1\n",
    "        if ok_times >= 20:\n",
    "            break    \n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "    # Model Validation\n",
    "    test(labels, features, adj, idx_test)\n",
    "\n",
    "for wid in range(iteration_num, num_graphs):\n",
    "    # Load data\n",
    "    adj, features, labels, idx_train, idx_val, idx_test = load_data(path = data_path + \"graph/\", dataset = \"sample-plan-\" + str(wid))\n",
    "    \n",
    "    # Model Testing\n",
    "    t_total = time.time()\n",
    "    test(labels, features, adj, idx_test)\n",
    "    print(\"Testing Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ihz22FiQNcQg"
   },
   "outputs": [],
   "source": [
    "feature_num = 3\n",
    "num_graphs = 10\n",
    "iteration_num = int(round(0.8 * num_graphs, 0))\n",
    "\n",
    "def test_cross_dataset_generalibility(data_path_A : str, data_path_B : str):\n",
    "  # We assume that the data set sizes are the same.\n",
    "  model = GCN(nfeat=feature_num,\n",
    "            nhid=args.hidden,\n",
    "            nclass=node_dim, \n",
    "            dropout=args.dropout)    \n",
    "\n",
    "  optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "  \n",
    "  for wid in range(iteration_num):\n",
    "    print(\"[graph {}]\".format(wid))\n",
    "    # Load data \n",
    "    adj, features, labels, idx_train, idx_val, idx_test = load_data(path = data_path_A + \"graph/\", dataset = \"sample-plan-\" + str(wid))\n",
    "    # print(adj.shape)\n",
    "    \n",
    "    # Model Training\n",
    "    ok_times = 0\n",
    "    t_total = time.time()\n",
    "    for epoch in range(args.epochs):\n",
    "        # print(features.shape, adj.shape)\n",
    "        loss_train = train(epoch, labels, features, adj, idx_train)\n",
    "        if loss_train < 0.002:\n",
    "            ok_times += 1\n",
    "        if ok_times >= 20:\n",
    "            break    \n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "    # Model Validation\n",
    "    test(labels, features, adj, idx_test)\n",
    "\n",
    "  test_losses = []\n",
    "  for wid in range(num_graphs - iteration_num):\n",
    "    # Load data\n",
    "    adj, features, labels, idx_train, idx_val, idx_test = load_data(path = data_path_B + \"graph/\", dataset = \"sample-plan-\" + str(wid))\n",
    "    \n",
    "    # Model Testing\n",
    "    t_total = time.time()\n",
    "    test_losses.append(test(labels, features, adj, idx_test))\n",
    "    print(\"Testing Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "  return np.array(test_losses).mean()\n",
    "\n",
    "def cross_product_experiment(data_paths : list):\n",
    "  square_loss_array = np.zeros((len(data_paths), len(data_paths)))\n",
    "  for i in range(len(data_paths)):\n",
    "    for j in range(len(data_paths)):\n",
    "      square_loss_array[i][j] = test_cross_dataset_generalibility(data_paths[i], data_paths[j])\n",
    "      print(f\"train {data_paths[i]} test {data_paths[j]} loss {square_loss_array[i][j]}\")\n",
    "\n",
    "  return square_loss_array\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UtbTTK5aNcQg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of performance-graphembedding-checkpoint.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
